{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgCqjRhN0WNe"
      },
      "source": [
        "# Imports and dataset download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugOtZey1tDFs"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import torch \n",
        "import sklearn\n",
        "import scipy\n",
        "from scipy import io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets\n",
        "import torch.utils.data as data\n",
        "from sklearn.metrics import accuracy_score, silhouette_score\n",
        "from sklearn.metrics.cluster import normalized_mutual_info_score, adjusted_rand_score\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn import metrics\n",
        "from sklearn import decomposition\n",
        "from sklearn import manifold\n",
        "from tqdm.notebook import trange, tqdm\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.manifold import TSNE\n",
        "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
        "from typing import Optional\n",
        "import seaborn as sns\n",
        "\n",
        "import copy\n",
        "import random\n",
        "import time\n",
        "from math import floor\n",
        "\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NogYSYKMav-3"
      },
      "outputs": [],
      "source": [
        "cuda = True if torch.cuda.is_available() else False\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "# torch.cuda.set_device(device)\n",
        "\n",
        "float_Tensor = torch.FloatTensor\n",
        "if cuda: float_Tensor = torch.cuda.FloatTensor\n",
        "\t\n",
        "drop_last = True\n",
        "shuffle = False\n",
        "IMG_SIZE = 28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSmHyYZ-3Z62"
      },
      "outputs": [],
      "source": [
        "def get_fashionMNIST_np(create_subset=False, data_per_pattern=1000):\n",
        "  notebook_path = os.path.abspath(\"CNN_Fashion_MNIST.ipynb\")\n",
        "  labels_path = os.path.join(os.path.dirname(notebook_path), \"drive/MyDrive/Diploma/FashionMNIST/FashionMNIST.npy\")\n",
        "  data_path = os.path.join(os.path.dirname(notebook_path), \"drive/MyDrive/Diploma/FashionMNIST_labels.npy\")\n",
        "\n",
        "  if(create_subset):\n",
        "    transform = transforms.Compose([transforms.Resize(IMG_SIZE), transforms.ToTensor()])\n",
        "    train = datasets.FashionMNIST(\"./datasets/FashionMNIST\", train=True, download=True, transform=transform)\n",
        "    trainset = torch.utils.data.DataLoader(train, batch_size=1, shuffle=shuffle)\n",
        "    data = []\n",
        "    labels = []\n",
        "    dict_counter = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
        "    data_per_pattern = data_per_pattern\n",
        "\n",
        "    for datapoint in enumerate(trainset):\n",
        "      current_size = sum(dict_counter.values())\n",
        "      target_size = len(dict_counter) * data_per_pattern\n",
        "\n",
        "      if current_size >= target_size: break\n",
        "      batch_idx, (example_data, example_targets) = datapoint\n",
        "      example_targets = example_targets.item()\n",
        "\n",
        "      if example_targets in dict_counter and dict_counter[example_targets] < data_per_pattern:\n",
        "        dict_counter[example_targets] += 1\n",
        "        example_data = example_data.view(1, IMG_SIZE, IMG_SIZE)\n",
        "        data.append(np.array(example_data))\n",
        "        labels.append(example_targets)\n",
        "    np.save(data_path, data)\n",
        "    np.save(labels_path, labels)\n",
        "\n",
        "  # Load data\n",
        "  data = np.load(data_path)\n",
        "  labels = np.load(labels_path)\n",
        "\n",
        "  return data, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cl45xL5K5W5r"
      },
      "outputs": [],
      "source": [
        "def get_FashionMNIST_dataloader(batch_size=64):\n",
        "\ttransform = transforms.Compose([transforms.Resize(IMG_SIZE), transforms.ToTensor()])\n",
        "\ttrain = datasets.FashionMNIST(\"./datasets/FashionMNIST\", train=True, download=True, transform=transform)\n",
        "\tdataloader = torch.utils.data.DataLoader(train, batch_size=batch_size, drop_last=drop_last, shuffle=shuffle)\n",
        "\treturn dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ro1KwYuVODTk"
      },
      "outputs": [],
      "source": [
        "images, labels= get_fashionMNIST_np(True, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZxxLplJ6k8i",
        "outputId": "7b240a33-09ec-4844-fbc6-039a5fc52d5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 1, 28, 28)\n",
            "(10000, 784)\n"
          ]
        }
      ],
      "source": [
        "images_reshaped = images.reshape(len(images),-1) # reshape for k-means\n",
        "print(images.shape)\n",
        "print(images_reshaped.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfYx576kAUMb"
      },
      "outputs": [],
      "source": [
        "images = torch.Tensor(images)\n",
        "labels = torch.Tensor(labels)\n",
        "final_dataset = TensorDataset(images, labels)\n",
        "dataloader = DataLoader(final_dataset, batch_size=64, drop_last=drop_last, shuffle=shuffle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kztBh_520Bu2"
      },
      "source": [
        "# Autoencoder/Custom dataset/function declarations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkP3eEAYNQu-"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, datapoints, labels, transform=None):\n",
        "\n",
        "    self.datapoints = datapoints\n",
        "    self.labels = labels\n",
        "    self.transform = transform\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    sample = self.datapoints[index], self.labels[index]\n",
        "\n",
        "    if self.transform:\n",
        "      sample = self.transform(sample)\n",
        "\n",
        "    return sample\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.datapoints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3m1mESr0uiu"
      },
      "outputs": [],
      "source": [
        "# TODO: search hungarian algorithm\n",
        "def cluster_accuracy(y_true, y_predicted, cluster_number: Optional[int] = None):\n",
        "\t\"\"\"\n",
        "\tCalculate clustering accuracy after using the linear_sum_assignment function in SciPy to\n",
        "\tdetermine reassignments.\n",
        "\n",
        "\t:param y_true: list of true cluster numbers, an integer array 0-indexed\n",
        "\t:param y_predicted: list of predicted cluster numbers, an integer array 0-indexed\n",
        "\t:param cluster_number: number of clusters, if None then calculated from input\n",
        "\t:return: reassignment dictionary, clustering accuracy\n",
        "\t\"\"\"\n",
        "\tif cluster_number is None:\n",
        "\t\t# assume labels are 0-indexed\n",
        "\t\tcluster_number = (max(y_predicted.max(), y_true.max()) + 1)\n",
        "\tcount_matrix = np.zeros((cluster_number, cluster_number), dtype=np.int64)\n",
        "\tfor i in range(y_predicted.size):\n",
        "\t\tcount_matrix[y_predicted[i], y_true[i]] += 1\n",
        "\n",
        "\trow_ind, col_ind = linear_assignment(count_matrix.max() - count_matrix)\n",
        "\treassignment = dict(zip(row_ind, col_ind))\n",
        "\taccuracy = count_matrix[row_ind, col_ind].sum() / y_predicted.size\n",
        "\treturn reassignment, accuracy\n",
        "\n",
        "def transform_clusters_to_labels(clusters, labels):\n",
        "\t# Find the cluster ids (labels)\n",
        "\tc_ids = np.unique(clusters)\n",
        "\n",
        "\t# Dictionary to transform cluster label to real label\n",
        "\tdict_clusters_to_labels = dict()\n",
        "\n",
        "\t# For every cluster find the most frequent data label\n",
        "\tfor c_id in c_ids:\n",
        "\t\tindexes_of_cluster_i = np.where(c_id == clusters)\n",
        "\t\telements, frequency = np.unique(labels[indexes_of_cluster_i], return_counts=True)\n",
        "\t\ttrue_label_index = np.argmax(frequency)\n",
        "\t\ttrue_label = elements[true_label_index]\n",
        "\t\tdict_clusters_to_labels[c_id] = true_label\n",
        "\n",
        "\t# Change the cluster labels to real labels\n",
        "\tfor i, element in enumerate(clusters):\n",
        "\t\tclusters[i] = dict_clusters_to_labels[element]\n",
        "\n",
        "\treturn clusters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HQVkP882REx"
      },
      "outputs": [],
      "source": [
        "def check_number_of_representatives(Y): # TODO: check for empty clusters\n",
        "  cluster_map = {}\n",
        "\n",
        "  for element in Y:\n",
        "    if element not in cluster_map:\n",
        "      cluster_map[element] = 1\n",
        "    else:\n",
        "      cluster_map[element] += 1\n",
        "\n",
        "  for cluster_number in cluster_map.keys():\n",
        "    print(\"Cluster\", cluster_number, \":\", cluster_map[cluster_number], \"elements \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YD8nqJpeH0SN"
      },
      "outputs": [],
      "source": [
        "class Autoencoder(nn.Module):\n",
        "  \n",
        "    def __init__(self, input_dimension, hl1_neurons = 32, hl2_neurons = 64, hl3_neurons = 128, latent_dimension = 10):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder Model\n",
        "        # self.input_dimension = input_dimension\n",
        "        # self.hl1_neurons = hl1_neurons\n",
        "        # self.hl2_neurons = hl2_neurons\n",
        "        # self.latent_dimension = latent_dimension\n",
        "\n",
        "        self.encoder_model = nn.Sequential(\n",
        "          nn.Conv2d(1, hl1_neurons, kernel_size = 5, stride = 2, padding = 2),\n",
        "          nn.Sigmoid(),\n",
        "          nn.BatchNorm2d(hl1_neurons),\n",
        "\n",
        "          nn.Conv2d(hl1_neurons, hl2_neurons, kernel_size = 5, stride = 2, padding = 2),\n",
        "          nn.Sigmoid(),\n",
        "          nn.BatchNorm2d(hl2_neurons),\n",
        "\n",
        "          nn.Conv2d(hl2_neurons, hl3_neurons, kernel_size = 3, stride = 2, padding = 0),\n",
        "          nn.Sigmoid(),\n",
        "          nn.BatchNorm2d(hl3_neurons),\n",
        "\n",
        "          nn.Flatten(start_dim=1),\n",
        "          nn.Linear(hl3_neurons * 3 * 3, latent_dimension, bias=True), # latent_dimension * 3 * 3\n",
        "          nn.Sigmoid(),\n",
        "          nn.BatchNorm1d(latent_dimension)\n",
        "        )\n",
        "        \n",
        "        # Latent Space\n",
        "        # self.latent_space = nn.Sequential(\n",
        "        #   nn.Linear(self.hl2_neurons, self.latent_dimension),\n",
        "        #   nn.Sigmoid(),\n",
        "        #   nn.BatchNorm1d(self.latent_dimension),\n",
        "        # )\n",
        "\n",
        "        # Decoder Model\n",
        "        self.decoder_model = nn.Sequential(\n",
        "          nn.Linear(latent_dimension,  hl3_neurons * 3 * 3, bias=True),\n",
        "          nn.Sigmoid(),\n",
        "          nn.BatchNorm1d(hl3_neurons * 3 * 3),\n",
        "          nn.Unflatten(dim = 1, unflattened_size =  (hl3_neurons, 3, 3)),\n",
        "\n",
        "          nn.ConvTranspose2d(hl3_neurons, hl2_neurons, kernel_size = 3, stride = 2, padding = 0),\n",
        "          nn.Sigmoid(),\n",
        "          nn.BatchNorm2d(hl2_neurons),\n",
        "          \n",
        "          nn.ConvTranspose2d(hl2_neurons, hl1_neurons, kernel_size = 5, stride = 2, padding = 2, output_padding = 1),\n",
        "          nn.Sigmoid(),\n",
        "          nn.BatchNorm2d(hl1_neurons),\n",
        "          \n",
        "          nn.ConvTranspose2d(hl1_neurons, 1, kernel_size = 5, stride = 2, padding = 2, output_padding = 1),\n",
        "          nn.Sigmoid(),\n",
        "          nn.BatchNorm2d(1)\n",
        "        )\n",
        "  \n",
        "    def forward(self, x):\n",
        "      x = self.encoder_model(x)\n",
        "      x = self.decoder_model(x)\n",
        "      return x\n",
        "  \n",
        "    def encoder(self, x, device):\n",
        "      x = self.encoder_model(x.to(device=device))\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Jls3BixmHpG"
      },
      "outputs": [],
      "source": [
        "def train_autoencoder(device, dataloader, autoencoder, cluster_centroids, cluster_labels, epochs=500):\n",
        "\tcriterion = nn.MSELoss().to(device)\n",
        "\toptimizer = optim.Adam(autoencoder.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\tloss_list = list()\n",
        "\tbatch_size = 64\n",
        "\t\n",
        "\tautoencoder.train()\n",
        "\tfor epoch in range(epochs):\n",
        "\t\tloss = 0\n",
        "\t\tfor batch_index, (batch, labels) in enumerate(dataloader):\n",
        "\t\t\t# Data to the GPU (cuda)\n",
        "\t\t\tbatch = Variable(batch.type(float_Tensor))\n",
        "\t\t\t\n",
        "\t\t\t# Indexes\n",
        "\t\t\tlow_index = batch_index * batch_size\n",
        "\t\t\thigh_index = (batch_index + 1) * batch_size\n",
        "\t\t\t\n",
        "\t\t\t# Select centroids\n",
        "\t\t\tbatch_cluster_labels = cluster_labels[low_index:high_index]\n",
        "\t\t\tbatch_cluster_centroids = cluster_centroids[batch_cluster_labels]\n",
        "\t\t\tbatch_cluster_centroids = torch.Tensor(batch_cluster_centroids)\n",
        "\t\t\tbatch_cluster_centroids = Variable(batch_cluster_centroids.type(float_Tensor))\n",
        "\t\t\tbatch_cluster_centroids = torch.reshape(batch_cluster_centroids, (batch_size, 1, 28, 28))\n",
        "\t\t\t\n",
        "\t\t\t# reset the gradients to zero\n",
        "\t\t\toptimizer.zero_grad()\n",
        "\t\t\tautoencoder.zero_grad()\n",
        "\n",
        "\t\t\t# compute reconstructions\n",
        "\t\t\treconstructions = autoencoder(batch)\n",
        "\t\t\t\n",
        "\t\n",
        "\t\t\t# compute training reconstruction loss\n",
        "\t\t\ttrain_loss = criterion(reconstructions, batch_cluster_centroids)\n",
        "\t \n",
        "\t\t\t# compute accumulated gradients\n",
        "\t\t\ttrain_loss.backward()\n",
        "\n",
        "\t\t\t# perform parameter update based on current gradients\n",
        "\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t# add the mini-batch training loss to epoch loss\n",
        "\t\t\tloss += train_loss.item()\n",
        "\n",
        "\t\tloss = loss / len(dataloader)\n",
        "\t\tloss_list.append(loss)\n",
        "\t\tprint(\"Epoch: {}/{}, Loss: {:.6f}\".format(epoch + 1, epochs, loss))\n",
        "\t\t\n",
        "\tautoencoder.eval()\n",
        "\treturn autoencoder, loss_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL6sg4u2OW21"
      },
      "source": [
        "# Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmeiJnVkmWzg"
      },
      "outputs": [],
      "source": [
        "kmeans_10_clusters = KMeans(n_clusters=10, n_init=100).fit(images_reshaped)\n",
        "retrieved_labels_10_clusters = transform_clusters_to_labels(kmeans_10_clusters.labels_, labels)\n",
        "\n",
        "print(\"K_means greedy accuracy score for 8 clusters on initial space:\",accuracy_score(labels, retrieved_labels_10_clusters))\n",
        "print(\"K_means hungarian accuracy score for 8 clusters on initial space:\",cluster_accuracy(labels, kmeans_10_clusters.labels_)[1])\n",
        "print(\"K-means normalised mutual info score for 8 clusters on initial space:\",normalized_mutual_info_score(labels, kmeans_10_clusters.labels_))\n",
        "print(\"K-means ARI for 8 clusters on initial space:\",adjusted_rand_score(labels, kmeans_10_clusters.labels_))\n",
        "print(\"K-means silhouette score for 2 clusters on initial space:\",silhouette_score(images_reshaped, kmeans_10_clusters.labels_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6PgL45sT2V6"
      },
      "outputs": [],
      "source": [
        "aggloClustering_10_clusters = AgglomerativeClustering(n_clusters=10).fit(images_reshaped)\n",
        "agglo_retrieved_labels = transform_clusters_to_labels(aggloClustering_10_clusters.labels_, labels)\n",
        "\n",
        "# print the stats on agglomerative clustering\n",
        "\n",
        "print(\"Agglomerative clustering on initial space greedy accuracy score:\",accuracy_score(labels, agglo_retrieved_labels))\n",
        "print(\"Agglomerative clustering on initial space hungarian accuracy score:\",cluster_accuracy(labels, aggloClustering_10_clusters.labels_)[1])\n",
        "print(\"Normalised mutual info score on agglomerative clustering on initial space:\",normalized_mutual_info_score(labels, aggloClustering_10_clusters.labels_))\n",
        "print(\"Agglomerative clustering ARI on initial space:\",adjusted_rand_score(labels, aggloClustering_10_clusters.labels_))\n",
        "print(\"Agglomerative clustering silhouette score for 2 clusters on initial space:\",silhouette_score(images_reshaped, aggloClustering_10_clusters.labels_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xzNK8z6KdIo"
      },
      "outputs": [],
      "source": [
        "def run_experiment(K, dataloader, data_shape, datapoints, datapoints_reshaped, labels, epochs=30, hl1_neurons=32, hl2_neurons=64, hl3_neurons=128, latent_dimension=10):\n",
        "\n",
        "  print('Experiment results for k-means with k =', K, 'clusters:\\n')\n",
        "  print('Running k-means algorithm in order to get our pseudolabels: \\n')\n",
        "\n",
        "  kmeans_initial = KMeans(n_clusters=K, n_init=10).fit(datapoints_reshaped) \n",
        "  cluster_centers = kmeans_initial.cluster_centers_\n",
        "  cluster_labels = kmeans_initial.labels_\n",
        "  real_labels = transform_clusters_to_labels(cluster_labels, labels)\n",
        "\n",
        "  # check TSNE representation of our features according to the k categories\n",
        "\n",
        "  # print(\"Creating TSNE representation of our features according to the k categories...\\n\")\n",
        "\n",
        "  # pseudolabels_indices = [i for i in range(K)]\n",
        "\n",
        "  # plt.figure(figsize=(16,10)) \n",
        "  # kmeans_scatterplot = plt.scatter(\n",
        "  #     x=data_TSNE[:,0], y=data_TSNE[:,1],\n",
        "  #     c=cluster_labels,\n",
        "  #     cmap=\"gist_rainbow\"\n",
        "  # )\n",
        "\n",
        "  # handles = kmeans_scatterplot.legend_elements(num=pseudolabels_indices)[0]\n",
        "  # plt.legend(handles, pseudolabels_indices)\n",
        "\n",
        "  # savestring = 'pseudolabels_TSNE_initial_space_k=' + str(K) + '.png'\n",
        "  # plt.savefig(savestring)\n",
        "\n",
        "  # find out accuracy of the algorithm in the initial space\n",
        "\n",
        "  # kmeans_initial_hungarian_acc = cluster_accuracy(labels, cluster_labels)[1]\n",
        "  kmeans_initial_NMI = normalized_mutual_info_score(labels, cluster_labels)\n",
        "  kmeans_initial_ARI = adjusted_rand_score(labels, cluster_labels)\n",
        "\n",
        "  print(\"K_means greedy accuracy score (initial space):\",accuracy_score(labels, real_labels))\n",
        "  # print(\"K_means hungarian accuracy score (initial space):\",kmeans_initial_hungarian_acc)\n",
        "  print(\"Normalised mutual info score (initial space):\",kmeans_initial_NMI)\n",
        "  print(\"ARI (initial space):\",kmeans_initial_ARI, \"\\n\")\n",
        "\n",
        "  # using the autoencoder model on our data\n",
        "\n",
        "  print('Using the autoencoder model on our data: \\n')\n",
        "\n",
        "  kmeans_accuracy_scores = []\n",
        "  k_means_silhouette_scores = []\n",
        "  k_means_cluster_error_scores = []\n",
        "  kmeans_NMI_scores = []\n",
        "  kmeans_ARI_scores = []\n",
        "  agglo_accuracy_scores = []\n",
        "  agglo_NMI_scores = []\n",
        "  agglo_ARI_scores = []\n",
        "  agglo_silhouette_scores = []\n",
        "  \n",
        "  for i in range(5):\n",
        "\n",
        "    print(\"ROUND NUMBER \",i + 1,\":\\n\")\n",
        "    autoencoder = Autoencoder(input_dimension=data_shape, hl1_neurons=hl1_neurons, hl2_neurons=hl2_neurons, hl3_neurons=hl3_neurons, latent_dimension=latent_dimension).to(device)\n",
        "    autoencoder, loss_list = train_autoencoder(device, dataloader, autoencoder, cluster_centers, cluster_labels, epochs=epochs)\n",
        "    latent_data = autoencoder.encoder(datapoints, device).cpu().detach().numpy() \n",
        "\n",
        "    # data_TSNE = TSNE(n_components=2, verbose=1, perplexity=20, n_iter=400, learning_rate='auto').fit_transform(latent_data)\n",
        "\n",
        "    # dataset_labels = [0,1,2,3,4,5,6,7,8,9]\n",
        "\n",
        "    # plt.figure(figsize=(16,10))\n",
        "    # scatterplot = plt.scatter(\n",
        "    #     x=data_TSNE[:,0], y=data_TSNE[:,1],\n",
        "    #     c=labels,\n",
        "    #     cmap=\"tab10\"\n",
        "    # )\n",
        "\n",
        "    # handles, _ = scatterplot.legend_elements(prop='colors')\n",
        "    # plt.legend(handles, dataset_labels)\n",
        "    \n",
        "    print(\"Creating a k-means model on latent data:\\n\")\n",
        "\n",
        "    # Maybe we need Standar Scaler\n",
        "\n",
        "    # std_scaler = StandardScaler()\n",
        "    # latent_data = std_scaler.fit_transform(latent_data)\n",
        "\n",
        "    # Clustering on transformed space\n",
        "\n",
        "    new_kmeans = KMeans(n_clusters=10, n_init=100).fit(latent_data)\n",
        "\n",
        "    kmeans_clusters = new_kmeans.labels_\n",
        "    kmeans_cluster_error = new_kmeans.inertia_\n",
        "    k_means_cluster_error_scores.append(kmeans_cluster_error)\n",
        "    kmeans_greedy_labels = transform_clusters_to_labels(kmeans_clusters, labels)\n",
        "\n",
        "    # print the stats on the transformed space\n",
        "    kmeans_greedy_acc = accuracy_score(labels, kmeans_greedy_labels)\n",
        "\n",
        "    # These metrics as input they use the output of the clustering algorithm\n",
        "    # kmeans_hungarian_acc = cluster_accuracy(labels, kmeans_clusters)[1]\n",
        "    kmeans_NMI = normalized_mutual_info_score(labels, kmeans_clusters)\n",
        "    kmeans_ARI = adjusted_rand_score(labels, kmeans_clusters)\n",
        "\n",
        "    kmeans_accuracy_scores.append(kmeans_greedy_acc)\n",
        "    kmeans_NMI_scores.append(kmeans_NMI)\n",
        "    kmeans_ARI_scores.append(kmeans_ARI)\n",
        "\n",
        "    k_means_silhouette_score = silhouette_score(latent_data, kmeans_clusters)\n",
        "    k_means_silhouette_scores.append(k_means_silhouette_score)\n",
        "\n",
        "    print(\"K-means with 10 clusters on latent space stats: \\n\")\n",
        "    print(\"K-means on latent space greedy accuracy score:\",kmeans_greedy_acc)\n",
        "    # print(\"K-means on latent space hungarian accuracy score:\",kmeans_hungarian_acc)\n",
        "    print(\"Normalised mutual info score on k-means on latent space:\", kmeans_NMI)\n",
        "    print(\"ARI score on k-means on latent space:\", kmeans_ARI)\n",
        "    print(\"K-means cluster error on latent space:\", kmeans_cluster_error)\n",
        "    print(\"K-means silhouette score on latent space:\", k_means_silhouette_score, \"\\n\")\n",
        "\n",
        "    # do agglomerative clustering on the transformed space\n",
        "\n",
        "    print(\"Doing agglomerative clustering on MLP output vectors:\\n\")\n",
        "    aggloClustering = AgglomerativeClustering(n_clusters=10).fit(latent_data)\n",
        "    aggloClustering_clusters = aggloClustering.labels_\n",
        "    agglo_greedy_labels = transform_clusters_to_labels(aggloClustering_clusters, labels)\n",
        "\n",
        "    # print the stats on agglomerative clustering\n",
        "    agglo_greedy_acc = accuracy_score(labels, agglo_greedy_labels)\n",
        "    # agglo_hungarian_acc = cluster_accuracy(labels, aggloClustering_clusters)[1]\n",
        "    agglo_NMI = normalized_mutual_info_score(labels, aggloClustering_clusters)\n",
        "    agglo_ARI = adjusted_rand_score(labels, aggloClustering_clusters)\n",
        "\n",
        "    agglo_accuracy_scores.append(agglo_greedy_acc)\n",
        "    agglo_NMI_scores.append(agglo_NMI)\n",
        "    agglo_ARI_scores.append(agglo_ARI)\n",
        "\n",
        "    agglo_silhouette_score = silhouette_score(latent_data, aggloClustering_clusters)\n",
        "    agglo_silhouette_scores.append(agglo_silhouette_score)\n",
        "\n",
        "    print(\"Agglomerative clustering on latent space greedy accuracy score:\", agglo_greedy_acc)\n",
        "    # print(\"Agglomerative clustering on latent space hungarian accuracy score:\", agglo_hungarian_acc)\n",
        "    print(\"Normalised mutual info score on agglomerative clustering on latent space:\",agglo_NMI, \"\\n\")\n",
        "    print(\"ARI score on agglomerative clustering on latent space:\", agglo_ARI, \"\\n\")\n",
        "\n",
        "  print(\"Average k-means accuracy score at latent space:\", sum(kmeans_accuracy_scores) / len(kmeans_accuracy_scores), \"\\n\")\n",
        "  print(\"Average k-means NMI score at latent space:\", sum(kmeans_NMI_scores) / len(kmeans_NMI_scores), \"\\n\")\n",
        "  print(\"Average k-means ARI score at latent space:\", sum(kmeans_ARI_scores) / len(kmeans_ARI_scores), \"\\n\")\n",
        "  print(\"Average agglomerative clustering accuracy score at latent space:\", sum(agglo_accuracy_scores) / len(agglo_accuracy_scores), \"\\n\")\n",
        "  print(\"Average agglomerative clustering NMI score at latent space:\", sum(agglo_NMI_scores) / len(agglo_NMI_scores), \"\\n\")\n",
        "  print(\"Average agglomerative clustering ARI score at latent space:\", sum(agglo_ARI_scores) / len(agglo_ARI_scores), \"\\n\")\n",
        "  print(\"Average k-means silhouette score on latent space:\", sum(k_means_silhouette_scores) / len(k_means_silhouette_scores), \"\\n\")\n",
        "  print(\"Average k-means cluster error on latent space:\", sum(k_means_cluster_error_scores) / len(k_means_cluster_error_scores), \"\\n\")\n",
        "\n",
        "  return [ kmeans_accuracy_scores, kmeans_NMI_scores, kmeans_ARI_scores, agglo_accuracy_scores, agglo_NMI_scores, agglo_ARI_scores, k_means_silhouette_scores, agglo_silhouette_scores]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "3foB6QAtGFoL",
        "outputId": "063ecec4-2b72-4ab8-8d23-27e968f301bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment results for k-means with k = 20 clusters:\n",
            "\n",
            "Running k-means algorithm in order to get our pseudolabels: \n",
            "\n",
            "K_means greedy accuracy score (initial space): 0.6677\n",
            "Normalised mutual info score (initial space): 0.592977052476283\n",
            "ARI (initial space): 0.4771964447957748 \n",
            "\n",
            "Using the autoencoder model on our data: \n",
            "\n",
            "ROUND NUMBER  1 :\n",
            "\n",
            "Epoch: 1/50, Loss: 0.849026\n",
            "Epoch: 2/50, Loss: 0.642582\n",
            "Epoch: 3/50, Loss: 0.571893\n",
            "Epoch: 4/50, Loss: 0.534249\n",
            "Epoch: 5/50, Loss: 0.502260\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-05b8a1882dbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults_for_k_20\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatapoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatapoints_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages_reshaped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhl1_neurons\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhl2_neurons\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhl3_neurons\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-27-5a172f16db0c>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(K, dataloader, data_shape, datapoints, datapoints_reshaped, labels, epochs, hl1_neurons, hl2_neurons, hl3_neurons, latent_dimension)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ROUND NUMBER \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\":\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mautoencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhl1_neurons\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhl1_neurons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhl2_neurons\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhl2_neurons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhl3_neurons\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhl3_neurons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlatent_dimension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mautoencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_autoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_centers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mlatent_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-0521ff1f3a12>\u001b[0m in \u001b[0;36mtrain_autoencoder\u001b[0;34m(device, dataloader, autoencoder, cluster_centroids, cluster_labels, epochs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                         \u001b[0;31m# compute accumulated gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                         \u001b[0;31m# perform parameter update based on current gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "results_for_k_20 = run_experiment(20, dataloader, 784, datapoints = images, datapoints_reshaped = images_reshaped, labels = labels, epochs=50, hl1_neurons=32, hl2_neurons=64, hl3_neurons=128, latent_dimension=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-IJqn7D7yUZ"
      },
      "outputs": [],
      "source": [
        "results_for_k_30 = run_experiment(30, dataloader, 784, datapoints = images, datapoints_reshaped = images_reshaped, labels = labels, epochs=50, hl1_neurons=32, hl2_neurons=64, hl3_neurons=128, latent_dimension=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6t-Lv1M77ycE"
      },
      "outputs": [],
      "source": [
        "results_for_k_40 = run_experiment(40, dataloader, 784, datapoints = images, datapoints_reshaped = images_reshaped, labels = labels, epochs=50, hl1_neurons=32, hl2_neurons=64, hl3_neurons=128, latent_dimension=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icQOKDGT7yef"
      },
      "outputs": [],
      "source": [
        "results_for_k_50 = run_experiment(50, dataloader, 784, datapoints = images, datapoints_reshaped = images_reshaped, labels = labels, epochs=50, hl1_neurons=32, hl2_neurons=64, hl3_neurons=128, latent_dimension=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssu74yY07yhP"
      },
      "outputs": [],
      "source": [
        "results_for_k_60 = run_experiment(60, dataloader, 784, datapoints = images, datapoints_reshaped = images_reshaped, labels = labels, epochs=50, hl1_neurons=32, hl2_neurons=64, hl3_neurons=128, latent_dimension=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAi9YbAH7yj-"
      },
      "outputs": [],
      "source": [
        "results_for_k_70 = run_experiment(70, dataloader, 784, datapoints = images, datapoints_reshaped = images_reshaped, labels = labels, epochs=50, hl1_neurons=32, hl2_neurons=64, hl3_neurons=128, latent_dimension=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlTlH1et7ymn"
      },
      "outputs": [],
      "source": [
        "results_for_k_80 = run_experiment(80, dataloader, 784, datapoints = images, datapoints_reshaped = images_reshaped, labels = labels, epochs=50, hl1_neurons=32, hl2_neurons=64, hl3_neurons=128, latent_dimension=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_co3C7n7ypS"
      },
      "outputs": [],
      "source": [
        "results_for_k_100 = run_experiment(100, dataloader, 784, datapoints = images, datapoints_reshaped = images_reshaped, labels = labels, epochs=50, hl1_neurons=32, hl2_neurons=64, hl3_neurons=128, latent_dimension=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhFaEGm_7yr5"
      },
      "outputs": [],
      "source": [
        "results_for_k_150 = run_experiment(150, dataloader, 784, datapoints = images, datapoints_reshaped = images_reshaped, labels = labels, epochs=50, hl1_neurons=32, hl2_neurons=64, hl3_neurons=128, latent_dimension=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5hJWy_07yxe"
      },
      "outputs": [],
      "source": [
        "results_for_k_200 = run_experiment(200, dataloader, 784, datapoints = images, datapoints_reshaped = images_reshaped, labels = labels, epochs=50, hl1_neurons=32, hl2_neurons=64, hl3_neurons=128, latent_dimension=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rd88pQAdEkx"
      },
      "outputs": [],
      "source": [
        "kmeans_acc = np.zeros((10,10))\n",
        "kmeans_NMI = np.zeros((10,10))\n",
        "kmeans_ARI = np.zeros((10,10))\n",
        "agglo_acc = np.zeros((10,10))\n",
        "agglo_NMI = np.zeros((10,10))\n",
        "agglo_ARI = np.zeros((10,10))\n",
        "kmeans_silhouette = np.zeros((10,10))\n",
        "agglo_silhouette = np.zeros((10,10))\n",
        "\n",
        "for i in range(10):\n",
        "  \n",
        "  kmeans_acc[i][0] = results_for_k_20[0][i]\n",
        "  kmeans_acc[i][1] = results_for_k_30[0][i]\n",
        "  kmeans_acc[i][2] = results_for_k_40[0][i]\n",
        "  kmeans_acc[i][3] = results_for_k_50[0][i]\n",
        "  kmeans_acc[i][4] = results_for_k_60[0][i]\n",
        "  kmeans_acc[i][5] = results_for_k_70[0][i]\n",
        "  kmeans_acc[i][6] = results_for_k_80[0][i]\n",
        "  kmeans_acc[i][7] = results_for_k_100[0][i]\n",
        "  kmeans_acc[i][8] = results_for_k_150[0][i]\n",
        "  kmeans_acc[i][9] = results_for_k_200[0][i]\n",
        "\n",
        "  kmeans_NMI[i][0] = results_for_k_20[1][i]\n",
        "  kmeans_NMI[i][1] = results_for_k_30[1][i]\n",
        "  kmeans_NMI[i][2] = results_for_k_40[1][i]\n",
        "  kmeans_NMI[i][3] = results_for_k_50[1][i]\n",
        "  kmeans_NMI[i][4] = results_for_k_60[1][i]\n",
        "  kmeans_NMI[i][5] = results_for_k_70[1][i]\n",
        "  kmeans_NMI[i][6] = results_for_k_80[1][i]\n",
        "  kmeans_NMI[i][7] = results_for_k_100[1][i]\n",
        "  kmeans_NMI[i][8] = results_for_k_150[1][i]\n",
        "  kmeans_NMI[i][9] = results_for_k_200[1][i]\n",
        "\n",
        "  kmeans_ARI[i][0] = results_for_k_20[2][i]\n",
        "  kmeans_ARI[i][1] = results_for_k_30[2][i]\n",
        "  kmeans_ARI[i][2] = results_for_k_40[2][i]\n",
        "  kmeans_ARI[i][3] = results_for_k_50[2][i]\n",
        "  kmeans_ARI[i][4] = results_for_k_60[2][i]\n",
        "  kmeans_ARI[i][5] = results_for_k_70[2][i]\n",
        "  kmeans_ARI[i][6] = results_for_k_80[2][i]\n",
        "  kmeans_ARI[i][7] = results_for_k_100[2][i]\n",
        "  kmeans_ARI[i][8] = results_for_k_150[2][i]\n",
        "  kmeans_ARI[i][9] = results_for_k_200[2][i]\n",
        "\n",
        "  agglo_acc[i][0] = results_for_k_20[3][i]\n",
        "  agglo_acc[i][1] = results_for_k_30[3][i]\n",
        "  agglo_acc[i][2] = results_for_k_40[3][i]\n",
        "  agglo_acc[i][3] = results_for_k_50[3][i]\n",
        "  agglo_acc[i][4] = results_for_k_60[3][i]\n",
        "  agglo_acc[i][5] = results_for_k_70[3][i]\n",
        "  agglo_acc[i][6] = results_for_k_80[3][i]\n",
        "  agglo_acc[i][7] = results_for_k_100[3][i]\n",
        "  agglo_acc[i][8] = results_for_k_150[3][i]\n",
        "  agglo_acc[i][9] = results_for_k_200[3][i]\n",
        "\n",
        "  agglo_NMI[i][0] = results_for_k_20[4][i]\n",
        "  agglo_NMI[i][1] = results_for_k_30[4][i]\n",
        "  agglo_NMI[i][2] = results_for_k_40[4][i]\n",
        "  agglo_NMI[i][3] = results_for_k_50[4][i]\n",
        "  agglo_NMI[i][4] = results_for_k_60[4][i]\n",
        "  agglo_NMI[i][5] = results_for_k_70[4][i]\n",
        "  agglo_NMI[i][6] = results_for_k_80[4][i]\n",
        "  agglo_NMI[i][7] = results_for_k_100[4][i]\n",
        "  agglo_NMI[i][8] = results_for_k_150[4][i]\n",
        "  agglo_NMI[i][9] = results_for_k_200[4][i]\n",
        "\n",
        "  agglo_ARI[i][0] = results_for_k_20[5][i]\n",
        "  agglo_ARI[i][1] = results_for_k_30[5][i]\n",
        "  agglo_ARI[i][2] = results_for_k_40[5][i]\n",
        "  agglo_ARI[i][3] = results_for_k_50[5][i]\n",
        "  agglo_ARI[i][4] = results_for_k_60[5][i]\n",
        "  agglo_ARI[i][5] = results_for_k_70[5][i]\n",
        "  agglo_ARI[i][6] = results_for_k_80[5][i]\n",
        "  agglo_ARI[i][7] = results_for_k_100[5][i]\n",
        "  agglo_ARI[i][8] = results_for_k_150[5][i]\n",
        "  agglo_ARI[i][9] = results_for_k_200[5][i]\n",
        "\n",
        "  kmeans_silhouette[i][0] = results_for_k_20[6][i]\n",
        "  kmeans_silhouette[i][1] = results_for_k_30[6][i]\n",
        "  kmeans_silhouette[i][2] = results_for_k_40[6][i]\n",
        "  kmeans_silhouette[i][3] = results_for_k_50[6][i]\n",
        "  kmeans_silhouette[i][4] = results_for_k_60[6][i]\n",
        "  kmeans_silhouette[i][5] = results_for_k_70[6][i]\n",
        "  kmeans_silhouette[i][6] = results_for_k_80[6][i]\n",
        "  kmeans_silhouette[i][7] = results_for_k_100[6][i]\n",
        "  kmeans_silhouette[i][8] = results_for_k_150[6][i]\n",
        "  kmeans_silhouette[i][9] = results_for_k_200[6][i]\n",
        "\n",
        "  agglo_silhouette[i][0] = results_for_k_20[7][i]\n",
        "  agglo_silhouette[i][1] = results_for_k_30[7][i]\n",
        "  agglo_silhouette[i][2] = results_for_k_40[7][i]\n",
        "  agglo_silhouette[i][3] = results_for_k_50[7][i]\n",
        "  agglo_silhouette[i][4] = results_for_k_60[7][i]\n",
        "  agglo_silhouette[i][5] = results_for_k_70[7][i]\n",
        "  agglo_silhouette[i][6] = results_for_k_80[7][i]\n",
        "  agglo_silhouette[i][7] = results_for_k_100[7][i]\n",
        "  agglo_silhouette[i][8] = results_for_k_150[7][i]\n",
        "  agglo_silhouette[i][9] = results_for_k_200[7][i]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "06bNou-EjDN2"
      },
      "outputs": [],
      "source": [
        "columns = ['20', '30', '40', '50', '60', '70', '80', '100', '150', '200']\n",
        "\n",
        "df = pd.DataFrame(kmeans_acc,\n",
        "                  columns=columns)\n",
        "boxplot = df.boxplot(column=columns, figsize=(14,11))  \n",
        "boxplot.set_ylabel('K-means accuracy on latent space')\n",
        "boxplot.set_xlabel('K')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WqpclwnajDN2"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(kmeans_NMI,\n",
        "                  columns=columns)\n",
        "boxplot = df.boxplot(column=columns, figsize=(14,11))  \n",
        "boxplot.set_ylabel('K-means NMI on latent space')\n",
        "boxplot.set_xlabel('K')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "i0Jov5dHjDN2"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(kmeans_ARI,\n",
        "                  columns=columns)\n",
        "boxplot = df.boxplot(column=columns, figsize=(14,11))  \n",
        "boxplot.set_ylabel('K-means ARI on latent space')\n",
        "boxplot.set_xlabel('K')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ID5QKqAAjDN3"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(agglo_acc,\n",
        "                  columns=columns)\n",
        "boxplot = df.boxplot(column=columns, figsize=(14,11))  \n",
        "boxplot.set_ylabel('Agglomerative clustering accuracy on latent space')\n",
        "boxplot.set_xlabel('K')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "r6PLfFLnjDN3"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(agglo_NMI,\n",
        "                  columns=columns)\n",
        "boxplot = df.boxplot(column=columns, figsize=(14,11))  \n",
        "boxplot.set_ylabel('Agglomerative clustering NMI on latent space')\n",
        "boxplot.set_xlabel('K')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_xSfZpQVjDN4"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(agglo_ARI,\n",
        "                  columns=columns)\n",
        "boxplot = df.boxplot(column=columns, figsize=(14,11))  \n",
        "boxplot.set_ylabel('Agglomerative clustering ARI on latent space')\n",
        "boxplot.set_xlabel('K')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "z-RXXNi2jDN4"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(kmeans_silhouette,\n",
        "                  columns=columns)\n",
        "boxplot = df.boxplot(column=columns, figsize=(14,11))  \n",
        "boxplot.set_ylabel('K-means silhouette score on latent space')\n",
        "boxplot.set_xlabel('K')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7j9d3a8ICyWF"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(agglo_silhouette,\n",
        "                  columns=columns)\n",
        "boxplot = df.boxplot(column=columns, figsize=(14,11))  \n",
        "boxplot.set_ylabel('Agglomerative clustering silhouette score on latent space')\n",
        "boxplot.set_xlabel('K')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "KgCqjRhN0WNe",
        "kztBh_520Bu2"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}