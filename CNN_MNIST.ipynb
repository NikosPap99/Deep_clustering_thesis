{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "KgCqjRhN0WNe"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgCqjRhN0WNe"
      },
      "source": [
        "# Imports and dataset download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugOtZey1tDFs"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import torch \n",
        "import sklearn\n",
        "import scipy\n",
        "from scipy import io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets\n",
        "import torch.utils.data as data\n",
        "from sklearn.metrics import accuracy_score, silhouette_score\n",
        "from sklearn.metrics.cluster import normalized_mutual_info_score, adjusted_rand_score\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn import metrics\n",
        "from sklearn import decomposition\n",
        "from sklearn import manifold\n",
        "from tqdm.notebook import trange, tqdm\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.manifold import TSNE\n",
        "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
        "from typing import Optional\n",
        "import seaborn as sns\n",
        "\n",
        "import copy\n",
        "import random\n",
        "import time\n",
        "from math import floor\n",
        "\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cuda = True if torch.cuda.is_available() else False\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "# torch.cuda.set_device(device)\n",
        "\n",
        "float_Tensor = torch.FloatTensor\n",
        "if cuda: float_Tensor = torch.cuda.FloatTensor\n",
        "\t\n",
        "drop_last = True\n",
        "shuffle = False\n",
        "IMG_SIZE = 28"
      ],
      "metadata": {
        "id": "NogYSYKMav-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_MNIST_subset_np(create_subset=False, data_per_pattern=1000):\n",
        "  notebook_path = os.path.abspath(\"CNN_MNIST.ipynb\")\n",
        "  labels_path = os.path.join(os.path.dirname(notebook_path), \"data/MNIST/MNIST.npy\")\n",
        "  data_path = os.path.join(os.path.dirname(notebook_path), \"data/MNIST/MNIST_labels.npy\")\n",
        "\n",
        "  if(create_subset):\n",
        "    transform = transforms.Compose([transforms.Resize(IMG_SIZE), transforms.ToTensor()])\n",
        "    train = datasets.MNIST(\"./data/MNIST\", train=True, download=True, transform=transform)\n",
        "    trainset = torch.utils.data.DataLoader(train, batch_size=1, shuffle=shuffle)\n",
        "    data = []\n",
        "    labels = []\n",
        "    dict_counter = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
        "    data_per_pattern = data_per_pattern\n",
        "\n",
        "    for datapoint in enumerate(trainset):\n",
        "      current_size = sum(dict_counter.values())\n",
        "      target_size = len(dict_counter) * data_per_pattern\n",
        "\n",
        "      if current_size >= target_size: break\n",
        "      batch_idx, (example_data, example_targets) = datapoint\n",
        "      example_targets = example_targets.item()\n",
        "\n",
        "      if example_targets in dict_counter and dict_counter[example_targets] < data_per_pattern:\n",
        "        dict_counter[example_targets] += 1\n",
        "        example_data = example_data.view(1, IMG_SIZE, IMG_SIZE)\n",
        "        data.append(np.array(example_data))\n",
        "        labels.append(example_targets)\n",
        "        \n",
        "        \n",
        "    \n",
        "    np.save(data_path, data)\n",
        "    np.save(labels_path, labels)\n",
        "\n",
        "  # Load data\n",
        "  data = np.load(data_path)\n",
        "  labels = np.load(labels_path)\n",
        "\n",
        "  return data, labels"
      ],
      "metadata": {
        "id": "eBlaUqShgJ6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_MNIST_subset_dataloader(batch_size=50, create_subset=False, data_per_pattern=1000):\n",
        "\tdata, labels = get_MNIST_subset_np(create_subset, data_per_pattern)\n",
        "\t# data = data.astype(float) / 255 # normalise\n",
        "\tdata_reshaped = data.reshape(len(data),-1) # reshape for k-means\n",
        "\tprint(data.shape)\n",
        "\tprint(data_reshaped.shape)\n",
        "\n",
        "\t# Convert to tensor dataset\n",
        "\tdata = torch.Tensor(data)\n",
        "\tlabels = torch.Tensor(labels)\n",
        "\tfinal_dataset = TensorDataset(data, labels)\n",
        "\tdataloader = DataLoader(final_dataset, batch_size=batch_size, drop_last=drop_last, shuffle=shuffle)\n",
        "\treturn data, data_reshaped, labels, dataloader"
      ],
      "metadata": {
        "id": "gKgy-wMHjAL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, images_reshaped, labels, dataloader = get_MNIST_subset_dataloader(200, True, 1000)"
      ],
      "metadata": {
        "id": "ro1KwYuVODTk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "202a4c72-399e-4649-d95c-4f5cf26f5399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 1, 28, 28)\n",
            "(10000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kztBh_520Bu2"
      },
      "source": [
        "# Autoencoder/Custom dataset/function declarations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkP3eEAYNQu-"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, datapoints, labels, transform=None):\n",
        "\n",
        "    self.datapoints = datapoints\n",
        "    self.labels = labels\n",
        "    self.transform = transform\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    sample = self.datapoints[index], self.labels[index]\n",
        "\n",
        "    if self.transform:\n",
        "      sample = self.transform(sample)\n",
        "\n",
        "    return sample\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.datapoints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3m1mESr0uiu"
      },
      "outputs": [],
      "source": [
        "# TODO: search hungarian algorithm\n",
        "def cluster_accuracy(y_true, y_predicted, cluster_number: Optional[int] = None):\n",
        "\t\"\"\"\n",
        "\tCalculate clustering accuracy after using the linear_sum_assignment function in SciPy to\n",
        "\tdetermine reassignments.\n",
        "\n",
        "\t:param y_true: list of true cluster numbers, an integer array 0-indexed\n",
        "\t:param y_predicted: list of predicted cluster numbers, an integer array 0-indexed\n",
        "\t:param cluster_number: number of clusters, if None then calculated from input\n",
        "\t:return: reassignment dictionary, clustering accuracy\n",
        "\t\"\"\"\n",
        "\tif cluster_number is None:\n",
        "\t\t# assume labels are 0-indexed\n",
        "\t\tcluster_number = (max(y_predicted.max(), y_true.max()) + 1)\n",
        "\tcount_matrix = np.zeros((cluster_number, cluster_number), dtype=np.int64)\n",
        "\tfor i in range(y_predicted.size):\n",
        "\t\tcount_matrix[y_predicted[i], y_true[i]] += 1\n",
        "\n",
        "\trow_ind, col_ind = linear_assignment(count_matrix.max() - count_matrix)\n",
        "\treassignment = dict(zip(row_ind, col_ind))\n",
        "\taccuracy = count_matrix[row_ind, col_ind].sum() / y_predicted.size\n",
        "\treturn reassignment, accuracy\n",
        "\n",
        "def transform_clusters_to_labels(clusters, labels):\n",
        "\t# Find the cluster ids (labels)\n",
        "\tc_ids = np.unique(clusters)\n",
        "\n",
        "\t# Dictionary to transform cluster label to real label\n",
        "\tdict_clusters_to_labels = dict()\n",
        "\n",
        "\t# For every cluster find the most frequent data label\n",
        "\tfor c_id in c_ids:\n",
        "\t\tindexes_of_cluster_i = np.where(c_id == clusters)\n",
        "\t\telements, frequency = np.unique(labels[indexes_of_cluster_i], return_counts=True)\n",
        "\t\ttrue_label_index = np.argmax(frequency)\n",
        "\t\ttrue_label = elements[true_label_index]\n",
        "\t\tdict_clusters_to_labels[c_id] = true_label\n",
        "\n",
        "\t# Change the cluster labels to real labels\n",
        "\tfor i, element in enumerate(clusters):\n",
        "\t\tclusters[i] = dict_clusters_to_labels[element]\n",
        "\n",
        "\treturn clusters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HQVkP882REx"
      },
      "outputs": [],
      "source": [
        "def check_number_of_representatives(Y): # TODO: check for empty clusters\n",
        "  cluster_map = {}\n",
        "\n",
        "  for element in Y:\n",
        "    if element not in cluster_map:\n",
        "      cluster_map[element] = 1\n",
        "    else:\n",
        "      cluster_map[element] += 1\n",
        "\n",
        "  for cluster_number in cluster_map.keys():\n",
        "    print(\"Cluster\", cluster_number, \":\", cluster_map[cluster_number], \"elements \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YD8nqJpeH0SN"
      },
      "outputs": [],
      "source": [
        "class Autoencoder(nn.Module):\n",
        "  \n",
        "    def __init__(self, input_dimension, hl1_neurons = 32, hl2_neurons = 64, hl3_neurons = 128, latent_dimension = 10):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder Model\n",
        "        # self.input_dimension = input_dimension\n",
        "        # self.hl1_neurons = hl1_neurons\n",
        "        # self.hl2_neurons = hl2_neurons\n",
        "        # self.latent_dimension = latent_dimension\n",
        "\n",
        "        self.encoder_model = nn.Sequential(\n",
        "          nn.Conv2d(1, hl1_neurons, kernel_size = 5, stride = 2, padding = 2),\n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.BatchNorm2d(hl1_neurons),\n",
        "\n",
        "          nn.Conv2d(hl1_neurons, hl2_neurons, kernel_size = 5, stride = 2, padding = 2),\n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.BatchNorm2d(hl2_neurons),\n",
        "\n",
        "          nn.Conv2d(hl2_neurons, hl3_neurons, kernel_size = 3, stride = 2, padding = 0),\n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.BatchNorm2d(hl3_neurons),\n",
        "\n",
        "          nn.Flatten(start_dim=1),\n",
        "          nn.Linear(hl3_neurons * 3 * 3, latent_dimension, bias=True), # latent_dimension * 3 * 3\n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.BatchNorm1d(latent_dimension)\n",
        "        )\n",
        "        \n",
        "        # Latent Space\n",
        "        # self.latent_space = nn.Sequential(\n",
        "        #   nn.Linear(self.hl2_neurons, self.latent_dimension),\n",
        "        #   nn.Sigmoid(),\n",
        "        #   nn.BatchNorm1d(self.latent_dimension),\n",
        "        # )\n",
        "\n",
        "        # Decoder Model\n",
        "        self.decoder_model = nn.Sequential(\n",
        "          nn.Linear(latent_dimension,  hl3_neurons * 3 * 3, bias=True),\n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.BatchNorm1d(hl3_neurons * 3 * 3),\n",
        "          nn.Unflatten(dim = 1, unflattened_size =  (hl3_neurons, 3, 3)),\n",
        "\n",
        "          nn.ConvTranspose2d(hl3_neurons, hl2_neurons, kernel_size = 3, stride = 2, padding = 0),\n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.BatchNorm2d(hl2_neurons),\n",
        "          \n",
        "          nn.ConvTranspose2d(hl2_neurons, hl1_neurons, kernel_size = 5, stride = 2, padding = 2, output_padding = 1),\n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.BatchNorm2d(hl1_neurons),\n",
        "          \n",
        "          nn.ConvTranspose2d(hl1_neurons, 1, kernel_size = 5, stride = 2, padding = 2, output_padding = 1),\n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.BatchNorm2d(1)\n",
        "        )\n",
        "  \n",
        "    def forward(self, x):\n",
        "      x = self.encoder_model(x)\n",
        "      x = self.decoder_model(x)\n",
        "      return x\n",
        "  \n",
        "    def encoder(self, x, device):\n",
        "      x = self.encoder_model(x.to(device=device))\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_autoencoder(device, dataloader, autoencoder, epochs):\n",
        "\tcriterion = nn.MSELoss().to(device)\n",
        "\toptimizer = optim.Adam(autoencoder.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\tloss_list = list()\n",
        "\tbatch_size = 200\n",
        "\t\n",
        "\tautoencoder.train()\n",
        "\tfor epoch in range(epochs):\n",
        "\t\tloss = 0\n",
        "\t\tfor batch_index, (batch, labels) in enumerate(dataloader):\n",
        "\t\t\t# Data to the GPU (cuda)\n",
        "\t\t\tbatch = Variable(batch.type(float_Tensor))\n",
        "\t\t\t\n",
        "\t\t\t# Indexes\n",
        "\t\t\tlow_index = batch_index * batch_size\n",
        "\t\t\thigh_index = (batch_index + 1) * batch_size\n",
        "\t\t\t\n",
        "\t\t\t# reset the gradients to zero\n",
        "\t\t\toptimizer.zero_grad()\n",
        "\t\t\tautoencoder.zero_grad()\n",
        "\n",
        "\t\t\t# compute reconstructions\n",
        "\t\t\treconstructions = autoencoder(batch)\n",
        "\n",
        "\t\t\t# compute training reconstruction loss\n",
        "\t\t\ttrain_loss = criterion(reconstructions, batch)\n",
        "\n",
        "\t\t\t# compute accumulated gradients\n",
        "\t\t\ttrain_loss.backward()\n",
        "\n",
        "\t\t\t# perform parameter update based on current gradients\n",
        "\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t# add the mini-batch training loss to epoch loss\n",
        "\t\t\tloss += train_loss.item()\n",
        "\n",
        "\t\tloss = loss / len(dataloader)\n",
        "\t\tloss_list.append(loss)\n",
        "\t\tprint(\"Epoch: {}/{}, Loss: {:.6f}\".format(epoch + 1, epochs, loss))\n",
        "\t\t\n",
        "\tautoencoder.eval()\n",
        "\treturn autoencoder, loss_list"
      ],
      "metadata": {
        "id": "9Jls3BixmHpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering"
      ],
      "metadata": {
        "id": "OL6sg4u2OW21"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmeiJnVkmWzg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c4f8ca3-afeb-4797-c783-bb62c7b67170"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K_means greedy accuracy score for 8 clusters on initial space: 0.5581\n",
            "K-means normalised mutual info score for 8 clusters on initial space: 0.4927704835704977\n",
            "K-means ARI for 8 clusters on initial space: 0.3578612674690206\n",
            "K-means silhouette score for 2 clusters on initial space: 0.07476837\n"
          ]
        }
      ],
      "source": [
        "kmeans_10_clusters = KMeans(n_clusters=10, n_init=10).fit(images_reshaped)\n",
        "retrieved_labels_10_clusters = transform_clusters_to_labels(kmeans_10_clusters.labels_, labels)\n",
        "\n",
        "print(\"K_means greedy accuracy score for 8 clusters on initial space:\",accuracy_score(labels, retrieved_labels_10_clusters))\n",
        "# print(\"K_means hungarian accuracy score for 8 clusters on initial space:\",cluster_accuracy(labels, kmeans_10_clusters.labels_)[1])\n",
        "print(\"K-means normalised mutual info score for 8 clusters on initial space:\",normalized_mutual_info_score(labels, kmeans_10_clusters.labels_))\n",
        "print(\"K-means ARI for 8 clusters on initial space:\",adjusted_rand_score(labels, kmeans_10_clusters.labels_))\n",
        "print(\"K-means silhouette score for 2 clusters on initial space:\",silhouette_score(images_reshaped, kmeans_10_clusters.labels_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6PgL45sT2V6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29239800-4c31-4ff3-acf9-8f05f23cde20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agglomerative clustering on initial space greedy accuracy score: 0.6553\n",
            "Normalised mutual info score on agglomerative clustering on initial space: 0.6629064246339391\n",
            "Agglomerative clustering ARI on initial space: 0.4938958717431919\n",
            "Agglomerative clustering silhouette score for 2 clusters on initial space: 0.040455837\n"
          ]
        }
      ],
      "source": [
        "aggloClustering_10_clusters = AgglomerativeClustering(n_clusters=10).fit(images_reshaped)\n",
        "agglo_retrieved_labels = transform_clusters_to_labels(aggloClustering_10_clusters.labels_, labels)\n",
        "\n",
        "# print the stats on agglomerative clustering\n",
        "\n",
        "print(\"Agglomerative clustering on initial space greedy accuracy score:\",accuracy_score(labels, agglo_retrieved_labels))\n",
        "# print(\"Agglomerative clustering on initial space hungarian accuracy score:\",cluster_accuracy(labels, aggloClustering_10_clusters.labels_)[1])\n",
        "print(\"Normalised mutual info score on agglomerative clustering on initial space:\",normalized_mutual_info_score(labels, aggloClustering_10_clusters.labels_))\n",
        "print(\"Agglomerative clustering ARI on initial space:\",adjusted_rand_score(labels, aggloClustering_10_clusters.labels_))\n",
        "print(\"Agglomerative clustering silhouette score for 2 clusters on initial space:\",silhouette_score(images_reshaped, aggloClustering_10_clusters.labels_))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_TSNE = TSNE(n_components=2, verbose=1, perplexity=20, n_iter=400, learning_rate='auto').fit_transform(images_reshaped)\n",
        "\n",
        "dataset_labels = [0,1,2,3,4,5,6,7,8,9]\n",
        "\n",
        "plt.figure(figsize=(16,10))\n",
        "scatterplot = plt.scatter(\n",
        "    x=data_TSNE[:,0], y=data_TSNE[:,1],\n",
        "    c=labels,\n",
        "    cmap=\"tab10\"\n",
        ")\n",
        "\n",
        "handles, _ = scatterplot.legend_elements(prop='colors')\n",
        "plt.legend(handles, dataset_labels)"
      ],
      "metadata": {
        "id": "kGrjKJ-WuJ6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xzNK8z6KdIo"
      },
      "outputs": [],
      "source": [
        "def run_experiment(K, dataloader, data_shape, datapoints, datapoints_reshaped, labels, epochs=30, hl1_neurons=32, hl2_neurons=64, hl3_neurons=128, latent_dimension=10):\n",
        "\n",
        "  print('Experiment results for k-means with k =', K, 'clusters:\\n')\n",
        "  print('Running k-means algorithm in order to get our pseudolabels: \\n')\n",
        "\n",
        "  kmeans_initial = KMeans(n_clusters=K, n_init=10).fit(datapoints_reshaped) \n",
        "  # cluster_centers = kmeans_initial.cluster_centers_\n",
        "  cluster_labels = kmeans_initial.labels_\n",
        "  real_labels = transform_clusters_to_labels(cluster_labels, labels)\n",
        "\n",
        "  # check TSNE representation of our features according to the k categories\n",
        "\n",
        "  # print(\"Creating TSNE representation of our features according to the k categories...\\n\")\n",
        "\n",
        "  # pseudolabels_indices = [i for i in range(K)]\n",
        "\n",
        "  # plt.figure(figsize=(16,10)) \n",
        "  # kmeans_scatterplot = plt.scatter(\n",
        "  #     x=data_TSNE[:,0], y=data_TSNE[:,1],\n",
        "  #     c=cluster_labels,\n",
        "  #     cmap=\"gist_rainbow\"\n",
        "  # )\n",
        "\n",
        "  # handles = kmeans_scatterplot.legend_elements(num=pseudolabels_indices)[0]\n",
        "  # plt.legend(handles, pseudolabels_indices)\n",
        "\n",
        "  # savestring = 'pseudolabels_TSNE_initial_space_k=' + str(K) + '.png'\n",
        "  # plt.savefig(savestring)\n",
        "\n",
        "  # find out accuracy of the algorithm in the initial space\n",
        "\n",
        "  # kmeans_initial_hungarian_acc = cluster_accuracy(labels, cluster_labels)[1]\n",
        "  kmeans_initial_NMI = normalized_mutual_info_score(labels, cluster_labels)\n",
        "  kmeans_initial_ARI = adjusted_rand_score(labels, cluster_labels)\n",
        "\n",
        "  print(\"K_means greedy accuracy score (initial space):\",accuracy_score(labels, real_labels))\n",
        "  # print(\"K_means hungarian accuracy score (initial space):\",kmeans_initial_hungarian_acc)\n",
        "  print(\"Normalised mutual info score (initial space):\",kmeans_initial_NMI)\n",
        "  print(\"ARI (initial space):\",kmeans_initial_ARI, \"\\n\")\n",
        "\n",
        "  # using the autoencoder model on our data\n",
        "\n",
        "  print('Using the autoencoder model on our data: \\n')\n",
        "\n",
        "  kmeans_accuracy_scores = []\n",
        "  k_means_silhouette_scores = []\n",
        "  k_means_cluster_error_scores = []\n",
        "  kmeans_NMI_scores = []\n",
        "  kmeans_ARI_scores = []\n",
        "  agglo_accuracy_scores = []\n",
        "  agglo_NMI_scores = []\n",
        "  agglo_ARI_scores = []\n",
        "  agglo_silhouette_scores = []\n",
        "  \n",
        "  for i in range(10):\n",
        "\n",
        "    print(\"ROUND NUMBER \",i + 1,\":\\n\")\n",
        "    autoencoder = Autoencoder(input_dimension=data_shape, hl1_neurons=hl1_neurons, hl2_neurons=hl2_neurons, hl3_neurons=hl3_neurons, latent_dimension=latent_dimension).to(device)\n",
        "    autoencoder, loss_list = train_autoencoder(device, dataloader, autoencoder, epochs)\n",
        "    latent_data = autoencoder.encoder(datapoints, device).cpu().detach().numpy() \n",
        "\n",
        "    # data_TSNE = TSNE(n_components=2, verbose=1, perplexity=20, n_iter=400, learning_rate='auto').fit_transform(latent_data)\n",
        "\n",
        "    # dataset_labels = [0,1,2,3,4,5,6,7,8,9]\n",
        "\n",
        "    # plt.figure(figsize=(16,10))\n",
        "    # scatterplot = plt.scatter(\n",
        "    #     x=data_TSNE[:,0], y=data_TSNE[:,1],\n",
        "    #     c=labels,\n",
        "    #     cmap=\"tab10\"\n",
        "    # )\n",
        "\n",
        "    # handles, _ = scatterplot.legend_elements(prop='colors')\n",
        "    # plt.legend(handles, dataset_labels)\n",
        "    \n",
        "    print(\"Creating a k-means model on latent data:\\n\")\n",
        "\n",
        "    # Maybe we need Standar Scaler\n",
        "\n",
        "    # std_scaler = StandardScaler()\n",
        "    # latent_data = std_scaler.fit_transform(latent_data)\n",
        "\n",
        "    # Clustering on transformed space\n",
        "\n",
        "    new_kmeans = KMeans(n_clusters=10, n_init=100).fit(latent_data)\n",
        "\n",
        "    kmeans_clusters = new_kmeans.labels_\n",
        "    kmeans_cluster_error = new_kmeans.inertia_\n",
        "    k_means_cluster_error_scores.append(kmeans_cluster_error)\n",
        "    kmeans_greedy_labels = transform_clusters_to_labels(kmeans_clusters, labels)\n",
        "\n",
        "    # print the stats on the transformed space\n",
        "    kmeans_greedy_acc = accuracy_score(labels, kmeans_greedy_labels)\n",
        "\n",
        "    # These metrics as input they use the output of the clustering algorithm\n",
        "    # kmeans_hungarian_acc = cluster_accuracy(labels, kmeans_clusters)[1]\n",
        "    kmeans_NMI = normalized_mutual_info_score(labels, kmeans_clusters)\n",
        "    kmeans_ARI = adjusted_rand_score(labels, kmeans_clusters)\n",
        "\n",
        "    kmeans_accuracy_scores.append(kmeans_greedy_acc)\n",
        "    kmeans_NMI_scores.append(kmeans_NMI)\n",
        "    kmeans_ARI_scores.append(kmeans_ARI)\n",
        "\n",
        "    k_means_silhouette_score = silhouette_score(latent_data, kmeans_clusters)\n",
        "    k_means_silhouette_scores.append(k_means_silhouette_score)\n",
        "\n",
        "    print(\"K-means with 10 clusters on latent space stats: \\n\")\n",
        "    print(\"K-means on latent space greedy accuracy score:\",kmeans_greedy_acc)\n",
        "    # print(\"K-means on latent space hungarian accuracy score:\",kmeans_hungarian_acc)\n",
        "    print(\"Normalised mutual info score on k-means on latent space:\", kmeans_NMI)\n",
        "    print(\"ARI score on k-means on latent space:\", kmeans_ARI)\n",
        "    print(\"K-means cluster error on latent space:\", kmeans_cluster_error)\n",
        "    print(\"K-means silhouette score on latent space:\", k_means_silhouette_score, \"\\n\")\n",
        "\n",
        "    # do agglomerative clustering on the transformed space\n",
        "\n",
        "    print(\"Doing agglomerative clustering on MLP output vectors:\\n\")\n",
        "    aggloClustering = AgglomerativeClustering(n_clusters=10).fit(latent_data)\n",
        "    aggloClustering_clusters = aggloClustering.labels_\n",
        "    agglo_greedy_labels = transform_clusters_to_labels(aggloClustering_clusters, labels)\n",
        "\n",
        "    # print the stats on agglomerative clustering\n",
        "    agglo_greedy_acc = accuracy_score(labels, agglo_greedy_labels)\n",
        "    # agglo_hungarian_acc = cluster_accuracy(labels, aggloClustering_clusters)[1]\n",
        "    agglo_NMI = normalized_mutual_info_score(labels, aggloClustering_clusters)\n",
        "    agglo_ARI = adjusted_rand_score(labels, aggloClustering_clusters)\n",
        "\n",
        "    agglo_accuracy_scores.append(agglo_greedy_acc)\n",
        "    agglo_NMI_scores.append(agglo_NMI)\n",
        "    agglo_ARI_scores.append(agglo_ARI)\n",
        "\n",
        "    agglo_silhouette_score = silhouette_score(latent_data, aggloClustering_clusters)\n",
        "    agglo_silhouette_scores.append(agglo_silhouette_score)\n",
        "\n",
        "    print(\"Agglomerative clustering on latent space greedy accuracy score:\", agglo_greedy_acc)\n",
        "    # print(\"Agglomerative clustering on latent space hungarian accuracy score:\", agglo_hungarian_acc)\n",
        "    print(\"Normalised mutual info score on agglomerative clustering on latent space:\",agglo_NMI, \"\\n\")\n",
        "    print(\"ARI score on agglomerative clustering on latent space:\", agglo_ARI, \"\\n\")\n",
        "\n",
        "  print(\"Average k-means accuracy score at latent space:\", sum(kmeans_accuracy_scores) / len(kmeans_accuracy_scores), \"\\n\")\n",
        "  print(\"Average k-means NMI score at latent space:\", sum(kmeans_NMI_scores) / len(kmeans_NMI_scores), \"\\n\")\n",
        "  print(\"Average k-means ARI score at latent space:\", sum(kmeans_ARI_scores) / len(kmeans_ARI_scores), \"\\n\")\n",
        "  print(\"Average agglomerative clustering accuracy score at latent space:\", sum(agglo_accuracy_scores) / len(agglo_accuracy_scores), \"\\n\")\n",
        "  print(\"Average agglomerative clustering NMI score at latent space:\", sum(agglo_NMI_scores) / len(agglo_NMI_scores), \"\\n\")\n",
        "  print(\"Average agglomerative clustering ARI score at latent space:\", sum(agglo_ARI_scores) / len(agglo_ARI_scores), \"\\n\")\n",
        "  print(\"Average k-means silhouette score on latent space:\", sum(k_means_silhouette_scores) / len(k_means_silhouette_scores), \"\\n\")\n",
        "  print(\"Average k-means cluster error on latent space:\", sum(k_means_cluster_error_scores) / len(k_means_cluster_error_scores), \"\\n\")\n",
        "\n",
        "  return [ kmeans_accuracy_scores, kmeans_NMI_scores, kmeans_ARI_scores, agglo_accuracy_scores, agglo_NMI_scores, agglo_ARI_scores, k_means_silhouette_scores, agglo_silhouette_scores]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_for_k_20 = run_experiment(20, dataloader, 784, datapoints = images, datapoints_reshaped = images_reshaped, labels = labels, epochs=100, hl1_neurons=32, hl2_neurons=64, hl3_neurons=128, latent_dimension=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqu4m7YvR5tA",
        "outputId": "7f6fb5b4-f68f-48cf-8ffb-2a66ec510e11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment results for k-means with k = 20 clusters:\n",
            "\n",
            "Running k-means algorithm in order to get our pseudolabels: \n",
            "\n",
            "K_means greedy accuracy score (initial space): 0.7104\n",
            "Normalised mutual info score (initial space): 0.6094772166486544\n",
            "ARI (initial space): 0.5261557817034651 \n",
            "\n",
            "Using the autoencoder model on our data: \n",
            "\n",
            "ROUND NUMBER  1 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.937462\n",
            "Epoch: 2/100, Loss: 0.732700\n",
            "Epoch: 3/100, Loss: 0.650502\n",
            "Epoch: 4/100, Loss: 0.610216\n",
            "Epoch: 5/100, Loss: 0.586235\n",
            "Epoch: 6/100, Loss: 0.568578\n",
            "Epoch: 7/100, Loss: 0.554025\n",
            "Epoch: 8/100, Loss: 0.541302\n",
            "Epoch: 9/100, Loss: 0.529806\n",
            "Epoch: 10/100, Loss: 0.519195\n",
            "Epoch: 11/100, Loss: 0.509235\n",
            "Epoch: 12/100, Loss: 0.499784\n",
            "Epoch: 13/100, Loss: 0.490734\n",
            "Epoch: 14/100, Loss: 0.482018\n",
            "Epoch: 15/100, Loss: 0.473590\n",
            "Epoch: 16/100, Loss: 0.465421\n",
            "Epoch: 17/100, Loss: 0.457479\n",
            "Epoch: 18/100, Loss: 0.449743\n",
            "Epoch: 19/100, Loss: 0.442192\n",
            "Epoch: 20/100, Loss: 0.434822\n",
            "Epoch: 21/100, Loss: 0.427624\n",
            "Epoch: 22/100, Loss: 0.420577\n",
            "Epoch: 23/100, Loss: 0.413670\n",
            "Epoch: 24/100, Loss: 0.406894\n",
            "Epoch: 25/100, Loss: 0.400247\n",
            "Epoch: 26/100, Loss: 0.393722\n",
            "Epoch: 27/100, Loss: 0.387318\n",
            "Epoch: 28/100, Loss: 0.381019\n",
            "Epoch: 29/100, Loss: 0.374826\n",
            "Epoch: 30/100, Loss: 0.368731\n",
            "Epoch: 31/100, Loss: 0.362732\n",
            "Epoch: 32/100, Loss: 0.356832\n",
            "Epoch: 33/100, Loss: 0.351021\n",
            "Epoch: 34/100, Loss: 0.345299\n",
            "Epoch: 35/100, Loss: 0.339653\n",
            "Epoch: 36/100, Loss: 0.334089\n",
            "Epoch: 37/100, Loss: 0.328603\n",
            "Epoch: 38/100, Loss: 0.323194\n",
            "Epoch: 39/100, Loss: 0.317859\n",
            "Epoch: 40/100, Loss: 0.312595\n",
            "Epoch: 41/100, Loss: 0.307397\n",
            "Epoch: 42/100, Loss: 0.302269\n",
            "Epoch: 43/100, Loss: 0.297207\n",
            "Epoch: 44/100, Loss: 0.292209\n",
            "Epoch: 45/100, Loss: 0.287275\n",
            "Epoch: 46/100, Loss: 0.282406\n",
            "Epoch: 47/100, Loss: 0.277600\n",
            "Epoch: 48/100, Loss: 0.272854\n",
            "Epoch: 49/100, Loss: 0.268165\n",
            "Epoch: 50/100, Loss: 0.263537\n",
            "Epoch: 51/100, Loss: 0.258966\n",
            "Epoch: 52/100, Loss: 0.254448\n",
            "Epoch: 53/100, Loss: 0.249988\n",
            "Epoch: 54/100, Loss: 0.245584\n",
            "Epoch: 55/100, Loss: 0.241237\n",
            "Epoch: 56/100, Loss: 0.236943\n",
            "Epoch: 57/100, Loss: 0.232701\n",
            "Epoch: 58/100, Loss: 0.228510\n",
            "Epoch: 59/100, Loss: 0.224368\n",
            "Epoch: 60/100, Loss: 0.220283\n",
            "Epoch: 61/100, Loss: 0.216249\n",
            "Epoch: 62/100, Loss: 0.212266\n",
            "Epoch: 63/100, Loss: 0.208334\n",
            "Epoch: 64/100, Loss: 0.204452\n",
            "Epoch: 65/100, Loss: 0.200622\n",
            "Epoch: 66/100, Loss: 0.196841\n",
            "Epoch: 67/100, Loss: 0.193107\n",
            "Epoch: 68/100, Loss: 0.189422\n",
            "Epoch: 69/100, Loss: 0.185785\n",
            "Epoch: 70/100, Loss: 0.182197\n",
            "Epoch: 71/100, Loss: 0.178655\n",
            "Epoch: 72/100, Loss: 0.175165\n",
            "Epoch: 73/100, Loss: 0.171718\n",
            "Epoch: 74/100, Loss: 0.168317\n",
            "Epoch: 75/100, Loss: 0.164962\n",
            "Epoch: 76/100, Loss: 0.161652\n",
            "Epoch: 77/100, Loss: 0.158387\n",
            "Epoch: 78/100, Loss: 0.155172\n",
            "Epoch: 79/100, Loss: 0.151999\n",
            "Epoch: 80/100, Loss: 0.148876\n",
            "Epoch: 81/100, Loss: 0.145796\n",
            "Epoch: 82/100, Loss: 0.142764\n",
            "Epoch: 83/100, Loss: 0.139776\n",
            "Epoch: 84/100, Loss: 0.136832\n",
            "Epoch: 85/100, Loss: 0.133935\n",
            "Epoch: 86/100, Loss: 0.131093\n",
            "Epoch: 87/100, Loss: 0.128302\n",
            "Epoch: 88/100, Loss: 0.125562\n",
            "Epoch: 89/100, Loss: 0.122872\n",
            "Epoch: 90/100, Loss: 0.120233\n",
            "Epoch: 91/100, Loss: 0.117636\n",
            "Epoch: 92/100, Loss: 0.115052\n",
            "Epoch: 93/100, Loss: 0.112517\n",
            "Epoch: 94/100, Loss: 0.110029\n",
            "Epoch: 95/100, Loss: 0.107566\n",
            "Epoch: 96/100, Loss: 0.105130\n",
            "Epoch: 97/100, Loss: 0.102718\n",
            "Epoch: 98/100, Loss: 0.100340\n",
            "Epoch: 99/100, Loss: 0.098028\n",
            "Epoch: 100/100, Loss: 0.095760\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6892\n",
            "Normalised mutual info score on k-means on latent space: 0.6163251347316518\n",
            "ARI score on k-means on latent space: 0.5224886950702909\n",
            "K-means cluster error on latent space: 42711.15234375\n",
            "K-means silhouette score on latent space: 0.18999663 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7219\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.7016933571401376 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5802348085611322 \n",
            "\n",
            "ROUND NUMBER  2 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.915595\n",
            "Epoch: 2/100, Loss: 0.711325\n",
            "Epoch: 3/100, Loss: 0.640284\n",
            "Epoch: 4/100, Loss: 0.606993\n",
            "Epoch: 5/100, Loss: 0.584834\n",
            "Epoch: 6/100, Loss: 0.567638\n",
            "Epoch: 7/100, Loss: 0.553237\n",
            "Epoch: 8/100, Loss: 0.540593\n",
            "Epoch: 9/100, Loss: 0.529110\n",
            "Epoch: 10/100, Loss: 0.518457\n",
            "Epoch: 11/100, Loss: 0.508446\n",
            "Epoch: 12/100, Loss: 0.498939\n",
            "Epoch: 13/100, Loss: 0.489856\n",
            "Epoch: 14/100, Loss: 0.481120\n",
            "Epoch: 15/100, Loss: 0.472679\n",
            "Epoch: 16/100, Loss: 0.464505\n",
            "Epoch: 17/100, Loss: 0.456552\n",
            "Epoch: 18/100, Loss: 0.448809\n",
            "Epoch: 19/100, Loss: 0.441260\n",
            "Epoch: 20/100, Loss: 0.433884\n",
            "Epoch: 21/100, Loss: 0.426672\n",
            "Epoch: 22/100, Loss: 0.419611\n",
            "Epoch: 23/100, Loss: 0.412694\n",
            "Epoch: 24/100, Loss: 0.405913\n",
            "Epoch: 25/100, Loss: 0.399266\n",
            "Epoch: 26/100, Loss: 0.392740\n",
            "Epoch: 27/100, Loss: 0.386332\n",
            "Epoch: 28/100, Loss: 0.380033\n",
            "Epoch: 29/100, Loss: 0.373837\n",
            "Epoch: 30/100, Loss: 0.367742\n",
            "Epoch: 31/100, Loss: 0.361745\n",
            "Epoch: 32/100, Loss: 0.355837\n",
            "Epoch: 33/100, Loss: 0.350019\n",
            "Epoch: 34/100, Loss: 0.344293\n",
            "Epoch: 35/100, Loss: 0.338647\n",
            "Epoch: 36/100, Loss: 0.333083\n",
            "Epoch: 37/100, Loss: 0.327598\n",
            "Epoch: 38/100, Loss: 0.322193\n",
            "Epoch: 39/100, Loss: 0.316860\n",
            "Epoch: 40/100, Loss: 0.311597\n",
            "Epoch: 41/100, Loss: 0.306407\n",
            "Epoch: 42/100, Loss: 0.301283\n",
            "Epoch: 43/100, Loss: 0.296226\n",
            "Epoch: 44/100, Loss: 0.291236\n",
            "Epoch: 45/100, Loss: 0.286308\n",
            "Epoch: 46/100, Loss: 0.281444\n",
            "Epoch: 47/100, Loss: 0.276642\n",
            "Epoch: 48/100, Loss: 0.271902\n",
            "Epoch: 49/100, Loss: 0.267216\n",
            "Epoch: 50/100, Loss: 0.262592\n",
            "Epoch: 51/100, Loss: 0.258027\n",
            "Epoch: 52/100, Loss: 0.253519\n",
            "Epoch: 53/100, Loss: 0.249065\n",
            "Epoch: 54/100, Loss: 0.244667\n",
            "Epoch: 55/100, Loss: 0.240326\n",
            "Epoch: 56/100, Loss: 0.236037\n",
            "Epoch: 57/100, Loss: 0.231803\n",
            "Epoch: 58/100, Loss: 0.227622\n",
            "Epoch: 59/100, Loss: 0.223491\n",
            "Epoch: 60/100, Loss: 0.219411\n",
            "Epoch: 61/100, Loss: 0.215383\n",
            "Epoch: 62/100, Loss: 0.211412\n",
            "Epoch: 63/100, Loss: 0.207487\n",
            "Epoch: 64/100, Loss: 0.203614\n",
            "Epoch: 65/100, Loss: 0.199788\n",
            "Epoch: 66/100, Loss: 0.196011\n",
            "Epoch: 67/100, Loss: 0.192282\n",
            "Epoch: 68/100, Loss: 0.188601\n",
            "Epoch: 69/100, Loss: 0.184968\n",
            "Epoch: 70/100, Loss: 0.181382\n",
            "Epoch: 71/100, Loss: 0.177845\n",
            "Epoch: 72/100, Loss: 0.174356\n",
            "Epoch: 73/100, Loss: 0.170915\n",
            "Epoch: 74/100, Loss: 0.167523\n",
            "Epoch: 75/100, Loss: 0.164175\n",
            "Epoch: 76/100, Loss: 0.160875\n",
            "Epoch: 77/100, Loss: 0.157626\n",
            "Epoch: 78/100, Loss: 0.154420\n",
            "Epoch: 79/100, Loss: 0.151259\n",
            "Epoch: 80/100, Loss: 0.148147\n",
            "Epoch: 81/100, Loss: 0.145083\n",
            "Epoch: 82/100, Loss: 0.142069\n",
            "Epoch: 83/100, Loss: 0.139107\n",
            "Epoch: 84/100, Loss: 0.136193\n",
            "Epoch: 85/100, Loss: 0.133324\n",
            "Epoch: 86/100, Loss: 0.130493\n",
            "Epoch: 87/100, Loss: 0.127718\n",
            "Epoch: 88/100, Loss: 0.124994\n",
            "Epoch: 89/100, Loss: 0.122269\n",
            "Epoch: 90/100, Loss: 0.119551\n",
            "Epoch: 91/100, Loss: 0.116875\n",
            "Epoch: 92/100, Loss: 0.114267\n",
            "Epoch: 93/100, Loss: 0.111736\n",
            "Epoch: 94/100, Loss: 0.109267\n",
            "Epoch: 95/100, Loss: 0.106844\n",
            "Epoch: 96/100, Loss: 0.104440\n",
            "Epoch: 97/100, Loss: 0.102054\n",
            "Epoch: 98/100, Loss: 0.099719\n",
            "Epoch: 99/100, Loss: 0.097416\n",
            "Epoch: 100/100, Loss: 0.095148\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6673\n",
            "Normalised mutual info score on k-means on latent space: 0.6150847042206928\n",
            "ARI score on k-means on latent space: 0.5182030206031809\n",
            "K-means cluster error on latent space: 44312.265625\n",
            "K-means silhouette score on latent space: 0.17926137 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6782\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.645865534146449 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.4977742473004661 \n",
            "\n",
            "ROUND NUMBER  3 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.917595\n",
            "Epoch: 2/100, Loss: 0.723606\n",
            "Epoch: 3/100, Loss: 0.648795\n",
            "Epoch: 4/100, Loss: 0.611222\n",
            "Epoch: 5/100, Loss: 0.587198\n",
            "Epoch: 6/100, Loss: 0.569384\n",
            "Epoch: 7/100, Loss: 0.554655\n",
            "Epoch: 8/100, Loss: 0.541800\n",
            "Epoch: 9/100, Loss: 0.530170\n",
            "Epoch: 10/100, Loss: 0.519406\n",
            "Epoch: 11/100, Loss: 0.509299\n",
            "Epoch: 12/100, Loss: 0.499716\n",
            "Epoch: 13/100, Loss: 0.490562\n",
            "Epoch: 14/100, Loss: 0.481766\n",
            "Epoch: 15/100, Loss: 0.473285\n",
            "Epoch: 16/100, Loss: 0.465072\n",
            "Epoch: 17/100, Loss: 0.457096\n",
            "Epoch: 18/100, Loss: 0.449329\n",
            "Epoch: 19/100, Loss: 0.441761\n",
            "Epoch: 20/100, Loss: 0.434376\n",
            "Epoch: 21/100, Loss: 0.427161\n",
            "Epoch: 22/100, Loss: 0.420101\n",
            "Epoch: 23/100, Loss: 0.413178\n",
            "Epoch: 24/100, Loss: 0.406401\n",
            "Epoch: 25/100, Loss: 0.399745\n",
            "Epoch: 26/100, Loss: 0.393214\n",
            "Epoch: 27/100, Loss: 0.386799\n",
            "Epoch: 28/100, Loss: 0.380497\n",
            "Epoch: 29/100, Loss: 0.374299\n",
            "Epoch: 30/100, Loss: 0.368200\n",
            "Epoch: 31/100, Loss: 0.362200\n",
            "Epoch: 32/100, Loss: 0.356297\n",
            "Epoch: 33/100, Loss: 0.350479\n",
            "Epoch: 34/100, Loss: 0.344749\n",
            "Epoch: 35/100, Loss: 0.339105\n",
            "Epoch: 36/100, Loss: 0.333542\n",
            "Epoch: 37/100, Loss: 0.328058\n",
            "Epoch: 38/100, Loss: 0.322645\n",
            "Epoch: 39/100, Loss: 0.317306\n",
            "Epoch: 40/100, Loss: 0.312041\n",
            "Epoch: 41/100, Loss: 0.306844\n",
            "Epoch: 42/100, Loss: 0.301716\n",
            "Epoch: 43/100, Loss: 0.296654\n",
            "Epoch: 44/100, Loss: 0.291655\n",
            "Epoch: 45/100, Loss: 0.286721\n",
            "Epoch: 46/100, Loss: 0.281852\n",
            "Epoch: 47/100, Loss: 0.277047\n",
            "Epoch: 48/100, Loss: 0.272303\n",
            "Epoch: 49/100, Loss: 0.267619\n",
            "Epoch: 50/100, Loss: 0.262991\n",
            "Epoch: 51/100, Loss: 0.258421\n",
            "Epoch: 52/100, Loss: 0.253906\n",
            "Epoch: 53/100, Loss: 0.249450\n",
            "Epoch: 54/100, Loss: 0.245049\n",
            "Epoch: 55/100, Loss: 0.240703\n",
            "Epoch: 56/100, Loss: 0.236412\n",
            "Epoch: 57/100, Loss: 0.232173\n",
            "Epoch: 58/100, Loss: 0.227985\n",
            "Epoch: 59/100, Loss: 0.223852\n",
            "Epoch: 60/100, Loss: 0.219771\n",
            "Epoch: 61/100, Loss: 0.215739\n",
            "Epoch: 62/100, Loss: 0.211759\n",
            "Epoch: 63/100, Loss: 0.207829\n",
            "Epoch: 64/100, Loss: 0.203949\n",
            "Epoch: 65/100, Loss: 0.200119\n",
            "Epoch: 66/100, Loss: 0.196337\n",
            "Epoch: 67/100, Loss: 0.192603\n",
            "Epoch: 68/100, Loss: 0.188920\n",
            "Epoch: 69/100, Loss: 0.185281\n",
            "Epoch: 70/100, Loss: 0.181689\n",
            "Epoch: 71/100, Loss: 0.178147\n",
            "Epoch: 72/100, Loss: 0.174653\n",
            "Epoch: 73/100, Loss: 0.171208\n",
            "Epoch: 74/100, Loss: 0.167806\n",
            "Epoch: 75/100, Loss: 0.164450\n",
            "Epoch: 76/100, Loss: 0.161142\n",
            "Epoch: 77/100, Loss: 0.157878\n",
            "Epoch: 78/100, Loss: 0.154662\n",
            "Epoch: 79/100, Loss: 0.151490\n",
            "Epoch: 80/100, Loss: 0.148362\n",
            "Epoch: 81/100, Loss: 0.145280\n",
            "Epoch: 82/100, Loss: 0.142243\n",
            "Epoch: 83/100, Loss: 0.139248\n",
            "Epoch: 84/100, Loss: 0.136302\n",
            "Epoch: 85/100, Loss: 0.133400\n",
            "Epoch: 86/100, Loss: 0.130553\n",
            "Epoch: 87/100, Loss: 0.127746\n",
            "Epoch: 88/100, Loss: 0.124983\n",
            "Epoch: 89/100, Loss: 0.122266\n",
            "Epoch: 90/100, Loss: 0.119594\n",
            "Epoch: 91/100, Loss: 0.116971\n",
            "Epoch: 92/100, Loss: 0.114387\n",
            "Epoch: 93/100, Loss: 0.111851\n",
            "Epoch: 94/100, Loss: 0.109354\n",
            "Epoch: 95/100, Loss: 0.106889\n",
            "Epoch: 96/100, Loss: 0.104438\n",
            "Epoch: 97/100, Loss: 0.102030\n",
            "Epoch: 98/100, Loss: 0.099685\n",
            "Epoch: 99/100, Loss: 0.097391\n",
            "Epoch: 100/100, Loss: 0.095128\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.7076\n",
            "Normalised mutual info score on k-means on latent space: 0.6199986447234698\n",
            "ARI score on k-means on latent space: 0.5365364759481961\n",
            "K-means cluster error on latent space: 43137.10546875\n",
            "K-means silhouette score on latent space: 0.19088915 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.682\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6423109214976847 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5046714629967993 \n",
            "\n",
            "ROUND NUMBER  4 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.937117\n",
            "Epoch: 2/100, Loss: 0.734495\n",
            "Epoch: 3/100, Loss: 0.651047\n",
            "Epoch: 4/100, Loss: 0.611990\n",
            "Epoch: 5/100, Loss: 0.587449\n",
            "Epoch: 6/100, Loss: 0.569493\n",
            "Epoch: 7/100, Loss: 0.554856\n",
            "Epoch: 8/100, Loss: 0.542042\n",
            "Epoch: 9/100, Loss: 0.530413\n",
            "Epoch: 10/100, Loss: 0.519644\n",
            "Epoch: 11/100, Loss: 0.509531\n",
            "Epoch: 12/100, Loss: 0.499933\n",
            "Epoch: 13/100, Loss: 0.490767\n",
            "Epoch: 14/100, Loss: 0.481960\n",
            "Epoch: 15/100, Loss: 0.473457\n",
            "Epoch: 16/100, Loss: 0.465221\n",
            "Epoch: 17/100, Loss: 0.457227\n",
            "Epoch: 18/100, Loss: 0.449455\n",
            "Epoch: 19/100, Loss: 0.441880\n",
            "Epoch: 20/100, Loss: 0.434484\n",
            "Epoch: 21/100, Loss: 0.427262\n",
            "Epoch: 22/100, Loss: 0.420196\n",
            "Epoch: 23/100, Loss: 0.413273\n",
            "Epoch: 24/100, Loss: 0.406494\n",
            "Epoch: 25/100, Loss: 0.399841\n",
            "Epoch: 26/100, Loss: 0.393310\n",
            "Epoch: 27/100, Loss: 0.386894\n",
            "Epoch: 28/100, Loss: 0.380591\n",
            "Epoch: 29/100, Loss: 0.374390\n",
            "Epoch: 30/100, Loss: 0.368294\n",
            "Epoch: 31/100, Loss: 0.362297\n",
            "Epoch: 32/100, Loss: 0.356395\n",
            "Epoch: 33/100, Loss: 0.350582\n",
            "Epoch: 34/100, Loss: 0.344855\n",
            "Epoch: 35/100, Loss: 0.339212\n",
            "Epoch: 36/100, Loss: 0.333649\n",
            "Epoch: 37/100, Loss: 0.328160\n",
            "Epoch: 38/100, Loss: 0.322751\n",
            "Epoch: 39/100, Loss: 0.317416\n",
            "Epoch: 40/100, Loss: 0.312148\n",
            "Epoch: 41/100, Loss: 0.306954\n",
            "Epoch: 42/100, Loss: 0.301828\n",
            "Epoch: 43/100, Loss: 0.296771\n",
            "Epoch: 44/100, Loss: 0.291775\n",
            "Epoch: 45/100, Loss: 0.286846\n",
            "Epoch: 46/100, Loss: 0.281979\n",
            "Epoch: 47/100, Loss: 0.277173\n",
            "Epoch: 48/100, Loss: 0.272430\n",
            "Epoch: 49/100, Loss: 0.267745\n",
            "Epoch: 50/100, Loss: 0.263115\n",
            "Epoch: 51/100, Loss: 0.258546\n",
            "Epoch: 52/100, Loss: 0.254037\n",
            "Epoch: 53/100, Loss: 0.249580\n",
            "Epoch: 54/100, Loss: 0.245176\n",
            "Epoch: 55/100, Loss: 0.240827\n",
            "Epoch: 56/100, Loss: 0.236534\n",
            "Epoch: 57/100, Loss: 0.232295\n",
            "Epoch: 58/100, Loss: 0.228105\n",
            "Epoch: 59/100, Loss: 0.223972\n",
            "Epoch: 60/100, Loss: 0.219886\n",
            "Epoch: 61/100, Loss: 0.215852\n",
            "Epoch: 62/100, Loss: 0.211867\n",
            "Epoch: 63/100, Loss: 0.207934\n",
            "Epoch: 64/100, Loss: 0.204052\n",
            "Epoch: 65/100, Loss: 0.200219\n",
            "Epoch: 66/100, Loss: 0.196436\n",
            "Epoch: 67/100, Loss: 0.192702\n",
            "Epoch: 68/100, Loss: 0.189017\n",
            "Epoch: 69/100, Loss: 0.185380\n",
            "Epoch: 70/100, Loss: 0.181793\n",
            "Epoch: 71/100, Loss: 0.178253\n",
            "Epoch: 72/100, Loss: 0.174761\n",
            "Epoch: 73/100, Loss: 0.171315\n",
            "Epoch: 74/100, Loss: 0.167916\n",
            "Epoch: 75/100, Loss: 0.164565\n",
            "Epoch: 76/100, Loss: 0.161256\n",
            "Epoch: 77/100, Loss: 0.157992\n",
            "Epoch: 78/100, Loss: 0.154775\n",
            "Epoch: 79/100, Loss: 0.151604\n",
            "Epoch: 80/100, Loss: 0.148481\n",
            "Epoch: 81/100, Loss: 0.145400\n",
            "Epoch: 82/100, Loss: 0.142366\n",
            "Epoch: 83/100, Loss: 0.139377\n",
            "Epoch: 84/100, Loss: 0.136433\n",
            "Epoch: 85/100, Loss: 0.133537\n",
            "Epoch: 86/100, Loss: 0.130695\n",
            "Epoch: 87/100, Loss: 0.127905\n",
            "Epoch: 88/100, Loss: 0.125180\n",
            "Epoch: 89/100, Loss: 0.122515\n",
            "Epoch: 90/100, Loss: 0.119866\n",
            "Epoch: 91/100, Loss: 0.117221\n",
            "Epoch: 92/100, Loss: 0.114608\n",
            "Epoch: 93/100, Loss: 0.112033\n",
            "Epoch: 94/100, Loss: 0.109496\n",
            "Epoch: 95/100, Loss: 0.107010\n",
            "Epoch: 96/100, Loss: 0.104580\n",
            "Epoch: 97/100, Loss: 0.102201\n",
            "Epoch: 98/100, Loss: 0.099875\n",
            "Epoch: 99/100, Loss: 0.097591\n",
            "Epoch: 100/100, Loss: 0.095360\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6573\n",
            "Normalised mutual info score on k-means on latent space: 0.6025037672305157\n",
            "ARI score on k-means on latent space: 0.5003242752818581\n",
            "K-means cluster error on latent space: 44887.8828125\n",
            "K-means silhouette score on latent space: 0.19012293 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7419\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.7129697128649113 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.6059877224627012 \n",
            "\n",
            "ROUND NUMBER  5 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.899977\n",
            "Epoch: 2/100, Loss: 0.708063\n",
            "Epoch: 3/100, Loss: 0.646550\n",
            "Epoch: 4/100, Loss: 0.614606\n",
            "Epoch: 5/100, Loss: 0.591856\n",
            "Epoch: 6/100, Loss: 0.574109\n",
            "Epoch: 7/100, Loss: 0.559245\n",
            "Epoch: 8/100, Loss: 0.546138\n",
            "Epoch: 9/100, Loss: 0.534188\n",
            "Epoch: 10/100, Loss: 0.523096\n",
            "Epoch: 11/100, Loss: 0.512676\n",
            "Epoch: 12/100, Loss: 0.502820\n",
            "Epoch: 13/100, Loss: 0.493422\n",
            "Epoch: 14/100, Loss: 0.484418\n",
            "Epoch: 15/100, Loss: 0.475751\n",
            "Epoch: 16/100, Loss: 0.467374\n",
            "Epoch: 17/100, Loss: 0.459255\n",
            "Epoch: 18/100, Loss: 0.451375\n",
            "Epoch: 19/100, Loss: 0.443703\n",
            "Epoch: 20/100, Loss: 0.436217\n",
            "Epoch: 21/100, Loss: 0.428907\n",
            "Epoch: 22/100, Loss: 0.421764\n",
            "Epoch: 23/100, Loss: 0.414772\n",
            "Epoch: 24/100, Loss: 0.407922\n",
            "Epoch: 25/100, Loss: 0.401212\n",
            "Epoch: 26/100, Loss: 0.394627\n",
            "Epoch: 27/100, Loss: 0.388161\n",
            "Epoch: 28/100, Loss: 0.381807\n",
            "Epoch: 29/100, Loss: 0.375566\n",
            "Epoch: 30/100, Loss: 0.369424\n",
            "Epoch: 31/100, Loss: 0.363386\n",
            "Epoch: 32/100, Loss: 0.357443\n",
            "Epoch: 33/100, Loss: 0.351594\n",
            "Epoch: 34/100, Loss: 0.345836\n",
            "Epoch: 35/100, Loss: 0.340163\n",
            "Epoch: 36/100, Loss: 0.334569\n",
            "Epoch: 37/100, Loss: 0.329057\n",
            "Epoch: 38/100, Loss: 0.323623\n",
            "Epoch: 39/100, Loss: 0.318262\n",
            "Epoch: 40/100, Loss: 0.312978\n",
            "Epoch: 41/100, Loss: 0.307766\n",
            "Epoch: 42/100, Loss: 0.302622\n",
            "Epoch: 43/100, Loss: 0.297546\n",
            "Epoch: 44/100, Loss: 0.292536\n",
            "Epoch: 45/100, Loss: 0.287590\n",
            "Epoch: 46/100, Loss: 0.282712\n",
            "Epoch: 47/100, Loss: 0.277896\n",
            "Epoch: 48/100, Loss: 0.273140\n",
            "Epoch: 49/100, Loss: 0.268442\n",
            "Epoch: 50/100, Loss: 0.263804\n",
            "Epoch: 51/100, Loss: 0.259224\n",
            "Epoch: 52/100, Loss: 0.254703\n",
            "Epoch: 53/100, Loss: 0.250235\n",
            "Epoch: 54/100, Loss: 0.245824\n",
            "Epoch: 55/100, Loss: 0.241469\n",
            "Epoch: 56/100, Loss: 0.237166\n",
            "Epoch: 57/100, Loss: 0.232919\n",
            "Epoch: 58/100, Loss: 0.228723\n",
            "Epoch: 59/100, Loss: 0.224583\n",
            "Epoch: 60/100, Loss: 0.220491\n",
            "Epoch: 61/100, Loss: 0.216451\n",
            "Epoch: 62/100, Loss: 0.212462\n",
            "Epoch: 63/100, Loss: 0.208524\n",
            "Epoch: 64/100, Loss: 0.204634\n",
            "Epoch: 65/100, Loss: 0.200799\n",
            "Epoch: 66/100, Loss: 0.197012\n",
            "Epoch: 67/100, Loss: 0.193277\n",
            "Epoch: 68/100, Loss: 0.189586\n",
            "Epoch: 69/100, Loss: 0.185947\n",
            "Epoch: 70/100, Loss: 0.182357\n",
            "Epoch: 71/100, Loss: 0.178813\n",
            "Epoch: 72/100, Loss: 0.175312\n",
            "Epoch: 73/100, Loss: 0.171856\n",
            "Epoch: 74/100, Loss: 0.168444\n",
            "Epoch: 75/100, Loss: 0.165082\n",
            "Epoch: 76/100, Loss: 0.161770\n",
            "Epoch: 77/100, Loss: 0.158501\n",
            "Epoch: 78/100, Loss: 0.155285\n",
            "Epoch: 79/100, Loss: 0.152120\n",
            "Epoch: 80/100, Loss: 0.149001\n",
            "Epoch: 81/100, Loss: 0.145933\n",
            "Epoch: 82/100, Loss: 0.142914\n",
            "Epoch: 83/100, Loss: 0.139959\n",
            "Epoch: 84/100, Loss: 0.137066\n",
            "Epoch: 85/100, Loss: 0.134234\n",
            "Epoch: 86/100, Loss: 0.131424\n",
            "Epoch: 87/100, Loss: 0.128625\n",
            "Epoch: 88/100, Loss: 0.125862\n",
            "Epoch: 89/100, Loss: 0.123138\n",
            "Epoch: 90/100, Loss: 0.120400\n",
            "Epoch: 91/100, Loss: 0.117647\n",
            "Epoch: 92/100, Loss: 0.114967\n",
            "Epoch: 93/100, Loss: 0.112355\n",
            "Epoch: 94/100, Loss: 0.109819\n",
            "Epoch: 95/100, Loss: 0.107328\n",
            "Epoch: 96/100, Loss: 0.104881\n",
            "Epoch: 97/100, Loss: 0.102480\n",
            "Epoch: 98/100, Loss: 0.100125\n",
            "Epoch: 99/100, Loss: 0.097829\n",
            "Epoch: 100/100, Loss: 0.095578\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6988\n",
            "Normalised mutual info score on k-means on latent space: 0.63724622845473\n",
            "ARI score on k-means on latent space: 0.5396327030413364\n",
            "K-means cluster error on latent space: 40970.32421875\n",
            "K-means silhouette score on latent space: 0.17711915 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6535\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6683348858381664 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5263561690723023 \n",
            "\n",
            "ROUND NUMBER  6 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.937066\n",
            "Epoch: 2/100, Loss: 0.735769\n",
            "Epoch: 3/100, Loss: 0.654644\n",
            "Epoch: 4/100, Loss: 0.614911\n",
            "Epoch: 5/100, Loss: 0.589434\n",
            "Epoch: 6/100, Loss: 0.571069\n",
            "Epoch: 7/100, Loss: 0.556092\n",
            "Epoch: 8/100, Loss: 0.543080\n",
            "Epoch: 9/100, Loss: 0.531341\n",
            "Epoch: 10/100, Loss: 0.520516\n",
            "Epoch: 11/100, Loss: 0.510372\n",
            "Epoch: 12/100, Loss: 0.500766\n",
            "Epoch: 13/100, Loss: 0.491579\n",
            "Epoch: 14/100, Loss: 0.482754\n",
            "Epoch: 15/100, Loss: 0.474233\n",
            "Epoch: 16/100, Loss: 0.465986\n",
            "Epoch: 17/100, Loss: 0.457973\n",
            "Epoch: 18/100, Loss: 0.450172\n",
            "Epoch: 19/100, Loss: 0.442566\n",
            "Epoch: 20/100, Loss: 0.435141\n",
            "Epoch: 21/100, Loss: 0.427887\n",
            "Epoch: 22/100, Loss: 0.420786\n",
            "Epoch: 23/100, Loss: 0.413844\n",
            "Epoch: 24/100, Loss: 0.407036\n",
            "Epoch: 25/100, Loss: 0.400359\n",
            "Epoch: 26/100, Loss: 0.393808\n",
            "Epoch: 27/100, Loss: 0.387374\n",
            "Epoch: 28/100, Loss: 0.381053\n",
            "Epoch: 29/100, Loss: 0.374843\n",
            "Epoch: 30/100, Loss: 0.368741\n",
            "Epoch: 31/100, Loss: 0.362730\n",
            "Epoch: 32/100, Loss: 0.356816\n",
            "Epoch: 33/100, Loss: 0.350991\n",
            "Epoch: 34/100, Loss: 0.345255\n",
            "Epoch: 35/100, Loss: 0.339603\n",
            "Epoch: 36/100, Loss: 0.334029\n",
            "Epoch: 37/100, Loss: 0.328533\n",
            "Epoch: 38/100, Loss: 0.323113\n",
            "Epoch: 39/100, Loss: 0.317769\n",
            "Epoch: 40/100, Loss: 0.312501\n",
            "Epoch: 41/100, Loss: 0.307301\n",
            "Epoch: 42/100, Loss: 0.302175\n",
            "Epoch: 43/100, Loss: 0.297111\n",
            "Epoch: 44/100, Loss: 0.292114\n",
            "Epoch: 45/100, Loss: 0.287182\n",
            "Epoch: 46/100, Loss: 0.282312\n",
            "Epoch: 47/100, Loss: 0.277504\n",
            "Epoch: 48/100, Loss: 0.272757\n",
            "Epoch: 49/100, Loss: 0.268071\n",
            "Epoch: 50/100, Loss: 0.263442\n",
            "Epoch: 51/100, Loss: 0.258871\n",
            "Epoch: 52/100, Loss: 0.254354\n",
            "Epoch: 53/100, Loss: 0.249895\n",
            "Epoch: 54/100, Loss: 0.245493\n",
            "Epoch: 55/100, Loss: 0.241145\n",
            "Epoch: 56/100, Loss: 0.236851\n",
            "Epoch: 57/100, Loss: 0.232612\n",
            "Epoch: 58/100, Loss: 0.228423\n",
            "Epoch: 59/100, Loss: 0.224290\n",
            "Epoch: 60/100, Loss: 0.220208\n",
            "Epoch: 61/100, Loss: 0.216175\n",
            "Epoch: 62/100, Loss: 0.212198\n",
            "Epoch: 63/100, Loss: 0.208271\n",
            "Epoch: 64/100, Loss: 0.204391\n",
            "Epoch: 65/100, Loss: 0.200562\n",
            "Epoch: 66/100, Loss: 0.196782\n",
            "Epoch: 67/100, Loss: 0.193051\n",
            "Epoch: 68/100, Loss: 0.189365\n",
            "Epoch: 69/100, Loss: 0.185727\n",
            "Epoch: 70/100, Loss: 0.182135\n",
            "Epoch: 71/100, Loss: 0.178594\n",
            "Epoch: 72/100, Loss: 0.175101\n",
            "Epoch: 73/100, Loss: 0.171655\n",
            "Epoch: 74/100, Loss: 0.168259\n",
            "Epoch: 75/100, Loss: 0.164903\n",
            "Epoch: 76/100, Loss: 0.161597\n",
            "Epoch: 77/100, Loss: 0.158335\n",
            "Epoch: 78/100, Loss: 0.155120\n",
            "Epoch: 79/100, Loss: 0.151950\n",
            "Epoch: 80/100, Loss: 0.148822\n",
            "Epoch: 81/100, Loss: 0.145741\n",
            "Epoch: 82/100, Loss: 0.142708\n",
            "Epoch: 83/100, Loss: 0.139726\n",
            "Epoch: 84/100, Loss: 0.136793\n",
            "Epoch: 85/100, Loss: 0.133905\n",
            "Epoch: 86/100, Loss: 0.131065\n",
            "Epoch: 87/100, Loss: 0.128279\n",
            "Epoch: 88/100, Loss: 0.125546\n",
            "Epoch: 89/100, Loss: 0.122890\n",
            "Epoch: 90/100, Loss: 0.120297\n",
            "Epoch: 91/100, Loss: 0.117715\n",
            "Epoch: 92/100, Loss: 0.115136\n",
            "Epoch: 93/100, Loss: 0.112642\n",
            "Epoch: 94/100, Loss: 0.110157\n",
            "Epoch: 95/100, Loss: 0.107653\n",
            "Epoch: 96/100, Loss: 0.105150\n",
            "Epoch: 97/100, Loss: 0.102709\n",
            "Epoch: 98/100, Loss: 0.100331\n",
            "Epoch: 99/100, Loss: 0.098000\n",
            "Epoch: 100/100, Loss: 0.095717\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6513\n",
            "Normalised mutual info score on k-means on latent space: 0.6028115764984614\n",
            "ARI score on k-means on latent space: 0.48505527996895087\n",
            "K-means cluster error on latent space: 42940.1875\n",
            "K-means silhouette score on latent space: 0.16008633 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6374\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.644772638913702 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5074636000172021 \n",
            "\n",
            "ROUND NUMBER  7 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.925189\n",
            "Epoch: 2/100, Loss: 0.717251\n",
            "Epoch: 3/100, Loss: 0.643200\n",
            "Epoch: 4/100, Loss: 0.608058\n",
            "Epoch: 5/100, Loss: 0.585413\n",
            "Epoch: 6/100, Loss: 0.568188\n",
            "Epoch: 7/100, Loss: 0.553764\n",
            "Epoch: 8/100, Loss: 0.541040\n",
            "Epoch: 9/100, Loss: 0.529462\n",
            "Epoch: 10/100, Loss: 0.518711\n",
            "Epoch: 11/100, Loss: 0.508611\n",
            "Epoch: 12/100, Loss: 0.499024\n",
            "Epoch: 13/100, Loss: 0.489860\n",
            "Epoch: 14/100, Loss: 0.481050\n",
            "Epoch: 15/100, Loss: 0.472543\n",
            "Epoch: 16/100, Loss: 0.464305\n",
            "Epoch: 17/100, Loss: 0.456305\n",
            "Epoch: 18/100, Loss: 0.448522\n",
            "Epoch: 19/100, Loss: 0.440941\n",
            "Epoch: 20/100, Loss: 0.433543\n",
            "Epoch: 21/100, Loss: 0.426314\n",
            "Epoch: 22/100, Loss: 0.419241\n",
            "Epoch: 23/100, Loss: 0.412313\n",
            "Epoch: 24/100, Loss: 0.405527\n",
            "Epoch: 25/100, Loss: 0.398869\n",
            "Epoch: 26/100, Loss: 0.392335\n",
            "Epoch: 27/100, Loss: 0.385920\n",
            "Epoch: 28/100, Loss: 0.379617\n",
            "Epoch: 29/100, Loss: 0.373417\n",
            "Epoch: 30/100, Loss: 0.367322\n",
            "Epoch: 31/100, Loss: 0.361322\n",
            "Epoch: 32/100, Loss: 0.355415\n",
            "Epoch: 33/100, Loss: 0.349599\n",
            "Epoch: 34/100, Loss: 0.343873\n",
            "Epoch: 35/100, Loss: 0.338230\n",
            "Epoch: 36/100, Loss: 0.332665\n",
            "Epoch: 37/100, Loss: 0.327180\n",
            "Epoch: 38/100, Loss: 0.321771\n",
            "Epoch: 39/100, Loss: 0.316435\n",
            "Epoch: 40/100, Loss: 0.311174\n",
            "Epoch: 41/100, Loss: 0.305983\n",
            "Epoch: 42/100, Loss: 0.300861\n",
            "Epoch: 43/100, Loss: 0.295805\n",
            "Epoch: 44/100, Loss: 0.290813\n",
            "Epoch: 45/100, Loss: 0.285886\n",
            "Epoch: 46/100, Loss: 0.281020\n",
            "Epoch: 47/100, Loss: 0.276214\n",
            "Epoch: 48/100, Loss: 0.271473\n",
            "Epoch: 49/100, Loss: 0.266788\n",
            "Epoch: 50/100, Loss: 0.262160\n",
            "Epoch: 51/100, Loss: 0.257594\n",
            "Epoch: 52/100, Loss: 0.253085\n",
            "Epoch: 53/100, Loss: 0.248631\n",
            "Epoch: 54/100, Loss: 0.244233\n",
            "Epoch: 55/100, Loss: 0.239889\n",
            "Epoch: 56/100, Loss: 0.235601\n",
            "Epoch: 57/100, Loss: 0.231366\n",
            "Epoch: 58/100, Loss: 0.227185\n",
            "Epoch: 59/100, Loss: 0.223057\n",
            "Epoch: 60/100, Loss: 0.218979\n",
            "Epoch: 61/100, Loss: 0.214954\n",
            "Epoch: 62/100, Loss: 0.210981\n",
            "Epoch: 63/100, Loss: 0.207056\n",
            "Epoch: 64/100, Loss: 0.203187\n",
            "Epoch: 65/100, Loss: 0.199363\n",
            "Epoch: 66/100, Loss: 0.195591\n",
            "Epoch: 67/100, Loss: 0.191868\n",
            "Epoch: 68/100, Loss: 0.188195\n",
            "Epoch: 69/100, Loss: 0.184566\n",
            "Epoch: 70/100, Loss: 0.180988\n",
            "Epoch: 71/100, Loss: 0.177461\n",
            "Epoch: 72/100, Loss: 0.173983\n",
            "Epoch: 73/100, Loss: 0.170552\n",
            "Epoch: 74/100, Loss: 0.167167\n",
            "Epoch: 75/100, Loss: 0.163832\n",
            "Epoch: 76/100, Loss: 0.160549\n",
            "Epoch: 77/100, Loss: 0.157320\n",
            "Epoch: 78/100, Loss: 0.154140\n",
            "Epoch: 79/100, Loss: 0.151006\n",
            "Epoch: 80/100, Loss: 0.147915\n",
            "Epoch: 81/100, Loss: 0.144879\n",
            "Epoch: 82/100, Loss: 0.141903\n",
            "Epoch: 83/100, Loss: 0.138954\n",
            "Epoch: 84/100, Loss: 0.136031\n",
            "Epoch: 85/100, Loss: 0.133143\n",
            "Epoch: 86/100, Loss: 0.130292\n",
            "Epoch: 87/100, Loss: 0.127521\n",
            "Epoch: 88/100, Loss: 0.124831\n",
            "Epoch: 89/100, Loss: 0.122156\n",
            "Epoch: 90/100, Loss: 0.119449\n",
            "Epoch: 91/100, Loss: 0.116707\n",
            "Epoch: 92/100, Loss: 0.114019\n",
            "Epoch: 93/100, Loss: 0.111398\n",
            "Epoch: 94/100, Loss: 0.108840\n",
            "Epoch: 95/100, Loss: 0.106343\n",
            "Epoch: 96/100, Loss: 0.103898\n",
            "Epoch: 97/100, Loss: 0.101505\n",
            "Epoch: 98/100, Loss: 0.099149\n",
            "Epoch: 99/100, Loss: 0.096837\n",
            "Epoch: 100/100, Loss: 0.094567\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6495\n",
            "Normalised mutual info score on k-means on latent space: 0.5890456393946721\n",
            "ARI score on k-means on latent space: 0.47047011319127857\n",
            "K-means cluster error on latent space: 44949.9296875\n",
            "K-means silhouette score on latent space: 0.17723466 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6396\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6369241048680745 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.49861315307843074 \n",
            "\n",
            "ROUND NUMBER  8 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.907436\n",
            "Epoch: 2/100, Loss: 0.719203\n",
            "Epoch: 3/100, Loss: 0.645998\n",
            "Epoch: 4/100, Loss: 0.608917\n",
            "Epoch: 5/100, Loss: 0.585622\n",
            "Epoch: 6/100, Loss: 0.568169\n",
            "Epoch: 7/100, Loss: 0.553692\n",
            "Epoch: 8/100, Loss: 0.540997\n",
            "Epoch: 9/100, Loss: 0.529482\n",
            "Epoch: 10/100, Loss: 0.518801\n",
            "Epoch: 11/100, Loss: 0.508746\n",
            "Epoch: 12/100, Loss: 0.499191\n",
            "Epoch: 13/100, Loss: 0.490053\n",
            "Epoch: 14/100, Loss: 0.481267\n",
            "Epoch: 15/100, Loss: 0.472780\n",
            "Epoch: 16/100, Loss: 0.464559\n",
            "Epoch: 17/100, Loss: 0.456569\n",
            "Epoch: 18/100, Loss: 0.448796\n",
            "Epoch: 19/100, Loss: 0.441226\n",
            "Epoch: 20/100, Loss: 0.433837\n",
            "Epoch: 21/100, Loss: 0.426619\n",
            "Epoch: 22/100, Loss: 0.419552\n",
            "Epoch: 23/100, Loss: 0.412633\n",
            "Epoch: 24/100, Loss: 0.405854\n",
            "Epoch: 25/100, Loss: 0.399207\n",
            "Epoch: 26/100, Loss: 0.392682\n",
            "Epoch: 27/100, Loss: 0.386276\n",
            "Epoch: 28/100, Loss: 0.379980\n",
            "Epoch: 29/100, Loss: 0.373793\n",
            "Epoch: 30/100, Loss: 0.367709\n",
            "Epoch: 31/100, Loss: 0.361717\n",
            "Epoch: 32/100, Loss: 0.355822\n",
            "Epoch: 33/100, Loss: 0.350016\n",
            "Epoch: 34/100, Loss: 0.344295\n",
            "Epoch: 35/100, Loss: 0.338661\n",
            "Epoch: 36/100, Loss: 0.333108\n",
            "Epoch: 37/100, Loss: 0.327631\n",
            "Epoch: 38/100, Loss: 0.322234\n",
            "Epoch: 39/100, Loss: 0.316909\n",
            "Epoch: 40/100, Loss: 0.311654\n",
            "Epoch: 41/100, Loss: 0.306469\n",
            "Epoch: 42/100, Loss: 0.301352\n",
            "Epoch: 43/100, Loss: 0.296303\n",
            "Epoch: 44/100, Loss: 0.291318\n",
            "Epoch: 45/100, Loss: 0.286396\n",
            "Epoch: 46/100, Loss: 0.281536\n",
            "Epoch: 47/100, Loss: 0.276735\n",
            "Epoch: 48/100, Loss: 0.271996\n",
            "Epoch: 49/100, Loss: 0.267317\n",
            "Epoch: 50/100, Loss: 0.262697\n",
            "Epoch: 51/100, Loss: 0.258138\n",
            "Epoch: 52/100, Loss: 0.253633\n",
            "Epoch: 53/100, Loss: 0.249183\n",
            "Epoch: 54/100, Loss: 0.244788\n",
            "Epoch: 55/100, Loss: 0.240449\n",
            "Epoch: 56/100, Loss: 0.236168\n",
            "Epoch: 57/100, Loss: 0.231935\n",
            "Epoch: 58/100, Loss: 0.227755\n",
            "Epoch: 59/100, Loss: 0.223627\n",
            "Epoch: 60/100, Loss: 0.219551\n",
            "Epoch: 61/100, Loss: 0.215529\n",
            "Epoch: 62/100, Loss: 0.211557\n",
            "Epoch: 63/100, Loss: 0.207631\n",
            "Epoch: 64/100, Loss: 0.203760\n",
            "Epoch: 65/100, Loss: 0.199935\n",
            "Epoch: 66/100, Loss: 0.196160\n",
            "Epoch: 67/100, Loss: 0.192433\n",
            "Epoch: 68/100, Loss: 0.188756\n",
            "Epoch: 69/100, Loss: 0.185126\n",
            "Epoch: 70/100, Loss: 0.181543\n",
            "Epoch: 71/100, Loss: 0.178005\n",
            "Epoch: 72/100, Loss: 0.174517\n",
            "Epoch: 73/100, Loss: 0.171077\n",
            "Epoch: 74/100, Loss: 0.167684\n",
            "Epoch: 75/100, Loss: 0.164334\n",
            "Epoch: 76/100, Loss: 0.161029\n",
            "Epoch: 77/100, Loss: 0.157772\n",
            "Epoch: 78/100, Loss: 0.154561\n",
            "Epoch: 79/100, Loss: 0.151397\n",
            "Epoch: 80/100, Loss: 0.148283\n",
            "Epoch: 81/100, Loss: 0.145220\n",
            "Epoch: 82/100, Loss: 0.142204\n",
            "Epoch: 83/100, Loss: 0.139234\n",
            "Epoch: 84/100, Loss: 0.136323\n",
            "Epoch: 85/100, Loss: 0.133485\n",
            "Epoch: 86/100, Loss: 0.130716\n",
            "Epoch: 87/100, Loss: 0.127959\n",
            "Epoch: 88/100, Loss: 0.125183\n",
            "Epoch: 89/100, Loss: 0.122391\n",
            "Epoch: 90/100, Loss: 0.119635\n",
            "Epoch: 91/100, Loss: 0.116947\n",
            "Epoch: 92/100, Loss: 0.114323\n",
            "Epoch: 93/100, Loss: 0.111760\n",
            "Epoch: 94/100, Loss: 0.109251\n",
            "Epoch: 95/100, Loss: 0.106783\n",
            "Epoch: 96/100, Loss: 0.104351\n",
            "Epoch: 97/100, Loss: 0.101950\n",
            "Epoch: 98/100, Loss: 0.099577\n",
            "Epoch: 99/100, Loss: 0.097251\n",
            "Epoch: 100/100, Loss: 0.094968\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.707\n",
            "Normalised mutual info score on k-means on latent space: 0.6189498067696775\n",
            "ARI score on k-means on latent space: 0.5393513625340481\n",
            "K-means cluster error on latent space: 42735.78515625\n",
            "K-means silhouette score on latent space: 0.19641572 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7715\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.7323052545705373 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.6271130030307176 \n",
            "\n",
            "ROUND NUMBER  9 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.919873\n",
            "Epoch: 2/100, Loss: 0.729591\n",
            "Epoch: 3/100, Loss: 0.648538\n",
            "Epoch: 4/100, Loss: 0.609982\n",
            "Epoch: 5/100, Loss: 0.586254\n",
            "Epoch: 6/100, Loss: 0.568566\n",
            "Epoch: 7/100, Loss: 0.553967\n",
            "Epoch: 8/100, Loss: 0.541191\n",
            "Epoch: 9/100, Loss: 0.529616\n",
            "Epoch: 10/100, Loss: 0.518907\n",
            "Epoch: 11/100, Loss: 0.508857\n",
            "Epoch: 12/100, Loss: 0.499314\n",
            "Epoch: 13/100, Loss: 0.490182\n",
            "Epoch: 14/100, Loss: 0.481392\n",
            "Epoch: 15/100, Loss: 0.472897\n",
            "Epoch: 16/100, Loss: 0.464670\n",
            "Epoch: 17/100, Loss: 0.456683\n",
            "Epoch: 18/100, Loss: 0.448912\n",
            "Epoch: 19/100, Loss: 0.441343\n",
            "Epoch: 20/100, Loss: 0.433959\n",
            "Epoch: 21/100, Loss: 0.426744\n",
            "Epoch: 22/100, Loss: 0.419692\n",
            "Epoch: 23/100, Loss: 0.412789\n",
            "Epoch: 24/100, Loss: 0.406024\n",
            "Epoch: 25/100, Loss: 0.399388\n",
            "Epoch: 26/100, Loss: 0.392877\n",
            "Epoch: 27/100, Loss: 0.386484\n",
            "Epoch: 28/100, Loss: 0.380204\n",
            "Epoch: 29/100, Loss: 0.374029\n",
            "Epoch: 30/100, Loss: 0.367952\n",
            "Epoch: 31/100, Loss: 0.361974\n",
            "Epoch: 32/100, Loss: 0.356093\n",
            "Epoch: 33/100, Loss: 0.350300\n",
            "Epoch: 34/100, Loss: 0.344593\n",
            "Epoch: 35/100, Loss: 0.338970\n",
            "Epoch: 36/100, Loss: 0.333425\n",
            "Epoch: 37/100, Loss: 0.327957\n",
            "Epoch: 38/100, Loss: 0.322564\n",
            "Epoch: 39/100, Loss: 0.317242\n",
            "Epoch: 40/100, Loss: 0.311991\n",
            "Epoch: 41/100, Loss: 0.306811\n",
            "Epoch: 42/100, Loss: 0.301698\n",
            "Epoch: 43/100, Loss: 0.296651\n",
            "Epoch: 44/100, Loss: 0.291671\n",
            "Epoch: 45/100, Loss: 0.286754\n",
            "Epoch: 46/100, Loss: 0.281896\n",
            "Epoch: 47/100, Loss: 0.277096\n",
            "Epoch: 48/100, Loss: 0.272358\n",
            "Epoch: 49/100, Loss: 0.267682\n",
            "Epoch: 50/100, Loss: 0.263058\n",
            "Epoch: 51/100, Loss: 0.258495\n",
            "Epoch: 52/100, Loss: 0.253985\n",
            "Epoch: 53/100, Loss: 0.249530\n",
            "Epoch: 54/100, Loss: 0.245132\n",
            "Epoch: 55/100, Loss: 0.240788\n",
            "Epoch: 56/100, Loss: 0.236501\n",
            "Epoch: 57/100, Loss: 0.232262\n",
            "Epoch: 58/100, Loss: 0.228079\n",
            "Epoch: 59/100, Loss: 0.223948\n",
            "Epoch: 60/100, Loss: 0.219871\n",
            "Epoch: 61/100, Loss: 0.215840\n",
            "Epoch: 62/100, Loss: 0.211862\n",
            "Epoch: 63/100, Loss: 0.207934\n",
            "Epoch: 64/100, Loss: 0.204054\n",
            "Epoch: 65/100, Loss: 0.200225\n",
            "Epoch: 66/100, Loss: 0.196445\n",
            "Epoch: 67/100, Loss: 0.192718\n",
            "Epoch: 68/100, Loss: 0.189032\n",
            "Epoch: 69/100, Loss: 0.185399\n",
            "Epoch: 70/100, Loss: 0.181812\n",
            "Epoch: 71/100, Loss: 0.178275\n",
            "Epoch: 72/100, Loss: 0.174786\n",
            "Epoch: 73/100, Loss: 0.171341\n",
            "Epoch: 74/100, Loss: 0.167948\n",
            "Epoch: 75/100, Loss: 0.164601\n",
            "Epoch: 76/100, Loss: 0.161299\n",
            "Epoch: 77/100, Loss: 0.158039\n",
            "Epoch: 78/100, Loss: 0.154823\n",
            "Epoch: 79/100, Loss: 0.151657\n",
            "Epoch: 80/100, Loss: 0.148541\n",
            "Epoch: 81/100, Loss: 0.145478\n",
            "Epoch: 82/100, Loss: 0.142462\n",
            "Epoch: 83/100, Loss: 0.139495\n",
            "Epoch: 84/100, Loss: 0.136573\n",
            "Epoch: 85/100, Loss: 0.133689\n",
            "Epoch: 86/100, Loss: 0.130841\n",
            "Epoch: 87/100, Loss: 0.128000\n",
            "Epoch: 88/100, Loss: 0.125193\n",
            "Epoch: 89/100, Loss: 0.122452\n",
            "Epoch: 90/100, Loss: 0.119786\n",
            "Epoch: 91/100, Loss: 0.117155\n",
            "Epoch: 92/100, Loss: 0.114548\n",
            "Epoch: 93/100, Loss: 0.112006\n",
            "Epoch: 94/100, Loss: 0.109534\n",
            "Epoch: 95/100, Loss: 0.107073\n",
            "Epoch: 96/100, Loss: 0.104637\n",
            "Epoch: 97/100, Loss: 0.102267\n",
            "Epoch: 98/100, Loss: 0.099961\n",
            "Epoch: 99/100, Loss: 0.097720\n",
            "Epoch: 100/100, Loss: 0.095498\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6549\n",
            "Normalised mutual info score on k-means on latent space: 0.5971454588432423\n",
            "ARI score on k-means on latent space: 0.4920815817493894\n",
            "K-means cluster error on latent space: 45618.3984375\n",
            "K-means silhouette score on latent space: 0.18362083 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6611\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6672669825370637 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5212613477485364 \n",
            "\n",
            "ROUND NUMBER  10 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.887679\n",
            "Epoch: 2/100, Loss: 0.706285\n",
            "Epoch: 3/100, Loss: 0.641824\n",
            "Epoch: 4/100, Loss: 0.607577\n",
            "Epoch: 5/100, Loss: 0.584776\n",
            "Epoch: 6/100, Loss: 0.567512\n",
            "Epoch: 7/100, Loss: 0.553220\n",
            "Epoch: 8/100, Loss: 0.540620\n",
            "Epoch: 9/100, Loss: 0.529146\n",
            "Epoch: 10/100, Loss: 0.518490\n",
            "Epoch: 11/100, Loss: 0.508455\n",
            "Epoch: 12/100, Loss: 0.498925\n",
            "Epoch: 13/100, Loss: 0.489812\n",
            "Epoch: 14/100, Loss: 0.481057\n",
            "Epoch: 15/100, Loss: 0.472605\n",
            "Epoch: 16/100, Loss: 0.464419\n",
            "Epoch: 17/100, Loss: 0.456470\n",
            "Epoch: 18/100, Loss: 0.448743\n",
            "Epoch: 19/100, Loss: 0.441213\n",
            "Epoch: 20/100, Loss: 0.433863\n",
            "Epoch: 21/100, Loss: 0.426677\n",
            "Epoch: 22/100, Loss: 0.419645\n",
            "Epoch: 23/100, Loss: 0.412759\n",
            "Epoch: 24/100, Loss: 0.406006\n",
            "Epoch: 25/100, Loss: 0.399376\n",
            "Epoch: 26/100, Loss: 0.392870\n",
            "Epoch: 27/100, Loss: 0.386481\n",
            "Epoch: 28/100, Loss: 0.380200\n",
            "Epoch: 29/100, Loss: 0.374020\n",
            "Epoch: 30/100, Loss: 0.367938\n",
            "Epoch: 31/100, Loss: 0.361954\n",
            "Epoch: 32/100, Loss: 0.356063\n",
            "Epoch: 33/100, Loss: 0.350256\n",
            "Epoch: 34/100, Loss: 0.344540\n",
            "Epoch: 35/100, Loss: 0.338899\n",
            "Epoch: 36/100, Loss: 0.333343\n",
            "Epoch: 37/100, Loss: 0.327863\n",
            "Epoch: 38/100, Loss: 0.322460\n",
            "Epoch: 39/100, Loss: 0.317122\n",
            "Epoch: 40/100, Loss: 0.311859\n",
            "Epoch: 41/100, Loss: 0.306662\n",
            "Epoch: 42/100, Loss: 0.301535\n",
            "Epoch: 43/100, Loss: 0.296473\n",
            "Epoch: 44/100, Loss: 0.291477\n",
            "Epoch: 45/100, Loss: 0.286546\n",
            "Epoch: 46/100, Loss: 0.281676\n",
            "Epoch: 47/100, Loss: 0.276871\n",
            "Epoch: 48/100, Loss: 0.272128\n",
            "Epoch: 49/100, Loss: 0.267444\n",
            "Epoch: 50/100, Loss: 0.262816\n",
            "Epoch: 51/100, Loss: 0.258247\n",
            "Epoch: 52/100, Loss: 0.253738\n",
            "Epoch: 53/100, Loss: 0.249278\n",
            "Epoch: 54/100, Loss: 0.244878\n",
            "Epoch: 55/100, Loss: 0.240533\n",
            "Epoch: 56/100, Loss: 0.236242\n",
            "Epoch: 57/100, Loss: 0.232004\n",
            "Epoch: 58/100, Loss: 0.227819\n",
            "Epoch: 59/100, Loss: 0.223688\n",
            "Epoch: 60/100, Loss: 0.219605\n",
            "Epoch: 61/100, Loss: 0.215572\n",
            "Epoch: 62/100, Loss: 0.211594\n",
            "Epoch: 63/100, Loss: 0.207670\n",
            "Epoch: 64/100, Loss: 0.203794\n",
            "Epoch: 65/100, Loss: 0.199967\n",
            "Epoch: 66/100, Loss: 0.196189\n",
            "Epoch: 67/100, Loss: 0.192464\n",
            "Epoch: 68/100, Loss: 0.188786\n",
            "Epoch: 69/100, Loss: 0.185155\n",
            "Epoch: 70/100, Loss: 0.181571\n",
            "Epoch: 71/100, Loss: 0.178032\n",
            "Epoch: 72/100, Loss: 0.174545\n",
            "Epoch: 73/100, Loss: 0.171105\n",
            "Epoch: 74/100, Loss: 0.167713\n",
            "Epoch: 75/100, Loss: 0.164366\n",
            "Epoch: 76/100, Loss: 0.161064\n",
            "Epoch: 77/100, Loss: 0.157811\n",
            "Epoch: 78/100, Loss: 0.154609\n",
            "Epoch: 79/100, Loss: 0.151450\n",
            "Epoch: 80/100, Loss: 0.148342\n",
            "Epoch: 81/100, Loss: 0.145272\n",
            "Epoch: 82/100, Loss: 0.142257\n",
            "Epoch: 83/100, Loss: 0.139295\n",
            "Epoch: 84/100, Loss: 0.136381\n",
            "Epoch: 85/100, Loss: 0.133510\n",
            "Epoch: 86/100, Loss: 0.130701\n",
            "Epoch: 87/100, Loss: 0.127978\n",
            "Epoch: 88/100, Loss: 0.125320\n",
            "Epoch: 89/100, Loss: 0.122693\n",
            "Epoch: 90/100, Loss: 0.120011\n",
            "Epoch: 91/100, Loss: 0.117236\n",
            "Epoch: 92/100, Loss: 0.114525\n",
            "Epoch: 93/100, Loss: 0.111921\n",
            "Epoch: 94/100, Loss: 0.109396\n",
            "Epoch: 95/100, Loss: 0.106929\n",
            "Epoch: 96/100, Loss: 0.104524\n",
            "Epoch: 97/100, Loss: 0.102169\n",
            "Epoch: 98/100, Loss: 0.099841\n",
            "Epoch: 99/100, Loss: 0.097537\n",
            "Epoch: 100/100, Loss: 0.095276\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.7075\n",
            "Normalised mutual info score on k-means on latent space: 0.6271054389026743\n",
            "ARI score on k-means on latent space: 0.5347522663734068\n",
            "K-means cluster error on latent space: 44102.06640625\n",
            "K-means silhouette score on latent space: 0.1922808 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7435\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.7195814444738494 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.59016360283909 \n",
            "\n",
            "Average k-means accuracy score at latent space: 0.67904 \n",
            "\n",
            "Average k-means NMI score at latent space: 0.6126216399769788 \n",
            "\n",
            "Average k-means ARI score at latent space: 0.5138895773761937 \n",
            "\n",
            "Average agglomerative clustering accuracy score at latent space: 0.69306 \n",
            "\n",
            "Average agglomerative clustering NMI score at latent space: 0.6772024836850576 \n",
            "\n",
            "Average agglomerative clustering ARI score at latent space: 0.5459639117107378 \n",
            "\n",
            "Average k-means silhouette score on latent space: 0.18370275795459748 \n",
            "\n",
            "Average k-means cluster error on latent space: 43636.509765625 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_for_k_30 = run_experiment(30, dataloader, 784, datapoints = images, datapoints_reshaped = images_reshaped, labels = labels, epochs=100, hl1_neurons=32, hl2_neurons=64, hl3_neurons=128, latent_dimension=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-IJqn7D7yUZ",
        "outputId": "7a5f3214-db0c-43f3-a70f-07cf9651b40e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment results for k-means with k = 30 clusters:\n",
            "\n",
            "Running k-means algorithm in order to get our pseudolabels: \n",
            "\n",
            "K_means greedy accuracy score (initial space): 0.7421\n",
            "Normalised mutual info score (initial space): 0.6449980640633176\n",
            "ARI (initial space): 0.5652713133960403 \n",
            "\n",
            "Using the autoencoder model on our data: \n",
            "\n",
            "ROUND NUMBER  1 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.932937\n",
            "Epoch: 2/100, Loss: 0.728996\n",
            "Epoch: 3/100, Loss: 0.648768\n",
            "Epoch: 4/100, Loss: 0.611429\n",
            "Epoch: 5/100, Loss: 0.588258\n",
            "Epoch: 6/100, Loss: 0.570784\n",
            "Epoch: 7/100, Loss: 0.556252\n",
            "Epoch: 8/100, Loss: 0.543443\n",
            "Epoch: 9/100, Loss: 0.531787\n",
            "Epoch: 10/100, Loss: 0.520973\n",
            "Epoch: 11/100, Loss: 0.510806\n",
            "Epoch: 12/100, Loss: 0.501148\n",
            "Epoch: 13/100, Loss: 0.491913\n",
            "Epoch: 14/100, Loss: 0.483033\n",
            "Epoch: 15/100, Loss: 0.474465\n",
            "Epoch: 16/100, Loss: 0.466167\n",
            "Epoch: 17/100, Loss: 0.458105\n",
            "Epoch: 18/100, Loss: 0.450264\n",
            "Epoch: 19/100, Loss: 0.442617\n",
            "Epoch: 20/100, Loss: 0.435153\n",
            "Epoch: 21/100, Loss: 0.427866\n",
            "Epoch: 22/100, Loss: 0.420744\n",
            "Epoch: 23/100, Loss: 0.413768\n",
            "Epoch: 24/100, Loss: 0.406933\n",
            "Epoch: 25/100, Loss: 0.400228\n",
            "Epoch: 26/100, Loss: 0.393652\n",
            "Epoch: 27/100, Loss: 0.387194\n",
            "Epoch: 28/100, Loss: 0.380849\n",
            "Epoch: 29/100, Loss: 0.374611\n",
            "Epoch: 30/100, Loss: 0.368477\n",
            "Epoch: 31/100, Loss: 0.362441\n",
            "Epoch: 32/100, Loss: 0.356503\n",
            "Epoch: 33/100, Loss: 0.350661\n",
            "Epoch: 34/100, Loss: 0.344911\n",
            "Epoch: 35/100, Loss: 0.339245\n",
            "Epoch: 36/100, Loss: 0.333658\n",
            "Epoch: 37/100, Loss: 0.328154\n",
            "Epoch: 38/100, Loss: 0.322729\n",
            "Epoch: 39/100, Loss: 0.317379\n",
            "Epoch: 40/100, Loss: 0.312099\n",
            "Epoch: 41/100, Loss: 0.306891\n",
            "Epoch: 42/100, Loss: 0.301750\n",
            "Epoch: 43/100, Loss: 0.296677\n",
            "Epoch: 44/100, Loss: 0.291673\n",
            "Epoch: 45/100, Loss: 0.286731\n",
            "Epoch: 46/100, Loss: 0.281857\n",
            "Epoch: 47/100, Loss: 0.277043\n",
            "Epoch: 48/100, Loss: 0.272293\n",
            "Epoch: 49/100, Loss: 0.267599\n",
            "Epoch: 50/100, Loss: 0.262963\n",
            "Epoch: 51/100, Loss: 0.258388\n",
            "Epoch: 52/100, Loss: 0.253866\n",
            "Epoch: 53/100, Loss: 0.249403\n",
            "Epoch: 54/100, Loss: 0.244996\n",
            "Epoch: 55/100, Loss: 0.240642\n",
            "Epoch: 56/100, Loss: 0.236347\n",
            "Epoch: 57/100, Loss: 0.232105\n",
            "Epoch: 58/100, Loss: 0.227917\n",
            "Epoch: 59/100, Loss: 0.223780\n",
            "Epoch: 60/100, Loss: 0.219694\n",
            "Epoch: 61/100, Loss: 0.215658\n",
            "Epoch: 62/100, Loss: 0.211674\n",
            "Epoch: 63/100, Loss: 0.207742\n",
            "Epoch: 64/100, Loss: 0.203861\n",
            "Epoch: 65/100, Loss: 0.200028\n",
            "Epoch: 66/100, Loss: 0.196242\n",
            "Epoch: 67/100, Loss: 0.192507\n",
            "Epoch: 68/100, Loss: 0.188827\n",
            "Epoch: 69/100, Loss: 0.185191\n",
            "Epoch: 70/100, Loss: 0.181602\n",
            "Epoch: 71/100, Loss: 0.178062\n",
            "Epoch: 72/100, Loss: 0.174569\n",
            "Epoch: 73/100, Loss: 0.171126\n",
            "Epoch: 74/100, Loss: 0.167729\n",
            "Epoch: 75/100, Loss: 0.164379\n",
            "Epoch: 76/100, Loss: 0.161075\n",
            "Epoch: 77/100, Loss: 0.157820\n",
            "Epoch: 78/100, Loss: 0.154614\n",
            "Epoch: 79/100, Loss: 0.151455\n",
            "Epoch: 80/100, Loss: 0.148347\n",
            "Epoch: 81/100, Loss: 0.145288\n",
            "Epoch: 82/100, Loss: 0.142280\n",
            "Epoch: 83/100, Loss: 0.139319\n",
            "Epoch: 84/100, Loss: 0.136395\n",
            "Epoch: 85/100, Loss: 0.133497\n",
            "Epoch: 86/100, Loss: 0.130639\n",
            "Epoch: 87/100, Loss: 0.127827\n",
            "Epoch: 88/100, Loss: 0.125065\n",
            "Epoch: 89/100, Loss: 0.122355\n",
            "Epoch: 90/100, Loss: 0.119691\n",
            "Epoch: 91/100, Loss: 0.117076\n",
            "Epoch: 92/100, Loss: 0.114519\n",
            "Epoch: 93/100, Loss: 0.111992\n",
            "Epoch: 94/100, Loss: 0.109492\n",
            "Epoch: 95/100, Loss: 0.107035\n",
            "Epoch: 96/100, Loss: 0.104632\n",
            "Epoch: 97/100, Loss: 0.102270\n",
            "Epoch: 98/100, Loss: 0.099950\n",
            "Epoch: 99/100, Loss: 0.097669\n",
            "Epoch: 100/100, Loss: 0.095410\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.668\n",
            "Normalised mutual info score on k-means on latent space: 0.5929261596180136\n",
            "ARI score on k-means on latent space: 0.49571526559976586\n",
            "K-means cluster error on latent space: 43971.4296875\n",
            "K-means silhouette score on latent space: 0.18438515 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7287\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6721771674666963 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5689448230532352 \n",
            "\n",
            "ROUND NUMBER  2 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.925400\n",
            "Epoch: 2/100, Loss: 0.731557\n",
            "Epoch: 3/100, Loss: 0.650659\n",
            "Epoch: 4/100, Loss: 0.610336\n",
            "Epoch: 5/100, Loss: 0.585766\n",
            "Epoch: 6/100, Loss: 0.568084\n",
            "Epoch: 7/100, Loss: 0.553619\n",
            "Epoch: 8/100, Loss: 0.540965\n",
            "Epoch: 9/100, Loss: 0.529508\n",
            "Epoch: 10/100, Loss: 0.518883\n",
            "Epoch: 11/100, Loss: 0.508886\n",
            "Epoch: 12/100, Loss: 0.499374\n",
            "Epoch: 13/100, Loss: 0.490267\n",
            "Epoch: 14/100, Loss: 0.481509\n",
            "Epoch: 15/100, Loss: 0.473063\n",
            "Epoch: 16/100, Loss: 0.464870\n",
            "Epoch: 17/100, Loss: 0.456913\n",
            "Epoch: 18/100, Loss: 0.449168\n",
            "Epoch: 19/100, Loss: 0.441616\n",
            "Epoch: 20/100, Loss: 0.434241\n",
            "Epoch: 21/100, Loss: 0.427036\n",
            "Epoch: 22/100, Loss: 0.419987\n",
            "Epoch: 23/100, Loss: 0.413084\n",
            "Epoch: 24/100, Loss: 0.406318\n",
            "Epoch: 25/100, Loss: 0.399681\n",
            "Epoch: 26/100, Loss: 0.393168\n",
            "Epoch: 27/100, Loss: 0.386769\n",
            "Epoch: 28/100, Loss: 0.380480\n",
            "Epoch: 29/100, Loss: 0.374297\n",
            "Epoch: 30/100, Loss: 0.368215\n",
            "Epoch: 31/100, Loss: 0.362233\n",
            "Epoch: 32/100, Loss: 0.356342\n",
            "Epoch: 33/100, Loss: 0.350539\n",
            "Epoch: 34/100, Loss: 0.344825\n",
            "Epoch: 35/100, Loss: 0.339194\n",
            "Epoch: 36/100, Loss: 0.333644\n",
            "Epoch: 37/100, Loss: 0.328170\n",
            "Epoch: 38/100, Loss: 0.322770\n",
            "Epoch: 39/100, Loss: 0.317444\n",
            "Epoch: 40/100, Loss: 0.312188\n",
            "Epoch: 41/100, Loss: 0.307002\n",
            "Epoch: 42/100, Loss: 0.301883\n",
            "Epoch: 43/100, Loss: 0.296828\n",
            "Epoch: 44/100, Loss: 0.291837\n",
            "Epoch: 45/100, Loss: 0.286915\n",
            "Epoch: 46/100, Loss: 0.282052\n",
            "Epoch: 47/100, Loss: 0.277252\n",
            "Epoch: 48/100, Loss: 0.272511\n",
            "Epoch: 49/100, Loss: 0.267829\n",
            "Epoch: 50/100, Loss: 0.263202\n",
            "Epoch: 51/100, Loss: 0.258635\n",
            "Epoch: 52/100, Loss: 0.254125\n",
            "Epoch: 53/100, Loss: 0.249671\n",
            "Epoch: 54/100, Loss: 0.245273\n",
            "Epoch: 55/100, Loss: 0.240931\n",
            "Epoch: 56/100, Loss: 0.236641\n",
            "Epoch: 57/100, Loss: 0.232402\n",
            "Epoch: 58/100, Loss: 0.228217\n",
            "Epoch: 59/100, Loss: 0.224091\n",
            "Epoch: 60/100, Loss: 0.220012\n",
            "Epoch: 61/100, Loss: 0.215987\n",
            "Epoch: 62/100, Loss: 0.212012\n",
            "Epoch: 63/100, Loss: 0.208088\n",
            "Epoch: 64/100, Loss: 0.204213\n",
            "Epoch: 65/100, Loss: 0.200389\n",
            "Epoch: 66/100, Loss: 0.196611\n",
            "Epoch: 67/100, Loss: 0.192886\n",
            "Epoch: 68/100, Loss: 0.189205\n",
            "Epoch: 69/100, Loss: 0.185572\n",
            "Epoch: 70/100, Loss: 0.181988\n",
            "Epoch: 71/100, Loss: 0.178450\n",
            "Epoch: 72/100, Loss: 0.174963\n",
            "Epoch: 73/100, Loss: 0.171519\n",
            "Epoch: 74/100, Loss: 0.168121\n",
            "Epoch: 75/100, Loss: 0.164772\n",
            "Epoch: 76/100, Loss: 0.161466\n",
            "Epoch: 77/100, Loss: 0.158209\n",
            "Epoch: 78/100, Loss: 0.154996\n",
            "Epoch: 79/100, Loss: 0.151831\n",
            "Epoch: 80/100, Loss: 0.148710\n",
            "Epoch: 81/100, Loss: 0.145633\n",
            "Epoch: 82/100, Loss: 0.142603\n",
            "Epoch: 83/100, Loss: 0.139619\n",
            "Epoch: 84/100, Loss: 0.136677\n",
            "Epoch: 85/100, Loss: 0.133777\n",
            "Epoch: 86/100, Loss: 0.130927\n",
            "Epoch: 87/100, Loss: 0.128123\n",
            "Epoch: 88/100, Loss: 0.125366\n",
            "Epoch: 89/100, Loss: 0.122655\n",
            "Epoch: 90/100, Loss: 0.119984\n",
            "Epoch: 91/100, Loss: 0.117340\n",
            "Epoch: 92/100, Loss: 0.114725\n",
            "Epoch: 93/100, Loss: 0.112156\n",
            "Epoch: 94/100, Loss: 0.109597\n",
            "Epoch: 95/100, Loss: 0.107088\n",
            "Epoch: 96/100, Loss: 0.104623\n",
            "Epoch: 97/100, Loss: 0.102194\n",
            "Epoch: 98/100, Loss: 0.099807\n",
            "Epoch: 99/100, Loss: 0.097465\n",
            "Epoch: 100/100, Loss: 0.095181\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6596\n",
            "Normalised mutual info score on k-means on latent space: 0.6037775403669321\n",
            "ARI score on k-means on latent space: 0.5012788158661259\n",
            "K-means cluster error on latent space: 42000.9921875\n",
            "K-means silhouette score on latent space: 0.19945788 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6796\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6692005361417926 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.534163800471909 \n",
            "\n",
            "ROUND NUMBER  3 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.959352\n",
            "Epoch: 2/100, Loss: 0.767223\n",
            "Epoch: 3/100, Loss: 0.676646\n",
            "Epoch: 4/100, Loss: 0.630238\n",
            "Epoch: 5/100, Loss: 0.601948\n",
            "Epoch: 6/100, Loss: 0.581567\n",
            "Epoch: 7/100, Loss: 0.564769\n",
            "Epoch: 8/100, Loss: 0.550186\n",
            "Epoch: 9/100, Loss: 0.537228\n",
            "Epoch: 10/100, Loss: 0.525485\n",
            "Epoch: 11/100, Loss: 0.514656\n",
            "Epoch: 12/100, Loss: 0.504519\n",
            "Epoch: 13/100, Loss: 0.494931\n",
            "Epoch: 14/100, Loss: 0.485795\n",
            "Epoch: 15/100, Loss: 0.477014\n",
            "Epoch: 16/100, Loss: 0.468564\n",
            "Epoch: 17/100, Loss: 0.460389\n",
            "Epoch: 18/100, Loss: 0.452457\n",
            "Epoch: 19/100, Loss: 0.444748\n",
            "Epoch: 20/100, Loss: 0.437240\n",
            "Epoch: 21/100, Loss: 0.429910\n",
            "Epoch: 22/100, Loss: 0.422745\n",
            "Epoch: 23/100, Loss: 0.415738\n",
            "Epoch: 24/100, Loss: 0.408876\n",
            "Epoch: 25/100, Loss: 0.402160\n",
            "Epoch: 26/100, Loss: 0.395569\n",
            "Epoch: 27/100, Loss: 0.389097\n",
            "Epoch: 28/100, Loss: 0.382734\n",
            "Epoch: 29/100, Loss: 0.376483\n",
            "Epoch: 30/100, Loss: 0.370334\n",
            "Epoch: 31/100, Loss: 0.364289\n",
            "Epoch: 32/100, Loss: 0.358339\n",
            "Epoch: 33/100, Loss: 0.352481\n",
            "Epoch: 34/100, Loss: 0.346715\n",
            "Epoch: 35/100, Loss: 0.341032\n",
            "Epoch: 36/100, Loss: 0.335433\n",
            "Epoch: 37/100, Loss: 0.329914\n",
            "Epoch: 38/100, Loss: 0.324470\n",
            "Epoch: 39/100, Loss: 0.319103\n",
            "Epoch: 40/100, Loss: 0.313808\n",
            "Epoch: 41/100, Loss: 0.308584\n",
            "Epoch: 42/100, Loss: 0.303431\n",
            "Epoch: 43/100, Loss: 0.298348\n",
            "Epoch: 44/100, Loss: 0.293330\n",
            "Epoch: 45/100, Loss: 0.288375\n",
            "Epoch: 46/100, Loss: 0.283482\n",
            "Epoch: 47/100, Loss: 0.278653\n",
            "Epoch: 48/100, Loss: 0.273885\n",
            "Epoch: 49/100, Loss: 0.269179\n",
            "Epoch: 50/100, Loss: 0.264533\n",
            "Epoch: 51/100, Loss: 0.259941\n",
            "Epoch: 52/100, Loss: 0.255410\n",
            "Epoch: 53/100, Loss: 0.250935\n",
            "Epoch: 54/100, Loss: 0.246515\n",
            "Epoch: 55/100, Loss: 0.242150\n",
            "Epoch: 56/100, Loss: 0.237840\n",
            "Epoch: 57/100, Loss: 0.233584\n",
            "Epoch: 58/100, Loss: 0.229384\n",
            "Epoch: 59/100, Loss: 0.225238\n",
            "Epoch: 60/100, Loss: 0.221142\n",
            "Epoch: 61/100, Loss: 0.217098\n",
            "Epoch: 62/100, Loss: 0.213104\n",
            "Epoch: 63/100, Loss: 0.209160\n",
            "Epoch: 64/100, Loss: 0.205267\n",
            "Epoch: 65/100, Loss: 0.201425\n",
            "Epoch: 66/100, Loss: 0.197629\n",
            "Epoch: 67/100, Loss: 0.193883\n",
            "Epoch: 68/100, Loss: 0.190187\n",
            "Epoch: 69/100, Loss: 0.186536\n",
            "Epoch: 70/100, Loss: 0.182932\n",
            "Epoch: 71/100, Loss: 0.179380\n",
            "Epoch: 72/100, Loss: 0.175873\n",
            "Epoch: 73/100, Loss: 0.172415\n",
            "Epoch: 74/100, Loss: 0.169005\n",
            "Epoch: 75/100, Loss: 0.165644\n",
            "Epoch: 76/100, Loss: 0.162328\n",
            "Epoch: 77/100, Loss: 0.159057\n",
            "Epoch: 78/100, Loss: 0.155832\n",
            "Epoch: 79/100, Loss: 0.152653\n",
            "Epoch: 80/100, Loss: 0.149519\n",
            "Epoch: 81/100, Loss: 0.146439\n",
            "Epoch: 82/100, Loss: 0.143408\n",
            "Epoch: 83/100, Loss: 0.140427\n",
            "Epoch: 84/100, Loss: 0.137494\n",
            "Epoch: 85/100, Loss: 0.134613\n",
            "Epoch: 86/100, Loss: 0.131797\n",
            "Epoch: 87/100, Loss: 0.128997\n",
            "Epoch: 88/100, Loss: 0.126213\n",
            "Epoch: 89/100, Loss: 0.123463\n",
            "Epoch: 90/100, Loss: 0.120738\n",
            "Epoch: 91/100, Loss: 0.118057\n",
            "Epoch: 92/100, Loss: 0.115440\n",
            "Epoch: 93/100, Loss: 0.112881\n",
            "Epoch: 94/100, Loss: 0.110371\n",
            "Epoch: 95/100, Loss: 0.107901\n",
            "Epoch: 96/100, Loss: 0.105488\n",
            "Epoch: 97/100, Loss: 0.103143\n",
            "Epoch: 98/100, Loss: 0.100858\n",
            "Epoch: 99/100, Loss: 0.098615\n",
            "Epoch: 100/100, Loss: 0.096435\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6482\n",
            "Normalised mutual info score on k-means on latent space: 0.5870032908083735\n",
            "ARI score on k-means on latent space: 0.4780948057631249\n",
            "K-means cluster error on latent space: 41297.6875\n",
            "K-means silhouette score on latent space: 0.19147521 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6831\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6596918631951512 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5398600386809469 \n",
            "\n",
            "ROUND NUMBER  4 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.957233\n",
            "Epoch: 2/100, Loss: 0.750243\n",
            "Epoch: 3/100, Loss: 0.662742\n",
            "Epoch: 4/100, Loss: 0.618934\n",
            "Epoch: 5/100, Loss: 0.592693\n",
            "Epoch: 6/100, Loss: 0.573783\n",
            "Epoch: 7/100, Loss: 0.558311\n",
            "Epoch: 8/100, Loss: 0.544892\n",
            "Epoch: 9/100, Loss: 0.532833\n",
            "Epoch: 10/100, Loss: 0.521738\n",
            "Epoch: 11/100, Loss: 0.511385\n",
            "Epoch: 12/100, Loss: 0.501603\n",
            "Epoch: 13/100, Loss: 0.492292\n",
            "Epoch: 14/100, Loss: 0.483377\n",
            "Epoch: 15/100, Loss: 0.474789\n",
            "Epoch: 16/100, Loss: 0.466480\n",
            "Epoch: 17/100, Loss: 0.458425\n",
            "Epoch: 18/100, Loss: 0.450593\n",
            "Epoch: 19/100, Loss: 0.442963\n",
            "Epoch: 20/100, Loss: 0.435517\n",
            "Epoch: 21/100, Loss: 0.428247\n",
            "Epoch: 22/100, Loss: 0.421130\n",
            "Epoch: 23/100, Loss: 0.414167\n",
            "Epoch: 24/100, Loss: 0.407345\n",
            "Epoch: 25/100, Loss: 0.400654\n",
            "Epoch: 26/100, Loss: 0.394087\n",
            "Epoch: 27/100, Loss: 0.387639\n",
            "Epoch: 28/100, Loss: 0.381305\n",
            "Epoch: 29/100, Loss: 0.375076\n",
            "Epoch: 30/100, Loss: 0.368957\n",
            "Epoch: 31/100, Loss: 0.362935\n",
            "Epoch: 32/100, Loss: 0.357004\n",
            "Epoch: 33/100, Loss: 0.351166\n",
            "Epoch: 34/100, Loss: 0.345419\n",
            "Epoch: 35/100, Loss: 0.339752\n",
            "Epoch: 36/100, Loss: 0.334171\n",
            "Epoch: 37/100, Loss: 0.328663\n",
            "Epoch: 38/100, Loss: 0.323235\n",
            "Epoch: 39/100, Loss: 0.317884\n",
            "Epoch: 40/100, Loss: 0.312601\n",
            "Epoch: 41/100, Loss: 0.307389\n",
            "Epoch: 42/100, Loss: 0.302248\n",
            "Epoch: 43/100, Loss: 0.297173\n",
            "Epoch: 44/100, Loss: 0.292163\n",
            "Epoch: 45/100, Loss: 0.287219\n",
            "Epoch: 46/100, Loss: 0.282336\n",
            "Epoch: 47/100, Loss: 0.277512\n",
            "Epoch: 48/100, Loss: 0.272749\n",
            "Epoch: 49/100, Loss: 0.268050\n",
            "Epoch: 50/100, Loss: 0.263408\n",
            "Epoch: 51/100, Loss: 0.258823\n",
            "Epoch: 52/100, Loss: 0.254292\n",
            "Epoch: 53/100, Loss: 0.249819\n",
            "Epoch: 54/100, Loss: 0.245402\n",
            "Epoch: 55/100, Loss: 0.241039\n",
            "Epoch: 56/100, Loss: 0.236731\n",
            "Epoch: 57/100, Loss: 0.232477\n",
            "Epoch: 58/100, Loss: 0.228277\n",
            "Epoch: 59/100, Loss: 0.224128\n",
            "Epoch: 60/100, Loss: 0.220031\n",
            "Epoch: 61/100, Loss: 0.215985\n",
            "Epoch: 62/100, Loss: 0.211993\n",
            "Epoch: 63/100, Loss: 0.208052\n",
            "Epoch: 64/100, Loss: 0.204161\n",
            "Epoch: 65/100, Loss: 0.200318\n",
            "Epoch: 66/100, Loss: 0.196529\n",
            "Epoch: 67/100, Loss: 0.192786\n",
            "Epoch: 68/100, Loss: 0.189092\n",
            "Epoch: 69/100, Loss: 0.185445\n",
            "Epoch: 70/100, Loss: 0.181847\n",
            "Epoch: 71/100, Loss: 0.178297\n",
            "Epoch: 72/100, Loss: 0.174791\n",
            "Epoch: 73/100, Loss: 0.171337\n",
            "Epoch: 74/100, Loss: 0.167933\n",
            "Epoch: 75/100, Loss: 0.164575\n",
            "Epoch: 76/100, Loss: 0.161266\n",
            "Epoch: 77/100, Loss: 0.158006\n",
            "Epoch: 78/100, Loss: 0.154797\n",
            "Epoch: 79/100, Loss: 0.151640\n",
            "Epoch: 80/100, Loss: 0.148534\n",
            "Epoch: 81/100, Loss: 0.145464\n",
            "Epoch: 82/100, Loss: 0.142424\n",
            "Epoch: 83/100, Loss: 0.139439\n",
            "Epoch: 84/100, Loss: 0.136503\n",
            "Epoch: 85/100, Loss: 0.133594\n",
            "Epoch: 86/100, Loss: 0.130720\n",
            "Epoch: 87/100, Loss: 0.127882\n",
            "Epoch: 88/100, Loss: 0.125106\n",
            "Epoch: 89/100, Loss: 0.122367\n",
            "Epoch: 90/100, Loss: 0.119666\n",
            "Epoch: 91/100, Loss: 0.117001\n",
            "Epoch: 92/100, Loss: 0.114377\n",
            "Epoch: 93/100, Loss: 0.111807\n",
            "Epoch: 94/100, Loss: 0.109272\n",
            "Epoch: 95/100, Loss: 0.106790\n",
            "Epoch: 96/100, Loss: 0.104371\n",
            "Epoch: 97/100, Loss: 0.102006\n",
            "Epoch: 98/100, Loss: 0.099698\n",
            "Epoch: 99/100, Loss: 0.097446\n",
            "Epoch: 100/100, Loss: 0.095257\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6718\n",
            "Normalised mutual info score on k-means on latent space: 0.6018103389526058\n",
            "ARI score on k-means on latent space: 0.5011324360481311\n",
            "K-means cluster error on latent space: 42709.54296875\n",
            "K-means silhouette score on latent space: 0.18450008 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.731\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6948911211209612 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5845522507915838 \n",
            "\n",
            "ROUND NUMBER  5 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.900122\n",
            "Epoch: 2/100, Loss: 0.711327\n",
            "Epoch: 3/100, Loss: 0.642076\n",
            "Epoch: 4/100, Loss: 0.607490\n",
            "Epoch: 5/100, Loss: 0.585154\n",
            "Epoch: 6/100, Loss: 0.568151\n",
            "Epoch: 7/100, Loss: 0.553857\n",
            "Epoch: 8/100, Loss: 0.541218\n",
            "Epoch: 9/100, Loss: 0.529697\n",
            "Epoch: 10/100, Loss: 0.519001\n",
            "Epoch: 11/100, Loss: 0.508943\n",
            "Epoch: 12/100, Loss: 0.499386\n",
            "Epoch: 13/100, Loss: 0.490255\n",
            "Epoch: 14/100, Loss: 0.481480\n",
            "Epoch: 15/100, Loss: 0.473013\n",
            "Epoch: 16/100, Loss: 0.464814\n",
            "Epoch: 17/100, Loss: 0.456844\n",
            "Epoch: 18/100, Loss: 0.449091\n",
            "Epoch: 19/100, Loss: 0.441531\n",
            "Epoch: 20/100, Loss: 0.434147\n",
            "Epoch: 21/100, Loss: 0.426925\n",
            "Epoch: 22/100, Loss: 0.419857\n",
            "Epoch: 23/100, Loss: 0.412940\n",
            "Epoch: 24/100, Loss: 0.406160\n",
            "Epoch: 25/100, Loss: 0.399509\n",
            "Epoch: 26/100, Loss: 0.392982\n",
            "Epoch: 27/100, Loss: 0.386567\n",
            "Epoch: 28/100, Loss: 0.380258\n",
            "Epoch: 29/100, Loss: 0.374052\n",
            "Epoch: 30/100, Loss: 0.367950\n",
            "Epoch: 31/100, Loss: 0.361944\n",
            "Epoch: 32/100, Loss: 0.356035\n",
            "Epoch: 33/100, Loss: 0.350213\n",
            "Epoch: 34/100, Loss: 0.344480\n",
            "Epoch: 35/100, Loss: 0.338829\n",
            "Epoch: 36/100, Loss: 0.333261\n",
            "Epoch: 37/100, Loss: 0.327773\n",
            "Epoch: 38/100, Loss: 0.322362\n",
            "Epoch: 39/100, Loss: 0.317023\n",
            "Epoch: 40/100, Loss: 0.311759\n",
            "Epoch: 41/100, Loss: 0.306561\n",
            "Epoch: 42/100, Loss: 0.301435\n",
            "Epoch: 43/100, Loss: 0.296375\n",
            "Epoch: 44/100, Loss: 0.291379\n",
            "Epoch: 45/100, Loss: 0.286453\n",
            "Epoch: 46/100, Loss: 0.281588\n",
            "Epoch: 47/100, Loss: 0.276782\n",
            "Epoch: 48/100, Loss: 0.272037\n",
            "Epoch: 49/100, Loss: 0.267356\n",
            "Epoch: 50/100, Loss: 0.262730\n",
            "Epoch: 51/100, Loss: 0.258164\n",
            "Epoch: 52/100, Loss: 0.253658\n",
            "Epoch: 53/100, Loss: 0.249204\n",
            "Epoch: 54/100, Loss: 0.244807\n",
            "Epoch: 55/100, Loss: 0.240465\n",
            "Epoch: 56/100, Loss: 0.236177\n",
            "Epoch: 57/100, Loss: 0.231944\n",
            "Epoch: 58/100, Loss: 0.227764\n",
            "Epoch: 59/100, Loss: 0.223639\n",
            "Epoch: 60/100, Loss: 0.219562\n",
            "Epoch: 61/100, Loss: 0.215534\n",
            "Epoch: 62/100, Loss: 0.211562\n",
            "Epoch: 63/100, Loss: 0.207638\n",
            "Epoch: 64/100, Loss: 0.203763\n",
            "Epoch: 65/100, Loss: 0.199935\n",
            "Epoch: 66/100, Loss: 0.196160\n",
            "Epoch: 67/100, Loss: 0.192433\n",
            "Epoch: 68/100, Loss: 0.188754\n",
            "Epoch: 69/100, Loss: 0.185125\n",
            "Epoch: 70/100, Loss: 0.181539\n",
            "Epoch: 71/100, Loss: 0.178004\n",
            "Epoch: 72/100, Loss: 0.174515\n",
            "Epoch: 73/100, Loss: 0.171074\n",
            "Epoch: 74/100, Loss: 0.167677\n",
            "Epoch: 75/100, Loss: 0.164329\n",
            "Epoch: 76/100, Loss: 0.161024\n",
            "Epoch: 77/100, Loss: 0.157769\n",
            "Epoch: 78/100, Loss: 0.154557\n",
            "Epoch: 79/100, Loss: 0.151389\n",
            "Epoch: 80/100, Loss: 0.148270\n",
            "Epoch: 81/100, Loss: 0.145197\n",
            "Epoch: 82/100, Loss: 0.142167\n",
            "Epoch: 83/100, Loss: 0.139182\n",
            "Epoch: 84/100, Loss: 0.136241\n",
            "Epoch: 85/100, Loss: 0.133341\n",
            "Epoch: 86/100, Loss: 0.130486\n",
            "Epoch: 87/100, Loss: 0.127678\n",
            "Epoch: 88/100, Loss: 0.124913\n",
            "Epoch: 89/100, Loss: 0.122199\n",
            "Epoch: 90/100, Loss: 0.119533\n",
            "Epoch: 91/100, Loss: 0.116928\n",
            "Epoch: 92/100, Loss: 0.114390\n",
            "Epoch: 93/100, Loss: 0.111907\n",
            "Epoch: 94/100, Loss: 0.109465\n",
            "Epoch: 95/100, Loss: 0.107033\n",
            "Epoch: 96/100, Loss: 0.104577\n",
            "Epoch: 97/100, Loss: 0.102122\n",
            "Epoch: 98/100, Loss: 0.099730\n",
            "Epoch: 99/100, Loss: 0.097418\n",
            "Epoch: 100/100, Loss: 0.095171\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6669\n",
            "Normalised mutual info score on k-means on latent space: 0.6115324495385216\n",
            "ARI score on k-means on latent space: 0.5054991908408534\n",
            "K-means cluster error on latent space: 42125.60546875\n",
            "K-means silhouette score on latent space: 0.19140685 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7616\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.7205667972251717 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.6266225959032145 \n",
            "\n",
            "ROUND NUMBER  6 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.924027\n",
            "Epoch: 2/100, Loss: 0.715664\n",
            "Epoch: 3/100, Loss: 0.644013\n",
            "Epoch: 4/100, Loss: 0.610323\n",
            "Epoch: 5/100, Loss: 0.587335\n",
            "Epoch: 6/100, Loss: 0.569390\n",
            "Epoch: 7/100, Loss: 0.554420\n",
            "Epoch: 8/100, Loss: 0.541389\n",
            "Epoch: 9/100, Loss: 0.529664\n",
            "Epoch: 10/100, Loss: 0.518882\n",
            "Epoch: 11/100, Loss: 0.508792\n",
            "Epoch: 12/100, Loss: 0.499239\n",
            "Epoch: 13/100, Loss: 0.490114\n",
            "Epoch: 14/100, Loss: 0.481345\n",
            "Epoch: 15/100, Loss: 0.472883\n",
            "Epoch: 16/100, Loss: 0.464681\n",
            "Epoch: 17/100, Loss: 0.456717\n",
            "Epoch: 18/100, Loss: 0.448963\n",
            "Epoch: 19/100, Loss: 0.441401\n",
            "Epoch: 20/100, Loss: 0.434016\n",
            "Epoch: 21/100, Loss: 0.426788\n",
            "Epoch: 22/100, Loss: 0.419717\n",
            "Epoch: 23/100, Loss: 0.412793\n",
            "Epoch: 24/100, Loss: 0.406003\n",
            "Epoch: 25/100, Loss: 0.399339\n",
            "Epoch: 26/100, Loss: 0.392797\n",
            "Epoch: 27/100, Loss: 0.386375\n",
            "Epoch: 28/100, Loss: 0.380061\n",
            "Epoch: 29/100, Loss: 0.373855\n",
            "Epoch: 30/100, Loss: 0.367754\n",
            "Epoch: 31/100, Loss: 0.361750\n",
            "Epoch: 32/100, Loss: 0.355838\n",
            "Epoch: 33/100, Loss: 0.350014\n",
            "Epoch: 34/100, Loss: 0.344279\n",
            "Epoch: 35/100, Loss: 0.338626\n",
            "Epoch: 36/100, Loss: 0.333056\n",
            "Epoch: 37/100, Loss: 0.327567\n",
            "Epoch: 38/100, Loss: 0.322152\n",
            "Epoch: 39/100, Loss: 0.316812\n",
            "Epoch: 40/100, Loss: 0.311542\n",
            "Epoch: 41/100, Loss: 0.306345\n",
            "Epoch: 42/100, Loss: 0.301213\n",
            "Epoch: 43/100, Loss: 0.296151\n",
            "Epoch: 44/100, Loss: 0.291155\n",
            "Epoch: 45/100, Loss: 0.286221\n",
            "Epoch: 46/100, Loss: 0.281351\n",
            "Epoch: 47/100, Loss: 0.276543\n",
            "Epoch: 48/100, Loss: 0.271795\n",
            "Epoch: 49/100, Loss: 0.267108\n",
            "Epoch: 50/100, Loss: 0.262481\n",
            "Epoch: 51/100, Loss: 0.257913\n",
            "Epoch: 52/100, Loss: 0.253402\n",
            "Epoch: 53/100, Loss: 0.248949\n",
            "Epoch: 54/100, Loss: 0.244550\n",
            "Epoch: 55/100, Loss: 0.240209\n",
            "Epoch: 56/100, Loss: 0.235922\n",
            "Epoch: 57/100, Loss: 0.231687\n",
            "Epoch: 58/100, Loss: 0.227506\n",
            "Epoch: 59/100, Loss: 0.223378\n",
            "Epoch: 60/100, Loss: 0.219306\n",
            "Epoch: 61/100, Loss: 0.215282\n",
            "Epoch: 62/100, Loss: 0.211311\n",
            "Epoch: 63/100, Loss: 0.207391\n",
            "Epoch: 64/100, Loss: 0.203521\n",
            "Epoch: 65/100, Loss: 0.199700\n",
            "Epoch: 66/100, Loss: 0.195926\n",
            "Epoch: 67/100, Loss: 0.192201\n",
            "Epoch: 68/100, Loss: 0.188522\n",
            "Epoch: 69/100, Loss: 0.184892\n",
            "Epoch: 70/100, Loss: 0.181311\n",
            "Epoch: 71/100, Loss: 0.177775\n",
            "Epoch: 72/100, Loss: 0.174290\n",
            "Epoch: 73/100, Loss: 0.170852\n",
            "Epoch: 74/100, Loss: 0.167459\n",
            "Epoch: 75/100, Loss: 0.164117\n",
            "Epoch: 76/100, Loss: 0.160819\n",
            "Epoch: 77/100, Loss: 0.157563\n",
            "Epoch: 78/100, Loss: 0.154350\n",
            "Epoch: 79/100, Loss: 0.151186\n",
            "Epoch: 80/100, Loss: 0.148073\n",
            "Epoch: 81/100, Loss: 0.145013\n",
            "Epoch: 82/100, Loss: 0.142002\n",
            "Epoch: 83/100, Loss: 0.139041\n",
            "Epoch: 84/100, Loss: 0.136127\n",
            "Epoch: 85/100, Loss: 0.133258\n",
            "Epoch: 86/100, Loss: 0.130419\n",
            "Epoch: 87/100, Loss: 0.127634\n",
            "Epoch: 88/100, Loss: 0.124900\n",
            "Epoch: 89/100, Loss: 0.122206\n",
            "Epoch: 90/100, Loss: 0.119538\n",
            "Epoch: 91/100, Loss: 0.116890\n",
            "Epoch: 92/100, Loss: 0.114271\n",
            "Epoch: 93/100, Loss: 0.111694\n",
            "Epoch: 94/100, Loss: 0.109176\n",
            "Epoch: 95/100, Loss: 0.106696\n",
            "Epoch: 96/100, Loss: 0.104240\n",
            "Epoch: 97/100, Loss: 0.101840\n",
            "Epoch: 98/100, Loss: 0.099493\n",
            "Epoch: 99/100, Loss: 0.097194\n",
            "Epoch: 100/100, Loss: 0.094940\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.7117\n",
            "Normalised mutual info score on k-means on latent space: 0.6396946841471227\n",
            "ARI score on k-means on latent space: 0.551722068790646\n",
            "K-means cluster error on latent space: 45141.875\n",
            "K-means silhouette score on latent space: 0.18475565 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7457\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6972687791700006 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.6223850288008026 \n",
            "\n",
            "ROUND NUMBER  7 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.938666\n",
            "Epoch: 2/100, Loss: 0.747036\n",
            "Epoch: 3/100, Loss: 0.662432\n",
            "Epoch: 4/100, Loss: 0.619384\n",
            "Epoch: 5/100, Loss: 0.593378\n",
            "Epoch: 6/100, Loss: 0.574206\n",
            "Epoch: 7/100, Loss: 0.558591\n",
            "Epoch: 8/100, Loss: 0.545066\n",
            "Epoch: 9/100, Loss: 0.532917\n",
            "Epoch: 10/100, Loss: 0.521774\n",
            "Epoch: 11/100, Loss: 0.511403\n",
            "Epoch: 12/100, Loss: 0.501635\n",
            "Epoch: 13/100, Loss: 0.492347\n",
            "Epoch: 14/100, Loss: 0.483456\n",
            "Epoch: 15/100, Loss: 0.474891\n",
            "Epoch: 16/100, Loss: 0.466608\n",
            "Epoch: 17/100, Loss: 0.458570\n",
            "Epoch: 18/100, Loss: 0.450759\n",
            "Epoch: 19/100, Loss: 0.443144\n",
            "Epoch: 20/100, Loss: 0.435719\n",
            "Epoch: 21/100, Loss: 0.428472\n",
            "Epoch: 22/100, Loss: 0.421380\n",
            "Epoch: 23/100, Loss: 0.414435\n",
            "Epoch: 24/100, Loss: 0.407627\n",
            "Epoch: 25/100, Loss: 0.400953\n",
            "Epoch: 26/100, Loss: 0.394397\n",
            "Epoch: 27/100, Loss: 0.387958\n",
            "Epoch: 28/100, Loss: 0.381635\n",
            "Epoch: 29/100, Loss: 0.375416\n",
            "Epoch: 30/100, Loss: 0.369298\n",
            "Epoch: 31/100, Loss: 0.363277\n",
            "Epoch: 32/100, Loss: 0.357353\n",
            "Epoch: 33/100, Loss: 0.351521\n",
            "Epoch: 34/100, Loss: 0.345772\n",
            "Epoch: 35/100, Loss: 0.340107\n",
            "Epoch: 36/100, Loss: 0.334527\n",
            "Epoch: 37/100, Loss: 0.329030\n",
            "Epoch: 38/100, Loss: 0.323604\n",
            "Epoch: 39/100, Loss: 0.318254\n",
            "Epoch: 40/100, Loss: 0.312979\n",
            "Epoch: 41/100, Loss: 0.307775\n",
            "Epoch: 42/100, Loss: 0.302639\n",
            "Epoch: 43/100, Loss: 0.297575\n",
            "Epoch: 44/100, Loss: 0.292576\n",
            "Epoch: 45/100, Loss: 0.287638\n",
            "Epoch: 46/100, Loss: 0.282766\n",
            "Epoch: 47/100, Loss: 0.277955\n",
            "Epoch: 48/100, Loss: 0.273200\n",
            "Epoch: 49/100, Loss: 0.268507\n",
            "Epoch: 50/100, Loss: 0.263875\n",
            "Epoch: 51/100, Loss: 0.259300\n",
            "Epoch: 52/100, Loss: 0.254782\n",
            "Epoch: 53/100, Loss: 0.250318\n",
            "Epoch: 54/100, Loss: 0.245911\n",
            "Epoch: 55/100, Loss: 0.241559\n",
            "Epoch: 56/100, Loss: 0.237257\n",
            "Epoch: 57/100, Loss: 0.233010\n",
            "Epoch: 58/100, Loss: 0.228815\n",
            "Epoch: 59/100, Loss: 0.224674\n",
            "Epoch: 60/100, Loss: 0.220588\n",
            "Epoch: 61/100, Loss: 0.216554\n",
            "Epoch: 62/100, Loss: 0.212569\n",
            "Epoch: 63/100, Loss: 0.208633\n",
            "Epoch: 64/100, Loss: 0.204750\n",
            "Epoch: 65/100, Loss: 0.200915\n",
            "Epoch: 66/100, Loss: 0.197130\n",
            "Epoch: 67/100, Loss: 0.193394\n",
            "Epoch: 68/100, Loss: 0.189703\n",
            "Epoch: 69/100, Loss: 0.186062\n",
            "Epoch: 70/100, Loss: 0.182470\n",
            "Epoch: 71/100, Loss: 0.178925\n",
            "Epoch: 72/100, Loss: 0.175428\n",
            "Epoch: 73/100, Loss: 0.171981\n",
            "Epoch: 74/100, Loss: 0.168580\n",
            "Epoch: 75/100, Loss: 0.165228\n",
            "Epoch: 76/100, Loss: 0.161927\n",
            "Epoch: 77/100, Loss: 0.158671\n",
            "Epoch: 78/100, Loss: 0.155465\n",
            "Epoch: 79/100, Loss: 0.152310\n",
            "Epoch: 80/100, Loss: 0.149201\n",
            "Epoch: 81/100, Loss: 0.146147\n",
            "Epoch: 82/100, Loss: 0.143128\n",
            "Epoch: 83/100, Loss: 0.140160\n",
            "Epoch: 84/100, Loss: 0.137253\n",
            "Epoch: 85/100, Loss: 0.134370\n",
            "Epoch: 86/100, Loss: 0.131499\n",
            "Epoch: 87/100, Loss: 0.128722\n",
            "Epoch: 88/100, Loss: 0.125979\n",
            "Epoch: 89/100, Loss: 0.123232\n",
            "Epoch: 90/100, Loss: 0.120488\n",
            "Epoch: 91/100, Loss: 0.117818\n",
            "Epoch: 92/100, Loss: 0.115211\n",
            "Epoch: 93/100, Loss: 0.112654\n",
            "Epoch: 94/100, Loss: 0.110159\n",
            "Epoch: 95/100, Loss: 0.107731\n",
            "Epoch: 96/100, Loss: 0.105334\n",
            "Epoch: 97/100, Loss: 0.102950\n",
            "Epoch: 98/100, Loss: 0.100551\n",
            "Epoch: 99/100, Loss: 0.098159\n",
            "Epoch: 100/100, Loss: 0.095813\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6703\n",
            "Normalised mutual info score on k-means on latent space: 0.6021550937294649\n",
            "ARI score on k-means on latent space: 0.5027341455842604\n",
            "K-means cluster error on latent space: 43584.84375\n",
            "K-means silhouette score on latent space: 0.1755793 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6495\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6250176986860759 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.4815888021809882 \n",
            "\n",
            "ROUND NUMBER  8 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.944447\n",
            "Epoch: 2/100, Loss: 0.744868\n",
            "Epoch: 3/100, Loss: 0.656059\n",
            "Epoch: 4/100, Loss: 0.613852\n",
            "Epoch: 5/100, Loss: 0.588985\n",
            "Epoch: 6/100, Loss: 0.570736\n",
            "Epoch: 7/100, Loss: 0.555835\n",
            "Epoch: 8/100, Loss: 0.542946\n",
            "Epoch: 9/100, Loss: 0.531339\n",
            "Epoch: 10/100, Loss: 0.520627\n",
            "Epoch: 11/100, Loss: 0.510572\n",
            "Epoch: 12/100, Loss: 0.501033\n",
            "Epoch: 13/100, Loss: 0.491903\n",
            "Epoch: 14/100, Loss: 0.483117\n",
            "Epoch: 15/100, Loss: 0.474625\n",
            "Epoch: 16/100, Loss: 0.466396\n",
            "Epoch: 17/100, Loss: 0.458397\n",
            "Epoch: 18/100, Loss: 0.450606\n",
            "Epoch: 19/100, Loss: 0.443007\n",
            "Epoch: 20/100, Loss: 0.435581\n",
            "Epoch: 21/100, Loss: 0.428320\n",
            "Epoch: 22/100, Loss: 0.421223\n",
            "Epoch: 23/100, Loss: 0.414272\n",
            "Epoch: 24/100, Loss: 0.407457\n",
            "Epoch: 25/100, Loss: 0.400770\n",
            "Epoch: 26/100, Loss: 0.394207\n",
            "Epoch: 27/100, Loss: 0.387760\n",
            "Epoch: 28/100, Loss: 0.381420\n",
            "Epoch: 29/100, Loss: 0.375187\n",
            "Epoch: 30/100, Loss: 0.369061\n",
            "Epoch: 31/100, Loss: 0.363037\n",
            "Epoch: 32/100, Loss: 0.357105\n",
            "Epoch: 33/100, Loss: 0.351263\n",
            "Epoch: 34/100, Loss: 0.345509\n",
            "Epoch: 35/100, Loss: 0.339843\n",
            "Epoch: 36/100, Loss: 0.334257\n",
            "Epoch: 37/100, Loss: 0.328751\n",
            "Epoch: 38/100, Loss: 0.323319\n",
            "Epoch: 39/100, Loss: 0.317959\n",
            "Epoch: 40/100, Loss: 0.312674\n",
            "Epoch: 41/100, Loss: 0.307459\n",
            "Epoch: 42/100, Loss: 0.302316\n",
            "Epoch: 43/100, Loss: 0.297239\n",
            "Epoch: 44/100, Loss: 0.292227\n",
            "Epoch: 45/100, Loss: 0.287282\n",
            "Epoch: 46/100, Loss: 0.282398\n",
            "Epoch: 47/100, Loss: 0.277579\n",
            "Epoch: 48/100, Loss: 0.272819\n",
            "Epoch: 49/100, Loss: 0.268119\n",
            "Epoch: 50/100, Loss: 0.263479\n",
            "Epoch: 51/100, Loss: 0.258899\n",
            "Epoch: 52/100, Loss: 0.254376\n",
            "Epoch: 53/100, Loss: 0.249909\n",
            "Epoch: 54/100, Loss: 0.245495\n",
            "Epoch: 55/100, Loss: 0.241136\n",
            "Epoch: 56/100, Loss: 0.236835\n",
            "Epoch: 57/100, Loss: 0.232584\n",
            "Epoch: 58/100, Loss: 0.228389\n",
            "Epoch: 59/100, Loss: 0.224242\n",
            "Epoch: 60/100, Loss: 0.220149\n",
            "Epoch: 61/100, Loss: 0.216108\n",
            "Epoch: 62/100, Loss: 0.212118\n",
            "Epoch: 63/100, Loss: 0.208178\n",
            "Epoch: 64/100, Loss: 0.204288\n",
            "Epoch: 65/100, Loss: 0.200447\n",
            "Epoch: 66/100, Loss: 0.196658\n",
            "Epoch: 67/100, Loss: 0.192916\n",
            "Epoch: 68/100, Loss: 0.189223\n",
            "Epoch: 69/100, Loss: 0.185580\n",
            "Epoch: 70/100, Loss: 0.181986\n",
            "Epoch: 71/100, Loss: 0.178440\n",
            "Epoch: 72/100, Loss: 0.174939\n",
            "Epoch: 73/100, Loss: 0.171485\n",
            "Epoch: 74/100, Loss: 0.168079\n",
            "Epoch: 75/100, Loss: 0.164721\n",
            "Epoch: 76/100, Loss: 0.161408\n",
            "Epoch: 77/100, Loss: 0.158142\n",
            "Epoch: 78/100, Loss: 0.154926\n",
            "Epoch: 79/100, Loss: 0.151754\n",
            "Epoch: 80/100, Loss: 0.148632\n",
            "Epoch: 81/100, Loss: 0.145559\n",
            "Epoch: 82/100, Loss: 0.142533\n",
            "Epoch: 83/100, Loss: 0.139558\n",
            "Epoch: 84/100, Loss: 0.136624\n",
            "Epoch: 85/100, Loss: 0.133728\n",
            "Epoch: 86/100, Loss: 0.130887\n",
            "Epoch: 87/100, Loss: 0.128089\n",
            "Epoch: 88/100, Loss: 0.125341\n",
            "Epoch: 89/100, Loss: 0.122657\n",
            "Epoch: 90/100, Loss: 0.120012\n",
            "Epoch: 91/100, Loss: 0.117380\n",
            "Epoch: 92/100, Loss: 0.114770\n",
            "Epoch: 93/100, Loss: 0.112200\n",
            "Epoch: 94/100, Loss: 0.109673\n",
            "Epoch: 95/100, Loss: 0.107193\n",
            "Epoch: 96/100, Loss: 0.104766\n",
            "Epoch: 97/100, Loss: 0.102382\n",
            "Epoch: 98/100, Loss: 0.100051\n",
            "Epoch: 99/100, Loss: 0.097746\n",
            "Epoch: 100/100, Loss: 0.095461\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6267\n",
            "Normalised mutual info score on k-means on latent space: 0.5766147852071155\n",
            "ARI score on k-means on latent space: 0.4608472396650949\n",
            "K-means cluster error on latent space: 43412.546875\n",
            "K-means silhouette score on latent space: 0.19189171 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7396\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6997173880649699 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5763302534964335 \n",
            "\n",
            "ROUND NUMBER  9 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.966056\n",
            "Epoch: 2/100, Loss: 0.769364\n",
            "Epoch: 3/100, Loss: 0.671656\n",
            "Epoch: 4/100, Loss: 0.624918\n",
            "Epoch: 5/100, Loss: 0.596755\n",
            "Epoch: 6/100, Loss: 0.576090\n",
            "Epoch: 7/100, Loss: 0.559599\n",
            "Epoch: 8/100, Loss: 0.545747\n",
            "Epoch: 9/100, Loss: 0.533462\n",
            "Epoch: 10/100, Loss: 0.522267\n",
            "Epoch: 11/100, Loss: 0.511855\n",
            "Epoch: 12/100, Loss: 0.502045\n",
            "Epoch: 13/100, Loss: 0.492729\n",
            "Epoch: 14/100, Loss: 0.483808\n",
            "Epoch: 15/100, Loss: 0.475220\n",
            "Epoch: 16/100, Loss: 0.466922\n",
            "Epoch: 17/100, Loss: 0.458877\n",
            "Epoch: 18/100, Loss: 0.451051\n",
            "Epoch: 19/100, Loss: 0.443431\n",
            "Epoch: 20/100, Loss: 0.436001\n",
            "Epoch: 21/100, Loss: 0.428741\n",
            "Epoch: 22/100, Loss: 0.421641\n",
            "Epoch: 23/100, Loss: 0.414690\n",
            "Epoch: 24/100, Loss: 0.407882\n",
            "Epoch: 25/100, Loss: 0.401209\n",
            "Epoch: 26/100, Loss: 0.394651\n",
            "Epoch: 27/100, Loss: 0.388208\n",
            "Epoch: 28/100, Loss: 0.381875\n",
            "Epoch: 29/100, Loss: 0.375650\n",
            "Epoch: 30/100, Loss: 0.369534\n",
            "Epoch: 31/100, Loss: 0.363512\n",
            "Epoch: 32/100, Loss: 0.357589\n",
            "Epoch: 33/100, Loss: 0.351758\n",
            "Epoch: 34/100, Loss: 0.346015\n",
            "Epoch: 35/100, Loss: 0.340358\n",
            "Epoch: 36/100, Loss: 0.334780\n",
            "Epoch: 37/100, Loss: 0.329282\n",
            "Epoch: 38/100, Loss: 0.323858\n",
            "Epoch: 39/100, Loss: 0.318512\n",
            "Epoch: 40/100, Loss: 0.313236\n",
            "Epoch: 41/100, Loss: 0.308029\n",
            "Epoch: 42/100, Loss: 0.302891\n",
            "Epoch: 43/100, Loss: 0.297818\n",
            "Epoch: 44/100, Loss: 0.292817\n",
            "Epoch: 45/100, Loss: 0.287877\n",
            "Epoch: 46/100, Loss: 0.283001\n",
            "Epoch: 47/100, Loss: 0.278191\n",
            "Epoch: 48/100, Loss: 0.273437\n",
            "Epoch: 49/100, Loss: 0.268745\n",
            "Epoch: 50/100, Loss: 0.264109\n",
            "Epoch: 51/100, Loss: 0.259536\n",
            "Epoch: 52/100, Loss: 0.255015\n",
            "Epoch: 53/100, Loss: 0.250549\n",
            "Epoch: 54/100, Loss: 0.246138\n",
            "Epoch: 55/100, Loss: 0.241785\n",
            "Epoch: 56/100, Loss: 0.237484\n",
            "Epoch: 57/100, Loss: 0.233236\n",
            "Epoch: 58/100, Loss: 0.229039\n",
            "Epoch: 59/100, Loss: 0.224895\n",
            "Epoch: 60/100, Loss: 0.220805\n",
            "Epoch: 61/100, Loss: 0.216768\n",
            "Epoch: 62/100, Loss: 0.212778\n",
            "Epoch: 63/100, Loss: 0.208837\n",
            "Epoch: 64/100, Loss: 0.204949\n",
            "Epoch: 65/100, Loss: 0.201112\n",
            "Epoch: 66/100, Loss: 0.197324\n",
            "Epoch: 67/100, Loss: 0.193583\n",
            "Epoch: 68/100, Loss: 0.189890\n",
            "Epoch: 69/100, Loss: 0.186244\n",
            "Epoch: 70/100, Loss: 0.182647\n",
            "Epoch: 71/100, Loss: 0.179096\n",
            "Epoch: 72/100, Loss: 0.175593\n",
            "Epoch: 73/100, Loss: 0.172140\n",
            "Epoch: 74/100, Loss: 0.168729\n",
            "Epoch: 75/100, Loss: 0.165370\n",
            "Epoch: 76/100, Loss: 0.162054\n",
            "Epoch: 77/100, Loss: 0.158782\n",
            "Epoch: 78/100, Loss: 0.155555\n",
            "Epoch: 79/100, Loss: 0.152374\n",
            "Epoch: 80/100, Loss: 0.149238\n",
            "Epoch: 81/100, Loss: 0.146149\n",
            "Epoch: 82/100, Loss: 0.143102\n",
            "Epoch: 83/100, Loss: 0.140106\n",
            "Epoch: 84/100, Loss: 0.137159\n",
            "Epoch: 85/100, Loss: 0.134254\n",
            "Epoch: 86/100, Loss: 0.131402\n",
            "Epoch: 87/100, Loss: 0.128595\n",
            "Epoch: 88/100, Loss: 0.125823\n",
            "Epoch: 89/100, Loss: 0.123091\n",
            "Epoch: 90/100, Loss: 0.120409\n",
            "Epoch: 91/100, Loss: 0.117785\n",
            "Epoch: 92/100, Loss: 0.115208\n",
            "Epoch: 93/100, Loss: 0.112690\n",
            "Epoch: 94/100, Loss: 0.110224\n",
            "Epoch: 95/100, Loss: 0.107788\n",
            "Epoch: 96/100, Loss: 0.105398\n",
            "Epoch: 97/100, Loss: 0.103060\n",
            "Epoch: 98/100, Loss: 0.100766\n",
            "Epoch: 99/100, Loss: 0.098484\n",
            "Epoch: 100/100, Loss: 0.096206\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6787\n",
            "Normalised mutual info score on k-means on latent space: 0.5930726135820129\n",
            "ARI score on k-means on latent space: 0.4989657244115998\n",
            "K-means cluster error on latent space: 42144.95703125\n",
            "K-means silhouette score on latent space: 0.19685227 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7089\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6976850310863156 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5581533871420535 \n",
            "\n",
            "ROUND NUMBER  10 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.914262\n",
            "Epoch: 2/100, Loss: 0.720511\n",
            "Epoch: 3/100, Loss: 0.647957\n",
            "Epoch: 4/100, Loss: 0.609066\n",
            "Epoch: 5/100, Loss: 0.584795\n",
            "Epoch: 6/100, Loss: 0.567218\n",
            "Epoch: 7/100, Loss: 0.552897\n",
            "Epoch: 8/100, Loss: 0.540403\n",
            "Epoch: 9/100, Loss: 0.529038\n",
            "Epoch: 10/100, Loss: 0.518471\n",
            "Epoch: 11/100, Loss: 0.508515\n",
            "Epoch: 12/100, Loss: 0.499043\n",
            "Epoch: 13/100, Loss: 0.489969\n",
            "Epoch: 14/100, Loss: 0.481227\n",
            "Epoch: 15/100, Loss: 0.472782\n",
            "Epoch: 16/100, Loss: 0.464594\n",
            "Epoch: 17/100, Loss: 0.456642\n",
            "Epoch: 18/100, Loss: 0.448901\n",
            "Epoch: 19/100, Loss: 0.441354\n",
            "Epoch: 20/100, Loss: 0.433983\n",
            "Epoch: 21/100, Loss: 0.426780\n",
            "Epoch: 22/100, Loss: 0.419733\n",
            "Epoch: 23/100, Loss: 0.412831\n",
            "Epoch: 24/100, Loss: 0.406060\n",
            "Epoch: 25/100, Loss: 0.399424\n",
            "Epoch: 26/100, Loss: 0.392914\n",
            "Epoch: 27/100, Loss: 0.386518\n",
            "Epoch: 28/100, Loss: 0.380230\n",
            "Epoch: 29/100, Loss: 0.374051\n",
            "Epoch: 30/100, Loss: 0.367972\n",
            "Epoch: 31/100, Loss: 0.361984\n",
            "Epoch: 32/100, Loss: 0.356091\n",
            "Epoch: 33/100, Loss: 0.350291\n",
            "Epoch: 34/100, Loss: 0.344575\n",
            "Epoch: 35/100, Loss: 0.338941\n",
            "Epoch: 36/100, Loss: 0.333394\n",
            "Epoch: 37/100, Loss: 0.327918\n",
            "Epoch: 38/100, Loss: 0.322522\n",
            "Epoch: 39/100, Loss: 0.317200\n",
            "Epoch: 40/100, Loss: 0.311947\n",
            "Epoch: 41/100, Loss: 0.306768\n",
            "Epoch: 42/100, Loss: 0.301655\n",
            "Epoch: 43/100, Loss: 0.296609\n",
            "Epoch: 44/100, Loss: 0.291627\n",
            "Epoch: 45/100, Loss: 0.286708\n",
            "Epoch: 46/100, Loss: 0.281852\n",
            "Epoch: 47/100, Loss: 0.277059\n",
            "Epoch: 48/100, Loss: 0.272324\n",
            "Epoch: 49/100, Loss: 0.267647\n",
            "Epoch: 50/100, Loss: 0.263027\n",
            "Epoch: 51/100, Loss: 0.258463\n",
            "Epoch: 52/100, Loss: 0.253959\n",
            "Epoch: 53/100, Loss: 0.249512\n",
            "Epoch: 54/100, Loss: 0.245117\n",
            "Epoch: 55/100, Loss: 0.240778\n",
            "Epoch: 56/100, Loss: 0.236496\n",
            "Epoch: 57/100, Loss: 0.232265\n",
            "Epoch: 58/100, Loss: 0.228087\n",
            "Epoch: 59/100, Loss: 0.223960\n",
            "Epoch: 60/100, Loss: 0.219887\n",
            "Epoch: 61/100, Loss: 0.215863\n",
            "Epoch: 62/100, Loss: 0.211892\n",
            "Epoch: 63/100, Loss: 0.207969\n",
            "Epoch: 64/100, Loss: 0.204099\n",
            "Epoch: 65/100, Loss: 0.200275\n",
            "Epoch: 66/100, Loss: 0.196497\n",
            "Epoch: 67/100, Loss: 0.192770\n",
            "Epoch: 68/100, Loss: 0.189091\n",
            "Epoch: 69/100, Loss: 0.185462\n",
            "Epoch: 70/100, Loss: 0.181881\n",
            "Epoch: 71/100, Loss: 0.178345\n",
            "Epoch: 72/100, Loss: 0.174859\n",
            "Epoch: 73/100, Loss: 0.171418\n",
            "Epoch: 74/100, Loss: 0.168026\n",
            "Epoch: 75/100, Loss: 0.164680\n",
            "Epoch: 76/100, Loss: 0.161381\n",
            "Epoch: 77/100, Loss: 0.158123\n",
            "Epoch: 78/100, Loss: 0.154911\n",
            "Epoch: 79/100, Loss: 0.151744\n",
            "Epoch: 80/100, Loss: 0.148623\n",
            "Epoch: 81/100, Loss: 0.145552\n",
            "Epoch: 82/100, Loss: 0.142524\n",
            "Epoch: 83/100, Loss: 0.139547\n",
            "Epoch: 84/100, Loss: 0.136620\n",
            "Epoch: 85/100, Loss: 0.133741\n",
            "Epoch: 86/100, Loss: 0.130914\n",
            "Epoch: 87/100, Loss: 0.128138\n",
            "Epoch: 88/100, Loss: 0.125408\n",
            "Epoch: 89/100, Loss: 0.122705\n",
            "Epoch: 90/100, Loss: 0.120029\n",
            "Epoch: 91/100, Loss: 0.117370\n",
            "Epoch: 92/100, Loss: 0.114718\n",
            "Epoch: 93/100, Loss: 0.112135\n",
            "Epoch: 94/100, Loss: 0.109623\n",
            "Epoch: 95/100, Loss: 0.107147\n",
            "Epoch: 96/100, Loss: 0.104702\n",
            "Epoch: 97/100, Loss: 0.102299\n",
            "Epoch: 98/100, Loss: 0.099941\n",
            "Epoch: 99/100, Loss: 0.097626\n",
            "Epoch: 100/100, Loss: 0.095331\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.658\n",
            "Normalised mutual info score on k-means on latent space: 0.5903539648883684\n",
            "ARI score on k-means on latent space: 0.48760801571286866\n",
            "K-means cluster error on latent space: 44513.640625\n",
            "K-means silhouette score on latent space: 0.19160786 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7218\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6845044967736652 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5671377009811571 \n",
            "\n",
            "Average k-means accuracy score at latent space: 0.6659900000000001 \n",
            "\n",
            "Average k-means NMI score at latent space: 0.5998940920838531 \n",
            "\n",
            "Average k-means ARI score at latent space: 0.4983597708282471 \n",
            "\n",
            "Average agglomerative clustering accuracy score at latent space: 0.71495 \n",
            "\n",
            "Average agglomerative clustering NMI score at latent space: 0.68207208789308 \n",
            "\n",
            "Average agglomerative clustering ARI score at latent space: 0.5659738681502324 \n",
            "\n",
            "Average k-means silhouette score on latent space: 0.18919119685888292 \n",
            "\n",
            "Average k-means cluster error on latent space: 43090.312109375 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_for_k_40 = run_experiment(40, dataloader, 784, datapoints = images, datapoints_reshaped = images_reshaped, labels = labels, epochs=100, hl1_neurons=32, hl2_neurons=64, hl3_neurons=128, latent_dimension=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6t-Lv1M77ycE",
        "outputId": "0a3b0fb5-e219-4dea-9732-057e8c93d41d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment results for k-means with k = 40 clusters:\n",
            "\n",
            "Running k-means algorithm in order to get our pseudolabels: \n",
            "\n",
            "K_means greedy accuracy score (initial space): 0.7844\n",
            "Normalised mutual info score (initial space): 0.6759921486097289\n",
            "ARI (initial space): 0.6199536553629289 \n",
            "\n",
            "Using the autoencoder model on our data: \n",
            "\n",
            "ROUND NUMBER  1 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.933880\n",
            "Epoch: 2/100, Loss: 0.761963\n",
            "Epoch: 3/100, Loss: 0.672428\n",
            "Epoch: 4/100, Loss: 0.621254\n",
            "Epoch: 5/100, Loss: 0.591482\n",
            "Epoch: 6/100, Loss: 0.571616\n",
            "Epoch: 7/100, Loss: 0.556237\n",
            "Epoch: 8/100, Loss: 0.543176\n",
            "Epoch: 9/100, Loss: 0.531543\n",
            "Epoch: 10/100, Loss: 0.520840\n",
            "Epoch: 11/100, Loss: 0.510788\n",
            "Epoch: 12/100, Loss: 0.501241\n",
            "Epoch: 13/100, Loss: 0.492117\n",
            "Epoch: 14/100, Loss: 0.483344\n",
            "Epoch: 15/100, Loss: 0.474881\n",
            "Epoch: 16/100, Loss: 0.466684\n",
            "Epoch: 17/100, Loss: 0.458723\n",
            "Epoch: 18/100, Loss: 0.450972\n",
            "Epoch: 19/100, Loss: 0.443415\n",
            "Epoch: 20/100, Loss: 0.436040\n",
            "Epoch: 21/100, Loss: 0.428831\n",
            "Epoch: 22/100, Loss: 0.421782\n",
            "Epoch: 23/100, Loss: 0.414875\n",
            "Epoch: 24/100, Loss: 0.408104\n",
            "Epoch: 25/100, Loss: 0.401458\n",
            "Epoch: 26/100, Loss: 0.394934\n",
            "Epoch: 27/100, Loss: 0.388527\n",
            "Epoch: 28/100, Loss: 0.382233\n",
            "Epoch: 29/100, Loss: 0.376042\n",
            "Epoch: 30/100, Loss: 0.369951\n",
            "Epoch: 31/100, Loss: 0.363956\n",
            "Epoch: 32/100, Loss: 0.358054\n",
            "Epoch: 33/100, Loss: 0.352241\n",
            "Epoch: 34/100, Loss: 0.346514\n",
            "Epoch: 35/100, Loss: 0.340870\n",
            "Epoch: 36/100, Loss: 0.335301\n",
            "Epoch: 37/100, Loss: 0.329811\n",
            "Epoch: 38/100, Loss: 0.324395\n",
            "Epoch: 39/100, Loss: 0.319055\n",
            "Epoch: 40/100, Loss: 0.313785\n",
            "Epoch: 41/100, Loss: 0.308585\n",
            "Epoch: 42/100, Loss: 0.303454\n",
            "Epoch: 43/100, Loss: 0.298389\n",
            "Epoch: 44/100, Loss: 0.293388\n",
            "Epoch: 45/100, Loss: 0.288451\n",
            "Epoch: 46/100, Loss: 0.283577\n",
            "Epoch: 47/100, Loss: 0.278763\n",
            "Epoch: 48/100, Loss: 0.274012\n",
            "Epoch: 49/100, Loss: 0.269317\n",
            "Epoch: 50/100, Loss: 0.264681\n",
            "Epoch: 51/100, Loss: 0.260100\n",
            "Epoch: 52/100, Loss: 0.255576\n",
            "Epoch: 53/100, Loss: 0.251110\n",
            "Epoch: 54/100, Loss: 0.246698\n",
            "Epoch: 55/100, Loss: 0.242338\n",
            "Epoch: 56/100, Loss: 0.238036\n",
            "Epoch: 57/100, Loss: 0.233784\n",
            "Epoch: 58/100, Loss: 0.229585\n",
            "Epoch: 59/100, Loss: 0.225440\n",
            "Epoch: 60/100, Loss: 0.221347\n",
            "Epoch: 61/100, Loss: 0.217305\n",
            "Epoch: 62/100, Loss: 0.213313\n",
            "Epoch: 63/100, Loss: 0.209370\n",
            "Epoch: 64/100, Loss: 0.205478\n",
            "Epoch: 65/100, Loss: 0.201636\n",
            "Epoch: 66/100, Loss: 0.197841\n",
            "Epoch: 67/100, Loss: 0.194093\n",
            "Epoch: 68/100, Loss: 0.190397\n",
            "Epoch: 69/100, Loss: 0.186751\n",
            "Epoch: 70/100, Loss: 0.183151\n",
            "Epoch: 71/100, Loss: 0.179598\n",
            "Epoch: 72/100, Loss: 0.176092\n",
            "Epoch: 73/100, Loss: 0.172635\n",
            "Epoch: 74/100, Loss: 0.169228\n",
            "Epoch: 75/100, Loss: 0.165864\n",
            "Epoch: 76/100, Loss: 0.162548\n",
            "Epoch: 77/100, Loss: 0.159281\n",
            "Epoch: 78/100, Loss: 0.156060\n",
            "Epoch: 79/100, Loss: 0.152885\n",
            "Epoch: 80/100, Loss: 0.149758\n",
            "Epoch: 81/100, Loss: 0.146680\n",
            "Epoch: 82/100, Loss: 0.143654\n",
            "Epoch: 83/100, Loss: 0.140682\n",
            "Epoch: 84/100, Loss: 0.137768\n",
            "Epoch: 85/100, Loss: 0.134910\n",
            "Epoch: 86/100, Loss: 0.132096\n",
            "Epoch: 87/100, Loss: 0.129333\n",
            "Epoch: 88/100, Loss: 0.126562\n",
            "Epoch: 89/100, Loss: 0.123746\n",
            "Epoch: 90/100, Loss: 0.120958\n",
            "Epoch: 91/100, Loss: 0.118251\n",
            "Epoch: 92/100, Loss: 0.115597\n",
            "Epoch: 93/100, Loss: 0.113000\n",
            "Epoch: 94/100, Loss: 0.110454\n",
            "Epoch: 95/100, Loss: 0.107959\n",
            "Epoch: 96/100, Loss: 0.105515\n",
            "Epoch: 97/100, Loss: 0.103115\n",
            "Epoch: 98/100, Loss: 0.100765\n",
            "Epoch: 99/100, Loss: 0.098456\n",
            "Epoch: 100/100, Loss: 0.096191\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6843\n",
            "Normalised mutual info score on k-means on latent space: 0.6115285975588497\n",
            "ARI score on k-means on latent space: 0.5220881012459171\n",
            "K-means cluster error on latent space: 40432.953125\n",
            "K-means silhouette score on latent space: 0.18276982 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6766\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.663517613984489 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5371886599144124 \n",
            "\n",
            "ROUND NUMBER  2 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.926788\n",
            "Epoch: 2/100, Loss: 0.731722\n",
            "Epoch: 3/100, Loss: 0.651430\n",
            "Epoch: 4/100, Loss: 0.611061\n",
            "Epoch: 5/100, Loss: 0.586830\n",
            "Epoch: 6/100, Loss: 0.569352\n",
            "Epoch: 7/100, Loss: 0.554928\n",
            "Epoch: 8/100, Loss: 0.542257\n",
            "Epoch: 9/100, Loss: 0.530732\n",
            "Epoch: 10/100, Loss: 0.520039\n",
            "Epoch: 11/100, Loss: 0.509983\n",
            "Epoch: 12/100, Loss: 0.500432\n",
            "Epoch: 13/100, Loss: 0.491299\n",
            "Epoch: 14/100, Loss: 0.482523\n",
            "Epoch: 15/100, Loss: 0.474047\n",
            "Epoch: 16/100, Loss: 0.465844\n",
            "Epoch: 17/100, Loss: 0.457876\n",
            "Epoch: 18/100, Loss: 0.450121\n",
            "Epoch: 19/100, Loss: 0.442562\n",
            "Epoch: 20/100, Loss: 0.435181\n",
            "Epoch: 21/100, Loss: 0.427966\n",
            "Epoch: 22/100, Loss: 0.420900\n",
            "Epoch: 23/100, Loss: 0.413975\n",
            "Epoch: 24/100, Loss: 0.407188\n",
            "Epoch: 25/100, Loss: 0.400531\n",
            "Epoch: 26/100, Loss: 0.393994\n",
            "Epoch: 27/100, Loss: 0.387571\n",
            "Epoch: 28/100, Loss: 0.381261\n",
            "Epoch: 29/100, Loss: 0.375057\n",
            "Epoch: 30/100, Loss: 0.368954\n",
            "Epoch: 31/100, Loss: 0.362948\n",
            "Epoch: 32/100, Loss: 0.357033\n",
            "Epoch: 33/100, Loss: 0.351210\n",
            "Epoch: 34/100, Loss: 0.345470\n",
            "Epoch: 35/100, Loss: 0.339817\n",
            "Epoch: 36/100, Loss: 0.334245\n",
            "Epoch: 37/100, Loss: 0.328749\n",
            "Epoch: 38/100, Loss: 0.323335\n",
            "Epoch: 39/100, Loss: 0.317996\n",
            "Epoch: 40/100, Loss: 0.312723\n",
            "Epoch: 41/100, Loss: 0.307523\n",
            "Epoch: 42/100, Loss: 0.302387\n",
            "Epoch: 43/100, Loss: 0.297321\n",
            "Epoch: 44/100, Loss: 0.292317\n",
            "Epoch: 45/100, Loss: 0.287378\n",
            "Epoch: 46/100, Loss: 0.282502\n",
            "Epoch: 47/100, Loss: 0.277691\n",
            "Epoch: 48/100, Loss: 0.272936\n",
            "Epoch: 49/100, Loss: 0.268242\n",
            "Epoch: 50/100, Loss: 0.263610\n",
            "Epoch: 51/100, Loss: 0.259033\n",
            "Epoch: 52/100, Loss: 0.254514\n",
            "Epoch: 53/100, Loss: 0.250049\n",
            "Epoch: 54/100, Loss: 0.245641\n",
            "Epoch: 55/100, Loss: 0.241290\n",
            "Epoch: 56/100, Loss: 0.236989\n",
            "Epoch: 57/100, Loss: 0.232742\n",
            "Epoch: 58/100, Loss: 0.228549\n",
            "Epoch: 59/100, Loss: 0.224411\n",
            "Epoch: 60/100, Loss: 0.220324\n",
            "Epoch: 61/100, Loss: 0.216286\n",
            "Epoch: 62/100, Loss: 0.212299\n",
            "Epoch: 63/100, Loss: 0.208361\n",
            "Epoch: 64/100, Loss: 0.204474\n",
            "Epoch: 65/100, Loss: 0.200638\n",
            "Epoch: 66/100, Loss: 0.196852\n",
            "Epoch: 67/100, Loss: 0.193114\n",
            "Epoch: 68/100, Loss: 0.189424\n",
            "Epoch: 69/100, Loss: 0.185786\n",
            "Epoch: 70/100, Loss: 0.182196\n",
            "Epoch: 71/100, Loss: 0.178654\n",
            "Epoch: 72/100, Loss: 0.175158\n",
            "Epoch: 73/100, Loss: 0.171710\n",
            "Epoch: 74/100, Loss: 0.168307\n",
            "Epoch: 75/100, Loss: 0.164952\n",
            "Epoch: 76/100, Loss: 0.161647\n",
            "Epoch: 77/100, Loss: 0.158391\n",
            "Epoch: 78/100, Loss: 0.155185\n",
            "Epoch: 79/100, Loss: 0.152023\n",
            "Epoch: 80/100, Loss: 0.148908\n",
            "Epoch: 81/100, Loss: 0.145849\n",
            "Epoch: 82/100, Loss: 0.142852\n",
            "Epoch: 83/100, Loss: 0.139907\n",
            "Epoch: 84/100, Loss: 0.137001\n",
            "Epoch: 85/100, Loss: 0.134129\n",
            "Epoch: 86/100, Loss: 0.131260\n",
            "Epoch: 87/100, Loss: 0.128393\n",
            "Epoch: 88/100, Loss: 0.125577\n",
            "Epoch: 89/100, Loss: 0.122818\n",
            "Epoch: 90/100, Loss: 0.120101\n",
            "Epoch: 91/100, Loss: 0.117446\n",
            "Epoch: 92/100, Loss: 0.114857\n",
            "Epoch: 93/100, Loss: 0.112337\n",
            "Epoch: 94/100, Loss: 0.109850\n",
            "Epoch: 95/100, Loss: 0.107397\n",
            "Epoch: 96/100, Loss: 0.104994\n",
            "Epoch: 97/100, Loss: 0.102633\n",
            "Epoch: 98/100, Loss: 0.100315\n",
            "Epoch: 99/100, Loss: 0.098051\n",
            "Epoch: 100/100, Loss: 0.095812\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6182\n",
            "Normalised mutual info score on k-means on latent space: 0.5424816971387053\n",
            "ARI score on k-means on latent space: 0.42952655222523284\n",
            "K-means cluster error on latent space: 42536.48828125\n",
            "K-means silhouette score on latent space: 0.173851 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6192\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6317304007838671 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.455496036327339 \n",
            "\n",
            "ROUND NUMBER  3 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.932234\n",
            "Epoch: 2/100, Loss: 0.743156\n",
            "Epoch: 3/100, Loss: 0.660751\n",
            "Epoch: 4/100, Loss: 0.617541\n",
            "Epoch: 5/100, Loss: 0.590546\n",
            "Epoch: 6/100, Loss: 0.571595\n",
            "Epoch: 7/100, Loss: 0.556510\n",
            "Epoch: 8/100, Loss: 0.543521\n",
            "Epoch: 9/100, Loss: 0.531842\n",
            "Epoch: 10/100, Loss: 0.521059\n",
            "Epoch: 11/100, Loss: 0.510935\n",
            "Epoch: 12/100, Loss: 0.501334\n",
            "Epoch: 13/100, Loss: 0.492151\n",
            "Epoch: 14/100, Loss: 0.483322\n",
            "Epoch: 15/100, Loss: 0.474801\n",
            "Epoch: 16/100, Loss: 0.466544\n",
            "Epoch: 17/100, Loss: 0.458530\n",
            "Epoch: 18/100, Loss: 0.450730\n",
            "Epoch: 19/100, Loss: 0.443126\n",
            "Epoch: 20/100, Loss: 0.435705\n",
            "Epoch: 21/100, Loss: 0.428453\n",
            "Epoch: 22/100, Loss: 0.421354\n",
            "Epoch: 23/100, Loss: 0.414402\n",
            "Epoch: 24/100, Loss: 0.407594\n",
            "Epoch: 25/100, Loss: 0.400911\n",
            "Epoch: 26/100, Loss: 0.394355\n",
            "Epoch: 27/100, Loss: 0.387915\n",
            "Epoch: 28/100, Loss: 0.381587\n",
            "Epoch: 29/100, Loss: 0.375367\n",
            "Epoch: 30/100, Loss: 0.369252\n",
            "Epoch: 31/100, Loss: 0.363234\n",
            "Epoch: 32/100, Loss: 0.357308\n",
            "Epoch: 33/100, Loss: 0.351473\n",
            "Epoch: 34/100, Loss: 0.345727\n",
            "Epoch: 35/100, Loss: 0.340062\n",
            "Epoch: 36/100, Loss: 0.334479\n",
            "Epoch: 37/100, Loss: 0.328975\n",
            "Epoch: 38/100, Loss: 0.323549\n",
            "Epoch: 39/100, Loss: 0.318199\n",
            "Epoch: 40/100, Loss: 0.312919\n",
            "Epoch: 41/100, Loss: 0.307713\n",
            "Epoch: 42/100, Loss: 0.302576\n",
            "Epoch: 43/100, Loss: 0.297504\n",
            "Epoch: 44/100, Loss: 0.292496\n",
            "Epoch: 45/100, Loss: 0.287555\n",
            "Epoch: 46/100, Loss: 0.282675\n",
            "Epoch: 47/100, Loss: 0.277856\n",
            "Epoch: 48/100, Loss: 0.273098\n",
            "Epoch: 49/100, Loss: 0.268401\n",
            "Epoch: 50/100, Loss: 0.263760\n",
            "Epoch: 51/100, Loss: 0.259177\n",
            "Epoch: 52/100, Loss: 0.254652\n",
            "Epoch: 53/100, Loss: 0.250184\n",
            "Epoch: 54/100, Loss: 0.245770\n",
            "Epoch: 55/100, Loss: 0.241409\n",
            "Epoch: 56/100, Loss: 0.237103\n",
            "Epoch: 57/100, Loss: 0.232852\n",
            "Epoch: 58/100, Loss: 0.228653\n",
            "Epoch: 59/100, Loss: 0.224508\n",
            "Epoch: 60/100, Loss: 0.220414\n",
            "Epoch: 61/100, Loss: 0.216370\n",
            "Epoch: 62/100, Loss: 0.212381\n",
            "Epoch: 63/100, Loss: 0.208440\n",
            "Epoch: 64/100, Loss: 0.204549\n",
            "Epoch: 65/100, Loss: 0.200708\n",
            "Epoch: 66/100, Loss: 0.196916\n",
            "Epoch: 67/100, Loss: 0.193173\n",
            "Epoch: 68/100, Loss: 0.189480\n",
            "Epoch: 69/100, Loss: 0.185835\n",
            "Epoch: 70/100, Loss: 0.182239\n",
            "Epoch: 71/100, Loss: 0.178687\n",
            "Epoch: 72/100, Loss: 0.175186\n",
            "Epoch: 73/100, Loss: 0.171730\n",
            "Epoch: 74/100, Loss: 0.168321\n",
            "Epoch: 75/100, Loss: 0.164959\n",
            "Epoch: 76/100, Loss: 0.161640\n",
            "Epoch: 77/100, Loss: 0.158375\n",
            "Epoch: 78/100, Loss: 0.155156\n",
            "Epoch: 79/100, Loss: 0.151983\n",
            "Epoch: 80/100, Loss: 0.148860\n",
            "Epoch: 81/100, Loss: 0.145792\n",
            "Epoch: 82/100, Loss: 0.142773\n",
            "Epoch: 83/100, Loss: 0.139809\n",
            "Epoch: 84/100, Loss: 0.136898\n",
            "Epoch: 85/100, Loss: 0.134048\n",
            "Epoch: 86/100, Loss: 0.131241\n",
            "Epoch: 87/100, Loss: 0.128467\n",
            "Epoch: 88/100, Loss: 0.125719\n",
            "Epoch: 89/100, Loss: 0.122978\n",
            "Epoch: 90/100, Loss: 0.120246\n",
            "Epoch: 91/100, Loss: 0.117557\n",
            "Epoch: 92/100, Loss: 0.114936\n",
            "Epoch: 93/100, Loss: 0.112370\n",
            "Epoch: 94/100, Loss: 0.109848\n",
            "Epoch: 95/100, Loss: 0.107371\n",
            "Epoch: 96/100, Loss: 0.104944\n",
            "Epoch: 97/100, Loss: 0.102579\n",
            "Epoch: 98/100, Loss: 0.100259\n",
            "Epoch: 99/100, Loss: 0.097968\n",
            "Epoch: 100/100, Loss: 0.095702\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6556\n",
            "Normalised mutual info score on k-means on latent space: 0.5895184877124906\n",
            "ARI score on k-means on latent space: 0.4967697252648829\n",
            "K-means cluster error on latent space: 42812.46875\n",
            "K-means silhouette score on latent space: 0.18169022 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6419\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6469500932395417 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.4933214393739496 \n",
            "\n",
            "ROUND NUMBER  4 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.904880\n",
            "Epoch: 2/100, Loss: 0.722988\n",
            "Epoch: 3/100, Loss: 0.653419\n",
            "Epoch: 4/100, Loss: 0.616118\n",
            "Epoch: 5/100, Loss: 0.591032\n",
            "Epoch: 6/100, Loss: 0.571853\n",
            "Epoch: 7/100, Loss: 0.556209\n",
            "Epoch: 8/100, Loss: 0.542738\n",
            "Epoch: 9/100, Loss: 0.530761\n",
            "Epoch: 10/100, Loss: 0.519822\n",
            "Epoch: 11/100, Loss: 0.509620\n",
            "Epoch: 12/100, Loss: 0.499973\n",
            "Epoch: 13/100, Loss: 0.490773\n",
            "Epoch: 14/100, Loss: 0.481944\n",
            "Epoch: 15/100, Loss: 0.473433\n",
            "Epoch: 16/100, Loss: 0.465207\n",
            "Epoch: 17/100, Loss: 0.457217\n",
            "Epoch: 18/100, Loss: 0.449454\n",
            "Epoch: 19/100, Loss: 0.441881\n",
            "Epoch: 20/100, Loss: 0.434494\n",
            "Epoch: 21/100, Loss: 0.427273\n",
            "Epoch: 22/100, Loss: 0.420205\n",
            "Epoch: 23/100, Loss: 0.413279\n",
            "Epoch: 24/100, Loss: 0.406490\n",
            "Epoch: 25/100, Loss: 0.399834\n",
            "Epoch: 26/100, Loss: 0.393300\n",
            "Epoch: 27/100, Loss: 0.386882\n",
            "Epoch: 28/100, Loss: 0.380576\n",
            "Epoch: 29/100, Loss: 0.374374\n",
            "Epoch: 30/100, Loss: 0.368277\n",
            "Epoch: 31/100, Loss: 0.362275\n",
            "Epoch: 32/100, Loss: 0.356367\n",
            "Epoch: 33/100, Loss: 0.350548\n",
            "Epoch: 34/100, Loss: 0.344814\n",
            "Epoch: 35/100, Loss: 0.339164\n",
            "Epoch: 36/100, Loss: 0.333593\n",
            "Epoch: 37/100, Loss: 0.328106\n",
            "Epoch: 38/100, Loss: 0.322688\n",
            "Epoch: 39/100, Loss: 0.317346\n",
            "Epoch: 40/100, Loss: 0.312075\n",
            "Epoch: 41/100, Loss: 0.306878\n",
            "Epoch: 42/100, Loss: 0.301747\n",
            "Epoch: 43/100, Loss: 0.296686\n",
            "Epoch: 44/100, Loss: 0.291688\n",
            "Epoch: 45/100, Loss: 0.286755\n",
            "Epoch: 46/100, Loss: 0.281883\n",
            "Epoch: 47/100, Loss: 0.277073\n",
            "Epoch: 48/100, Loss: 0.272322\n",
            "Epoch: 49/100, Loss: 0.267633\n",
            "Epoch: 50/100, Loss: 0.263002\n",
            "Epoch: 51/100, Loss: 0.258425\n",
            "Epoch: 52/100, Loss: 0.253911\n",
            "Epoch: 53/100, Loss: 0.249451\n",
            "Epoch: 54/100, Loss: 0.245044\n",
            "Epoch: 55/100, Loss: 0.240695\n",
            "Epoch: 56/100, Loss: 0.236400\n",
            "Epoch: 57/100, Loss: 0.232156\n",
            "Epoch: 58/100, Loss: 0.227963\n",
            "Epoch: 59/100, Loss: 0.223825\n",
            "Epoch: 60/100, Loss: 0.219737\n",
            "Epoch: 61/100, Loss: 0.215701\n",
            "Epoch: 62/100, Loss: 0.211715\n",
            "Epoch: 63/100, Loss: 0.207780\n",
            "Epoch: 64/100, Loss: 0.203895\n",
            "Epoch: 65/100, Loss: 0.200059\n",
            "Epoch: 66/100, Loss: 0.196273\n",
            "Epoch: 67/100, Loss: 0.192534\n",
            "Epoch: 68/100, Loss: 0.188846\n",
            "Epoch: 69/100, Loss: 0.185205\n",
            "Epoch: 70/100, Loss: 0.181616\n",
            "Epoch: 71/100, Loss: 0.178069\n",
            "Epoch: 72/100, Loss: 0.174573\n",
            "Epoch: 73/100, Loss: 0.171125\n",
            "Epoch: 74/100, Loss: 0.167725\n",
            "Epoch: 75/100, Loss: 0.164373\n",
            "Epoch: 76/100, Loss: 0.161073\n",
            "Epoch: 77/100, Loss: 0.157819\n",
            "Epoch: 78/100, Loss: 0.154614\n",
            "Epoch: 79/100, Loss: 0.151457\n",
            "Epoch: 80/100, Loss: 0.148361\n",
            "Epoch: 81/100, Loss: 0.145327\n",
            "Epoch: 82/100, Loss: 0.142352\n",
            "Epoch: 83/100, Loss: 0.139447\n",
            "Epoch: 84/100, Loss: 0.136603\n",
            "Epoch: 85/100, Loss: 0.133754\n",
            "Epoch: 86/100, Loss: 0.130864\n",
            "Epoch: 87/100, Loss: 0.127977\n",
            "Epoch: 88/100, Loss: 0.125118\n",
            "Epoch: 89/100, Loss: 0.122323\n",
            "Epoch: 90/100, Loss: 0.119590\n",
            "Epoch: 91/100, Loss: 0.116930\n",
            "Epoch: 92/100, Loss: 0.114335\n",
            "Epoch: 93/100, Loss: 0.111783\n",
            "Epoch: 94/100, Loss: 0.109282\n",
            "Epoch: 95/100, Loss: 0.106840\n",
            "Epoch: 96/100, Loss: 0.104466\n",
            "Epoch: 97/100, Loss: 0.102161\n",
            "Epoch: 98/100, Loss: 0.099909\n",
            "Epoch: 99/100, Loss: 0.097680\n",
            "Epoch: 100/100, Loss: 0.095421\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.7151\n",
            "Normalised mutual info score on k-means on latent space: 0.6327680896353204\n",
            "ARI score on k-means on latent space: 0.5453311820594939\n",
            "K-means cluster error on latent space: 43529.046875\n",
            "K-means silhouette score on latent space: 0.19267605 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7768\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.7421935491500958 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.6537669721230923 \n",
            "\n",
            "ROUND NUMBER  5 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.923649\n",
            "Epoch: 2/100, Loss: 0.723815\n",
            "Epoch: 3/100, Loss: 0.652168\n",
            "Epoch: 4/100, Loss: 0.614378\n",
            "Epoch: 5/100, Loss: 0.589862\n",
            "Epoch: 6/100, Loss: 0.571208\n",
            "Epoch: 7/100, Loss: 0.555891\n",
            "Epoch: 8/100, Loss: 0.542695\n",
            "Epoch: 9/100, Loss: 0.530844\n",
            "Epoch: 10/100, Loss: 0.519927\n",
            "Epoch: 11/100, Loss: 0.509709\n",
            "Epoch: 12/100, Loss: 0.500039\n",
            "Epoch: 13/100, Loss: 0.490818\n",
            "Epoch: 14/100, Loss: 0.481961\n",
            "Epoch: 15/100, Loss: 0.473419\n",
            "Epoch: 16/100, Loss: 0.465150\n",
            "Epoch: 17/100, Loss: 0.457123\n",
            "Epoch: 18/100, Loss: 0.449313\n",
            "Epoch: 19/100, Loss: 0.441702\n",
            "Epoch: 20/100, Loss: 0.434282\n",
            "Epoch: 21/100, Loss: 0.427027\n",
            "Epoch: 22/100, Loss: 0.419932\n",
            "Epoch: 23/100, Loss: 0.412993\n",
            "Epoch: 24/100, Loss: 0.406195\n",
            "Epoch: 25/100, Loss: 0.399525\n",
            "Epoch: 26/100, Loss: 0.392979\n",
            "Epoch: 27/100, Loss: 0.386554\n",
            "Epoch: 28/100, Loss: 0.380243\n",
            "Epoch: 29/100, Loss: 0.374039\n",
            "Epoch: 30/100, Loss: 0.367937\n",
            "Epoch: 31/100, Loss: 0.361935\n",
            "Epoch: 32/100, Loss: 0.356027\n",
            "Epoch: 33/100, Loss: 0.350206\n",
            "Epoch: 34/100, Loss: 0.344472\n",
            "Epoch: 35/100, Loss: 0.338823\n",
            "Epoch: 36/100, Loss: 0.333257\n",
            "Epoch: 37/100, Loss: 0.327766\n",
            "Epoch: 38/100, Loss: 0.322352\n",
            "Epoch: 39/100, Loss: 0.317010\n",
            "Epoch: 40/100, Loss: 0.311739\n",
            "Epoch: 41/100, Loss: 0.306539\n",
            "Epoch: 42/100, Loss: 0.301404\n",
            "Epoch: 43/100, Loss: 0.296338\n",
            "Epoch: 44/100, Loss: 0.291336\n",
            "Epoch: 45/100, Loss: 0.286398\n",
            "Epoch: 46/100, Loss: 0.281524\n",
            "Epoch: 47/100, Loss: 0.276713\n",
            "Epoch: 48/100, Loss: 0.271965\n",
            "Epoch: 49/100, Loss: 0.267275\n",
            "Epoch: 50/100, Loss: 0.262644\n",
            "Epoch: 51/100, Loss: 0.258072\n",
            "Epoch: 52/100, Loss: 0.253556\n",
            "Epoch: 53/100, Loss: 0.249093\n",
            "Epoch: 54/100, Loss: 0.244688\n",
            "Epoch: 55/100, Loss: 0.240334\n",
            "Epoch: 56/100, Loss: 0.236033\n",
            "Epoch: 57/100, Loss: 0.231785\n",
            "Epoch: 58/100, Loss: 0.227594\n",
            "Epoch: 59/100, Loss: 0.223454\n",
            "Epoch: 60/100, Loss: 0.219365\n",
            "Epoch: 61/100, Loss: 0.215329\n",
            "Epoch: 62/100, Loss: 0.211343\n",
            "Epoch: 63/100, Loss: 0.207411\n",
            "Epoch: 64/100, Loss: 0.203529\n",
            "Epoch: 65/100, Loss: 0.199695\n",
            "Epoch: 66/100, Loss: 0.195912\n",
            "Epoch: 67/100, Loss: 0.192177\n",
            "Epoch: 68/100, Loss: 0.188492\n",
            "Epoch: 69/100, Loss: 0.184856\n",
            "Epoch: 70/100, Loss: 0.181266\n",
            "Epoch: 71/100, Loss: 0.177725\n",
            "Epoch: 72/100, Loss: 0.174232\n",
            "Epoch: 73/100, Loss: 0.170781\n",
            "Epoch: 74/100, Loss: 0.167383\n",
            "Epoch: 75/100, Loss: 0.164026\n",
            "Epoch: 76/100, Loss: 0.160716\n",
            "Epoch: 77/100, Loss: 0.157452\n",
            "Epoch: 78/100, Loss: 0.154236\n",
            "Epoch: 79/100, Loss: 0.151067\n",
            "Epoch: 80/100, Loss: 0.147950\n",
            "Epoch: 81/100, Loss: 0.144881\n",
            "Epoch: 82/100, Loss: 0.141861\n",
            "Epoch: 83/100, Loss: 0.138888\n",
            "Epoch: 84/100, Loss: 0.135974\n",
            "Epoch: 85/100, Loss: 0.133108\n",
            "Epoch: 86/100, Loss: 0.130287\n",
            "Epoch: 87/100, Loss: 0.127499\n",
            "Epoch: 88/100, Loss: 0.124717\n",
            "Epoch: 89/100, Loss: 0.121945\n",
            "Epoch: 90/100, Loss: 0.119208\n",
            "Epoch: 91/100, Loss: 0.116531\n",
            "Epoch: 92/100, Loss: 0.113904\n",
            "Epoch: 93/100, Loss: 0.111328\n",
            "Epoch: 94/100, Loss: 0.108792\n",
            "Epoch: 95/100, Loss: 0.106307\n",
            "Epoch: 96/100, Loss: 0.103867\n",
            "Epoch: 97/100, Loss: 0.101475\n",
            "Epoch: 98/100, Loss: 0.099139\n",
            "Epoch: 99/100, Loss: 0.096856\n",
            "Epoch: 100/100, Loss: 0.094620\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6452\n",
            "Normalised mutual info score on k-means on latent space: 0.5643896085631813\n",
            "ARI score on k-means on latent space: 0.4613569506684822\n",
            "K-means cluster error on latent space: 43575.15234375\n",
            "K-means silhouette score on latent space: 0.17763172 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7333\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6926708891026954 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.591975504125807 \n",
            "\n",
            "ROUND NUMBER  6 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.917680\n",
            "Epoch: 2/100, Loss: 0.726051\n",
            "Epoch: 3/100, Loss: 0.648684\n",
            "Epoch: 4/100, Loss: 0.610391\n",
            "Epoch: 5/100, Loss: 0.586388\n",
            "Epoch: 6/100, Loss: 0.568566\n",
            "Epoch: 7/100, Loss: 0.553832\n",
            "Epoch: 8/100, Loss: 0.540979\n",
            "Epoch: 9/100, Loss: 0.529379\n",
            "Epoch: 10/100, Loss: 0.518656\n",
            "Epoch: 11/100, Loss: 0.508602\n",
            "Epoch: 12/100, Loss: 0.499067\n",
            "Epoch: 13/100, Loss: 0.489963\n",
            "Epoch: 14/100, Loss: 0.481215\n",
            "Epoch: 15/100, Loss: 0.472771\n",
            "Epoch: 16/100, Loss: 0.464586\n",
            "Epoch: 17/100, Loss: 0.456636\n",
            "Epoch: 18/100, Loss: 0.448906\n",
            "Epoch: 19/100, Loss: 0.441373\n",
            "Epoch: 20/100, Loss: 0.434020\n",
            "Epoch: 21/100, Loss: 0.426830\n",
            "Epoch: 22/100, Loss: 0.419794\n",
            "Epoch: 23/100, Loss: 0.412905\n",
            "Epoch: 24/100, Loss: 0.406152\n",
            "Epoch: 25/100, Loss: 0.399525\n",
            "Epoch: 26/100, Loss: 0.393018\n",
            "Epoch: 27/100, Loss: 0.386629\n",
            "Epoch: 28/100, Loss: 0.380347\n",
            "Epoch: 29/100, Loss: 0.374172\n",
            "Epoch: 30/100, Loss: 0.368094\n",
            "Epoch: 31/100, Loss: 0.362114\n",
            "Epoch: 32/100, Loss: 0.356227\n",
            "Epoch: 33/100, Loss: 0.350428\n",
            "Epoch: 34/100, Loss: 0.344713\n",
            "Epoch: 35/100, Loss: 0.339082\n",
            "Epoch: 36/100, Loss: 0.333529\n",
            "Epoch: 37/100, Loss: 0.328052\n",
            "Epoch: 38/100, Loss: 0.322654\n",
            "Epoch: 39/100, Loss: 0.317326\n",
            "Epoch: 40/100, Loss: 0.312073\n",
            "Epoch: 41/100, Loss: 0.306887\n",
            "Epoch: 42/100, Loss: 0.301770\n",
            "Epoch: 43/100, Loss: 0.296719\n",
            "Epoch: 44/100, Loss: 0.291735\n",
            "Epoch: 45/100, Loss: 0.286815\n",
            "Epoch: 46/100, Loss: 0.281957\n",
            "Epoch: 47/100, Loss: 0.277159\n",
            "Epoch: 48/100, Loss: 0.272419\n",
            "Epoch: 49/100, Loss: 0.267741\n",
            "Epoch: 50/100, Loss: 0.263119\n",
            "Epoch: 51/100, Loss: 0.258558\n",
            "Epoch: 52/100, Loss: 0.254049\n",
            "Epoch: 53/100, Loss: 0.249597\n",
            "Epoch: 54/100, Loss: 0.245200\n",
            "Epoch: 55/100, Loss: 0.240856\n",
            "Epoch: 56/100, Loss: 0.236567\n",
            "Epoch: 57/100, Loss: 0.232328\n",
            "Epoch: 58/100, Loss: 0.228145\n",
            "Epoch: 59/100, Loss: 0.224015\n",
            "Epoch: 60/100, Loss: 0.219937\n",
            "Epoch: 61/100, Loss: 0.215908\n",
            "Epoch: 62/100, Loss: 0.211931\n",
            "Epoch: 63/100, Loss: 0.208003\n",
            "Epoch: 64/100, Loss: 0.204127\n",
            "Epoch: 65/100, Loss: 0.200300\n",
            "Epoch: 66/100, Loss: 0.196520\n",
            "Epoch: 67/100, Loss: 0.192789\n",
            "Epoch: 68/100, Loss: 0.189105\n",
            "Epoch: 69/100, Loss: 0.185468\n",
            "Epoch: 70/100, Loss: 0.181884\n",
            "Epoch: 71/100, Loss: 0.178346\n",
            "Epoch: 72/100, Loss: 0.174854\n",
            "Epoch: 73/100, Loss: 0.171407\n",
            "Epoch: 74/100, Loss: 0.168007\n",
            "Epoch: 75/100, Loss: 0.164653\n",
            "Epoch: 76/100, Loss: 0.161345\n",
            "Epoch: 77/100, Loss: 0.158083\n",
            "Epoch: 78/100, Loss: 0.154866\n",
            "Epoch: 79/100, Loss: 0.151699\n",
            "Epoch: 80/100, Loss: 0.148576\n",
            "Epoch: 81/100, Loss: 0.145497\n",
            "Epoch: 82/100, Loss: 0.142468\n",
            "Epoch: 83/100, Loss: 0.139489\n",
            "Epoch: 84/100, Loss: 0.136555\n",
            "Epoch: 85/100, Loss: 0.133663\n",
            "Epoch: 86/100, Loss: 0.130802\n",
            "Epoch: 87/100, Loss: 0.127989\n",
            "Epoch: 88/100, Loss: 0.125235\n",
            "Epoch: 89/100, Loss: 0.122536\n",
            "Epoch: 90/100, Loss: 0.119899\n",
            "Epoch: 91/100, Loss: 0.117315\n",
            "Epoch: 92/100, Loss: 0.114753\n",
            "Epoch: 93/100, Loss: 0.112209\n",
            "Epoch: 94/100, Loss: 0.109670\n",
            "Epoch: 95/100, Loss: 0.107142\n",
            "Epoch: 96/100, Loss: 0.104657\n",
            "Epoch: 97/100, Loss: 0.102227\n",
            "Epoch: 98/100, Loss: 0.099858\n",
            "Epoch: 99/100, Loss: 0.097538\n",
            "Epoch: 100/100, Loss: 0.095266\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6962\n",
            "Normalised mutual info score on k-means on latent space: 0.6311030998085971\n",
            "ARI score on k-means on latent space: 0.5412317341094852\n",
            "K-means cluster error on latent space: 43688.21484375\n",
            "K-means silhouette score on latent space: 0.17643745 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7092\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6928988114793876 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5566287651396614 \n",
            "\n",
            "ROUND NUMBER  7 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.931885\n",
            "Epoch: 2/100, Loss: 0.740440\n",
            "Epoch: 3/100, Loss: 0.660741\n",
            "Epoch: 4/100, Loss: 0.619383\n",
            "Epoch: 5/100, Loss: 0.592227\n",
            "Epoch: 6/100, Loss: 0.572934\n",
            "Epoch: 7/100, Loss: 0.557742\n",
            "Epoch: 8/100, Loss: 0.544644\n",
            "Epoch: 9/100, Loss: 0.532853\n",
            "Epoch: 10/100, Loss: 0.521951\n",
            "Epoch: 11/100, Loss: 0.511720\n",
            "Epoch: 12/100, Loss: 0.502017\n",
            "Epoch: 13/100, Loss: 0.492750\n",
            "Epoch: 14/100, Loss: 0.483840\n",
            "Epoch: 15/100, Loss: 0.475235\n",
            "Epoch: 16/100, Loss: 0.466907\n",
            "Epoch: 17/100, Loss: 0.458827\n",
            "Epoch: 18/100, Loss: 0.450966\n",
            "Epoch: 19/100, Loss: 0.443307\n",
            "Epoch: 20/100, Loss: 0.435840\n",
            "Epoch: 21/100, Loss: 0.428551\n",
            "Epoch: 22/100, Loss: 0.421425\n",
            "Epoch: 23/100, Loss: 0.414454\n",
            "Epoch: 24/100, Loss: 0.407625\n",
            "Epoch: 25/100, Loss: 0.400930\n",
            "Epoch: 26/100, Loss: 0.394356\n",
            "Epoch: 27/100, Loss: 0.387899\n",
            "Epoch: 28/100, Loss: 0.381561\n",
            "Epoch: 29/100, Loss: 0.375324\n",
            "Epoch: 30/100, Loss: 0.369195\n",
            "Epoch: 31/100, Loss: 0.363164\n",
            "Epoch: 32/100, Loss: 0.357232\n",
            "Epoch: 33/100, Loss: 0.351391\n",
            "Epoch: 34/100, Loss: 0.345639\n",
            "Epoch: 35/100, Loss: 0.339971\n",
            "Epoch: 36/100, Loss: 0.334384\n",
            "Epoch: 37/100, Loss: 0.328877\n",
            "Epoch: 38/100, Loss: 0.323448\n",
            "Epoch: 39/100, Loss: 0.318094\n",
            "Epoch: 40/100, Loss: 0.312810\n",
            "Epoch: 41/100, Loss: 0.307599\n",
            "Epoch: 42/100, Loss: 0.302457\n",
            "Epoch: 43/100, Loss: 0.297380\n",
            "Epoch: 44/100, Loss: 0.292367\n",
            "Epoch: 45/100, Loss: 0.287421\n",
            "Epoch: 46/100, Loss: 0.282537\n",
            "Epoch: 47/100, Loss: 0.277717\n",
            "Epoch: 48/100, Loss: 0.272956\n",
            "Epoch: 49/100, Loss: 0.268259\n",
            "Epoch: 50/100, Loss: 0.263618\n",
            "Epoch: 51/100, Loss: 0.259040\n",
            "Epoch: 52/100, Loss: 0.254516\n",
            "Epoch: 53/100, Loss: 0.250050\n",
            "Epoch: 54/100, Loss: 0.245640\n",
            "Epoch: 55/100, Loss: 0.241284\n",
            "Epoch: 56/100, Loss: 0.236983\n",
            "Epoch: 57/100, Loss: 0.232735\n",
            "Epoch: 58/100, Loss: 0.228535\n",
            "Epoch: 59/100, Loss: 0.224394\n",
            "Epoch: 60/100, Loss: 0.220305\n",
            "Epoch: 61/100, Loss: 0.216266\n",
            "Epoch: 62/100, Loss: 0.212276\n",
            "Epoch: 63/100, Loss: 0.208338\n",
            "Epoch: 64/100, Loss: 0.204451\n",
            "Epoch: 65/100, Loss: 0.200613\n",
            "Epoch: 66/100, Loss: 0.196823\n",
            "Epoch: 67/100, Loss: 0.193082\n",
            "Epoch: 68/100, Loss: 0.189390\n",
            "Epoch: 69/100, Loss: 0.185748\n",
            "Epoch: 70/100, Loss: 0.182154\n",
            "Epoch: 71/100, Loss: 0.178607\n",
            "Epoch: 72/100, Loss: 0.175111\n",
            "Epoch: 73/100, Loss: 0.171658\n",
            "Epoch: 74/100, Loss: 0.168254\n",
            "Epoch: 75/100, Loss: 0.164893\n",
            "Epoch: 76/100, Loss: 0.161581\n",
            "Epoch: 77/100, Loss: 0.158314\n",
            "Epoch: 78/100, Loss: 0.155096\n",
            "Epoch: 79/100, Loss: 0.151923\n",
            "Epoch: 80/100, Loss: 0.148798\n",
            "Epoch: 81/100, Loss: 0.145718\n",
            "Epoch: 82/100, Loss: 0.142688\n",
            "Epoch: 83/100, Loss: 0.139706\n",
            "Epoch: 84/100, Loss: 0.136764\n",
            "Epoch: 85/100, Loss: 0.133870\n",
            "Epoch: 86/100, Loss: 0.131035\n",
            "Epoch: 87/100, Loss: 0.128251\n",
            "Epoch: 88/100, Loss: 0.125507\n",
            "Epoch: 89/100, Loss: 0.122788\n",
            "Epoch: 90/100, Loss: 0.120106\n",
            "Epoch: 91/100, Loss: 0.117476\n",
            "Epoch: 92/100, Loss: 0.114899\n",
            "Epoch: 93/100, Loss: 0.112361\n",
            "Epoch: 94/100, Loss: 0.109836\n",
            "Epoch: 95/100, Loss: 0.107342\n",
            "Epoch: 96/100, Loss: 0.104901\n",
            "Epoch: 97/100, Loss: 0.102487\n",
            "Epoch: 98/100, Loss: 0.100107\n",
            "Epoch: 99/100, Loss: 0.097749\n",
            "Epoch: 100/100, Loss: 0.095430\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6963\n",
            "Normalised mutual info score on k-means on latent space: 0.6062557418026354\n",
            "ARI score on k-means on latent space: 0.5187069549578897\n",
            "K-means cluster error on latent space: 42401.08984375\n",
            "K-means silhouette score on latent space: 0.1832855 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6769\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6679674573518627 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.531048000615042 \n",
            "\n",
            "ROUND NUMBER  8 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.922015\n",
            "Epoch: 2/100, Loss: 0.734874\n",
            "Epoch: 3/100, Loss: 0.658821\n",
            "Epoch: 4/100, Loss: 0.618120\n",
            "Epoch: 5/100, Loss: 0.590969\n",
            "Epoch: 6/100, Loss: 0.571318\n",
            "Epoch: 7/100, Loss: 0.555579\n",
            "Epoch: 8/100, Loss: 0.542193\n",
            "Epoch: 9/100, Loss: 0.530286\n",
            "Epoch: 10/100, Loss: 0.519356\n",
            "Epoch: 11/100, Loss: 0.509140\n",
            "Epoch: 12/100, Loss: 0.499484\n",
            "Epoch: 13/100, Loss: 0.490278\n",
            "Epoch: 14/100, Loss: 0.481439\n",
            "Epoch: 15/100, Loss: 0.472924\n",
            "Epoch: 16/100, Loss: 0.464685\n",
            "Epoch: 17/100, Loss: 0.456695\n",
            "Epoch: 18/100, Loss: 0.448931\n",
            "Epoch: 19/100, Loss: 0.441362\n",
            "Epoch: 20/100, Loss: 0.433976\n",
            "Epoch: 21/100, Loss: 0.426759\n",
            "Epoch: 22/100, Loss: 0.419703\n",
            "Epoch: 23/100, Loss: 0.412800\n",
            "Epoch: 24/100, Loss: 0.406033\n",
            "Epoch: 25/100, Loss: 0.399397\n",
            "Epoch: 26/100, Loss: 0.392885\n",
            "Epoch: 27/100, Loss: 0.386487\n",
            "Epoch: 28/100, Loss: 0.380204\n",
            "Epoch: 29/100, Loss: 0.374027\n",
            "Epoch: 30/100, Loss: 0.367950\n",
            "Epoch: 31/100, Loss: 0.361968\n",
            "Epoch: 32/100, Loss: 0.356079\n",
            "Epoch: 33/100, Loss: 0.350283\n",
            "Epoch: 34/100, Loss: 0.344570\n",
            "Epoch: 35/100, Loss: 0.338940\n",
            "Epoch: 36/100, Loss: 0.333396\n",
            "Epoch: 37/100, Loss: 0.327927\n",
            "Epoch: 38/100, Loss: 0.322532\n",
            "Epoch: 39/100, Loss: 0.317211\n",
            "Epoch: 40/100, Loss: 0.311963\n",
            "Epoch: 41/100, Loss: 0.306787\n",
            "Epoch: 42/100, Loss: 0.301678\n",
            "Epoch: 43/100, Loss: 0.296632\n",
            "Epoch: 44/100, Loss: 0.291652\n",
            "Epoch: 45/100, Loss: 0.286732\n",
            "Epoch: 46/100, Loss: 0.281878\n",
            "Epoch: 47/100, Loss: 0.277081\n",
            "Epoch: 48/100, Loss: 0.272347\n",
            "Epoch: 49/100, Loss: 0.267667\n",
            "Epoch: 50/100, Loss: 0.263048\n",
            "Epoch: 51/100, Loss: 0.258484\n",
            "Epoch: 52/100, Loss: 0.253980\n",
            "Epoch: 53/100, Loss: 0.249530\n",
            "Epoch: 54/100, Loss: 0.245136\n",
            "Epoch: 55/100, Loss: 0.240798\n",
            "Epoch: 56/100, Loss: 0.236510\n",
            "Epoch: 57/100, Loss: 0.232278\n",
            "Epoch: 58/100, Loss: 0.228098\n",
            "Epoch: 59/100, Loss: 0.223970\n",
            "Epoch: 60/100, Loss: 0.219893\n",
            "Epoch: 61/100, Loss: 0.215867\n",
            "Epoch: 62/100, Loss: 0.211891\n",
            "Epoch: 63/100, Loss: 0.207966\n",
            "Epoch: 64/100, Loss: 0.204090\n",
            "Epoch: 65/100, Loss: 0.200266\n",
            "Epoch: 66/100, Loss: 0.196490\n",
            "Epoch: 67/100, Loss: 0.192764\n",
            "Epoch: 68/100, Loss: 0.189084\n",
            "Epoch: 69/100, Loss: 0.185453\n",
            "Epoch: 70/100, Loss: 0.181871\n",
            "Epoch: 71/100, Loss: 0.178336\n",
            "Epoch: 72/100, Loss: 0.174847\n",
            "Epoch: 73/100, Loss: 0.171406\n",
            "Epoch: 74/100, Loss: 0.168009\n",
            "Epoch: 75/100, Loss: 0.164658\n",
            "Epoch: 76/100, Loss: 0.161354\n",
            "Epoch: 77/100, Loss: 0.158093\n",
            "Epoch: 78/100, Loss: 0.154878\n",
            "Epoch: 79/100, Loss: 0.151710\n",
            "Epoch: 80/100, Loss: 0.148585\n",
            "Epoch: 81/100, Loss: 0.145502\n",
            "Epoch: 82/100, Loss: 0.142465\n",
            "Epoch: 83/100, Loss: 0.139477\n",
            "Epoch: 84/100, Loss: 0.136538\n",
            "Epoch: 85/100, Loss: 0.133650\n",
            "Epoch: 86/100, Loss: 0.130810\n",
            "Epoch: 87/100, Loss: 0.128015\n",
            "Epoch: 88/100, Loss: 0.125266\n",
            "Epoch: 89/100, Loss: 0.122568\n",
            "Epoch: 90/100, Loss: 0.119922\n",
            "Epoch: 91/100, Loss: 0.117327\n",
            "Epoch: 92/100, Loss: 0.114785\n",
            "Epoch: 93/100, Loss: 0.112270\n",
            "Epoch: 94/100, Loss: 0.109726\n",
            "Epoch: 95/100, Loss: 0.107163\n",
            "Epoch: 96/100, Loss: 0.104651\n",
            "Epoch: 97/100, Loss: 0.102216\n",
            "Epoch: 98/100, Loss: 0.099834\n",
            "Epoch: 99/100, Loss: 0.097494\n",
            "Epoch: 100/100, Loss: 0.095205\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.679\n",
            "Normalised mutual info score on k-means on latent space: 0.5942950973984461\n",
            "ARI score on k-means on latent space: 0.4997177877792908\n",
            "K-means cluster error on latent space: 42422.33984375\n",
            "K-means silhouette score on latent space: 0.18526982 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7297\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.698320656418902 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5948201046482475 \n",
            "\n",
            "ROUND NUMBER  9 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.922487\n",
            "Epoch: 2/100, Loss: 0.716386\n",
            "Epoch: 3/100, Loss: 0.643741\n",
            "Epoch: 4/100, Loss: 0.610480\n",
            "Epoch: 5/100, Loss: 0.588385\n",
            "Epoch: 6/100, Loss: 0.570988\n",
            "Epoch: 7/100, Loss: 0.556203\n",
            "Epoch: 8/100, Loss: 0.543114\n",
            "Epoch: 9/100, Loss: 0.531228\n",
            "Epoch: 10/100, Loss: 0.520235\n",
            "Epoch: 11/100, Loss: 0.509950\n",
            "Epoch: 12/100, Loss: 0.500235\n",
            "Epoch: 13/100, Loss: 0.490984\n",
            "Epoch: 14/100, Loss: 0.482115\n",
            "Epoch: 15/100, Loss: 0.473563\n",
            "Epoch: 16/100, Loss: 0.465292\n",
            "Epoch: 17/100, Loss: 0.457276\n",
            "Epoch: 18/100, Loss: 0.449485\n",
            "Epoch: 19/100, Loss: 0.441895\n",
            "Epoch: 20/100, Loss: 0.434487\n",
            "Epoch: 21/100, Loss: 0.427247\n",
            "Epoch: 22/100, Loss: 0.420160\n",
            "Epoch: 23/100, Loss: 0.413219\n",
            "Epoch: 24/100, Loss: 0.406419\n",
            "Epoch: 25/100, Loss: 0.399744\n",
            "Epoch: 26/100, Loss: 0.393196\n",
            "Epoch: 27/100, Loss: 0.386766\n",
            "Epoch: 28/100, Loss: 0.380446\n",
            "Epoch: 29/100, Loss: 0.374235\n",
            "Epoch: 30/100, Loss: 0.368125\n",
            "Epoch: 31/100, Loss: 0.362114\n",
            "Epoch: 32/100, Loss: 0.356198\n",
            "Epoch: 33/100, Loss: 0.350374\n",
            "Epoch: 34/100, Loss: 0.344634\n",
            "Epoch: 35/100, Loss: 0.338979\n",
            "Epoch: 36/100, Loss: 0.333405\n",
            "Epoch: 37/100, Loss: 0.327911\n",
            "Epoch: 38/100, Loss: 0.322497\n",
            "Epoch: 39/100, Loss: 0.317155\n",
            "Epoch: 40/100, Loss: 0.311888\n",
            "Epoch: 41/100, Loss: 0.306689\n",
            "Epoch: 42/100, Loss: 0.301562\n",
            "Epoch: 43/100, Loss: 0.296501\n",
            "Epoch: 44/100, Loss: 0.291504\n",
            "Epoch: 45/100, Loss: 0.286570\n",
            "Epoch: 46/100, Loss: 0.281701\n",
            "Epoch: 47/100, Loss: 0.276896\n",
            "Epoch: 48/100, Loss: 0.272148\n",
            "Epoch: 49/100, Loss: 0.267465\n",
            "Epoch: 50/100, Loss: 0.262840\n",
            "Epoch: 51/100, Loss: 0.258269\n",
            "Epoch: 52/100, Loss: 0.253752\n",
            "Epoch: 53/100, Loss: 0.249295\n",
            "Epoch: 54/100, Loss: 0.244889\n",
            "Epoch: 55/100, Loss: 0.240544\n",
            "Epoch: 56/100, Loss: 0.236248\n",
            "Epoch: 57/100, Loss: 0.232009\n",
            "Epoch: 58/100, Loss: 0.227817\n",
            "Epoch: 59/100, Loss: 0.223683\n",
            "Epoch: 60/100, Loss: 0.219597\n",
            "Epoch: 61/100, Loss: 0.215563\n",
            "Epoch: 62/100, Loss: 0.211581\n",
            "Epoch: 63/100, Loss: 0.207647\n",
            "Epoch: 64/100, Loss: 0.203764\n",
            "Epoch: 65/100, Loss: 0.199932\n",
            "Epoch: 66/100, Loss: 0.196152\n",
            "Epoch: 67/100, Loss: 0.192419\n",
            "Epoch: 68/100, Loss: 0.188739\n",
            "Epoch: 69/100, Loss: 0.185107\n",
            "Epoch: 70/100, Loss: 0.181530\n",
            "Epoch: 71/100, Loss: 0.177995\n",
            "Epoch: 72/100, Loss: 0.174509\n",
            "Epoch: 73/100, Loss: 0.171078\n",
            "Epoch: 74/100, Loss: 0.167696\n",
            "Epoch: 75/100, Loss: 0.164360\n",
            "Epoch: 76/100, Loss: 0.161073\n",
            "Epoch: 77/100, Loss: 0.157838\n",
            "Epoch: 78/100, Loss: 0.154667\n",
            "Epoch: 79/100, Loss: 0.151554\n",
            "Epoch: 80/100, Loss: 0.148491\n",
            "Epoch: 81/100, Loss: 0.145439\n",
            "Epoch: 82/100, Loss: 0.142373\n",
            "Epoch: 83/100, Loss: 0.139333\n",
            "Epoch: 84/100, Loss: 0.136339\n",
            "Epoch: 85/100, Loss: 0.133408\n",
            "Epoch: 86/100, Loss: 0.130534\n",
            "Epoch: 87/100, Loss: 0.127717\n",
            "Epoch: 88/100, Loss: 0.124946\n",
            "Epoch: 89/100, Loss: 0.122227\n",
            "Epoch: 90/100, Loss: 0.119567\n",
            "Epoch: 91/100, Loss: 0.116956\n",
            "Epoch: 92/100, Loss: 0.114385\n",
            "Epoch: 93/100, Loss: 0.111860\n",
            "Epoch: 94/100, Loss: 0.109393\n",
            "Epoch: 95/100, Loss: 0.106981\n",
            "Epoch: 96/100, Loss: 0.104608\n",
            "Epoch: 97/100, Loss: 0.102270\n",
            "Epoch: 98/100, Loss: 0.099932\n",
            "Epoch: 99/100, Loss: 0.097550\n",
            "Epoch: 100/100, Loss: 0.095200\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6385\n",
            "Normalised mutual info score on k-means on latent space: 0.5807548784367327\n",
            "ARI score on k-means on latent space: 0.4675707258244615\n",
            "K-means cluster error on latent space: 43424.09375\n",
            "K-means silhouette score on latent space: 0.18789458 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6844\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6711134823502636 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5494357986648849 \n",
            "\n",
            "ROUND NUMBER  10 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.915491\n",
            "Epoch: 2/100, Loss: 0.713281\n",
            "Epoch: 3/100, Loss: 0.639918\n",
            "Epoch: 4/100, Loss: 0.606649\n",
            "Epoch: 5/100, Loss: 0.585202\n",
            "Epoch: 6/100, Loss: 0.568403\n",
            "Epoch: 7/100, Loss: 0.554107\n",
            "Epoch: 8/100, Loss: 0.541434\n",
            "Epoch: 9/100, Loss: 0.529870\n",
            "Epoch: 10/100, Loss: 0.519115\n",
            "Epoch: 11/100, Loss: 0.508990\n",
            "Epoch: 12/100, Loss: 0.499380\n",
            "Epoch: 13/100, Loss: 0.490196\n",
            "Epoch: 14/100, Loss: 0.481368\n",
            "Epoch: 15/100, Loss: 0.472865\n",
            "Epoch: 16/100, Loss: 0.464631\n",
            "Epoch: 17/100, Loss: 0.456637\n",
            "Epoch: 18/100, Loss: 0.448854\n",
            "Epoch: 19/100, Loss: 0.441272\n",
            "Epoch: 20/100, Loss: 0.433877\n",
            "Epoch: 21/100, Loss: 0.426646\n",
            "Epoch: 22/100, Loss: 0.419573\n",
            "Epoch: 23/100, Loss: 0.412652\n",
            "Epoch: 24/100, Loss: 0.405865\n",
            "Epoch: 25/100, Loss: 0.399209\n",
            "Epoch: 26/100, Loss: 0.392670\n",
            "Epoch: 27/100, Loss: 0.386257\n",
            "Epoch: 28/100, Loss: 0.379953\n",
            "Epoch: 29/100, Loss: 0.373764\n",
            "Epoch: 30/100, Loss: 0.367667\n",
            "Epoch: 31/100, Loss: 0.361668\n",
            "Epoch: 32/100, Loss: 0.355762\n",
            "Epoch: 33/100, Loss: 0.349943\n",
            "Epoch: 34/100, Loss: 0.344209\n",
            "Epoch: 35/100, Loss: 0.338559\n",
            "Epoch: 36/100, Loss: 0.332996\n",
            "Epoch: 37/100, Loss: 0.327511\n",
            "Epoch: 38/100, Loss: 0.322101\n",
            "Epoch: 39/100, Loss: 0.316765\n",
            "Epoch: 40/100, Loss: 0.311501\n",
            "Epoch: 41/100, Loss: 0.306310\n",
            "Epoch: 42/100, Loss: 0.301183\n",
            "Epoch: 43/100, Loss: 0.296124\n",
            "Epoch: 44/100, Loss: 0.291130\n",
            "Epoch: 45/100, Loss: 0.286201\n",
            "Epoch: 46/100, Loss: 0.281336\n",
            "Epoch: 47/100, Loss: 0.276529\n",
            "Epoch: 48/100, Loss: 0.271785\n",
            "Epoch: 49/100, Loss: 0.267096\n",
            "Epoch: 50/100, Loss: 0.262469\n",
            "Epoch: 51/100, Loss: 0.257904\n",
            "Epoch: 52/100, Loss: 0.253395\n",
            "Epoch: 53/100, Loss: 0.248942\n",
            "Epoch: 54/100, Loss: 0.244544\n",
            "Epoch: 55/100, Loss: 0.240201\n",
            "Epoch: 56/100, Loss: 0.235913\n",
            "Epoch: 57/100, Loss: 0.231680\n",
            "Epoch: 58/100, Loss: 0.227496\n",
            "Epoch: 59/100, Loss: 0.223365\n",
            "Epoch: 60/100, Loss: 0.219285\n",
            "Epoch: 61/100, Loss: 0.215260\n",
            "Epoch: 62/100, Loss: 0.211287\n",
            "Epoch: 63/100, Loss: 0.207365\n",
            "Epoch: 64/100, Loss: 0.203493\n",
            "Epoch: 65/100, Loss: 0.199670\n",
            "Epoch: 66/100, Loss: 0.195895\n",
            "Epoch: 67/100, Loss: 0.192171\n",
            "Epoch: 68/100, Loss: 0.188493\n",
            "Epoch: 69/100, Loss: 0.184864\n",
            "Epoch: 70/100, Loss: 0.181284\n",
            "Epoch: 71/100, Loss: 0.177751\n",
            "Epoch: 72/100, Loss: 0.174265\n",
            "Epoch: 73/100, Loss: 0.170828\n",
            "Epoch: 74/100, Loss: 0.167436\n",
            "Epoch: 75/100, Loss: 0.164090\n",
            "Epoch: 76/100, Loss: 0.160793\n",
            "Epoch: 77/100, Loss: 0.157543\n",
            "Epoch: 78/100, Loss: 0.154337\n",
            "Epoch: 79/100, Loss: 0.151176\n",
            "Epoch: 80/100, Loss: 0.148058\n",
            "Epoch: 81/100, Loss: 0.144989\n",
            "Epoch: 82/100, Loss: 0.141966\n",
            "Epoch: 83/100, Loss: 0.138990\n",
            "Epoch: 84/100, Loss: 0.136061\n",
            "Epoch: 85/100, Loss: 0.133181\n",
            "Epoch: 86/100, Loss: 0.130355\n",
            "Epoch: 87/100, Loss: 0.127576\n",
            "Epoch: 88/100, Loss: 0.124847\n",
            "Epoch: 89/100, Loss: 0.122146\n",
            "Epoch: 90/100, Loss: 0.119493\n",
            "Epoch: 91/100, Loss: 0.116876\n",
            "Epoch: 92/100, Loss: 0.114296\n",
            "Epoch: 93/100, Loss: 0.111768\n",
            "Epoch: 94/100, Loss: 0.109248\n",
            "Epoch: 95/100, Loss: 0.106718\n",
            "Epoch: 96/100, Loss: 0.104221\n",
            "Epoch: 97/100, Loss: 0.101759\n",
            "Epoch: 98/100, Loss: 0.099348\n",
            "Epoch: 99/100, Loss: 0.097002\n",
            "Epoch: 100/100, Loss: 0.094714\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6525\n",
            "Normalised mutual info score on k-means on latent space: 0.6053221203243158\n",
            "ARI score on k-means on latent space: 0.49830363127723326\n",
            "K-means cluster error on latent space: 43240.43359375\n",
            "K-means silhouette score on latent space: 0.17376591 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7068\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6972812121384958 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.59024601425316 \n",
            "\n",
            "Average k-means accuracy score at latent space: 0.66809 \n",
            "\n",
            "Average k-means NMI score at latent space: 0.5958417418379275 \n",
            "\n",
            "Average k-means ARI score at latent space: 0.49806033454123694 \n",
            "\n",
            "Average agglomerative clustering accuracy score at latent space: 0.6954800000000001 \n",
            "\n",
            "Average agglomerative clustering NMI score at latent space: 0.6804644165999602 \n",
            "\n",
            "Average agglomerative clustering ARI score at latent space: 0.5553927295185597 \n",
            "\n",
            "Average k-means silhouette score on latent space: 0.18152720779180526 \n",
            "\n",
            "Average k-means cluster error on latent space: 42806.228125 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_for_k_50 = run_experiment(50, dataloader, 784, datapoints = images, datapoints_reshaped = images_reshaped, labels = labels, epochs=100, hl1_neurons=32, hl2_neurons=64, hl3_neurons=128, latent_dimension=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icQOKDGT7yef",
        "outputId": "bb5cd5cf-04f3-4f8f-c699-899a7e2da801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment results for k-means with k = 50 clusters:\n",
            "\n",
            "Running k-means algorithm in order to get our pseudolabels: \n",
            "\n",
            "K_means greedy accuracy score (initial space): 0.8101\n",
            "Normalised mutual info score (initial space): 0.6983006792050575\n",
            "ARI (initial space): 0.6500735528737586 \n",
            "\n",
            "Using the autoencoder model on our data: \n",
            "\n",
            "ROUND NUMBER  1 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.901812\n",
            "Epoch: 2/100, Loss: 0.716151\n",
            "Epoch: 3/100, Loss: 0.644579\n",
            "Epoch: 4/100, Loss: 0.608878\n",
            "Epoch: 5/100, Loss: 0.585409\n",
            "Epoch: 6/100, Loss: 0.567781\n",
            "Epoch: 7/100, Loss: 0.553325\n",
            "Epoch: 8/100, Loss: 0.540695\n",
            "Epoch: 9/100, Loss: 0.529218\n",
            "Epoch: 10/100, Loss: 0.518557\n",
            "Epoch: 11/100, Loss: 0.508500\n",
            "Epoch: 12/100, Loss: 0.498934\n",
            "Epoch: 13/100, Loss: 0.489768\n",
            "Epoch: 14/100, Loss: 0.480947\n",
            "Epoch: 15/100, Loss: 0.472431\n",
            "Epoch: 16/100, Loss: 0.464179\n",
            "Epoch: 17/100, Loss: 0.456173\n",
            "Epoch: 18/100, Loss: 0.448385\n",
            "Epoch: 19/100, Loss: 0.440797\n",
            "Epoch: 20/100, Loss: 0.433393\n",
            "Epoch: 21/100, Loss: 0.426155\n",
            "Epoch: 22/100, Loss: 0.419079\n",
            "Epoch: 23/100, Loss: 0.412152\n",
            "Epoch: 24/100, Loss: 0.405363\n",
            "Epoch: 25/100, Loss: 0.398704\n",
            "Epoch: 26/100, Loss: 0.392166\n",
            "Epoch: 27/100, Loss: 0.385749\n",
            "Epoch: 28/100, Loss: 0.379443\n",
            "Epoch: 29/100, Loss: 0.373241\n",
            "Epoch: 30/100, Loss: 0.367142\n",
            "Epoch: 31/100, Loss: 0.361140\n",
            "Epoch: 32/100, Loss: 0.355236\n",
            "Epoch: 33/100, Loss: 0.349420\n",
            "Epoch: 34/100, Loss: 0.343695\n",
            "Epoch: 35/100, Loss: 0.338053\n",
            "Epoch: 36/100, Loss: 0.332491\n",
            "Epoch: 37/100, Loss: 0.327009\n",
            "Epoch: 38/100, Loss: 0.321606\n",
            "Epoch: 39/100, Loss: 0.316277\n",
            "Epoch: 40/100, Loss: 0.311018\n",
            "Epoch: 41/100, Loss: 0.305826\n",
            "Epoch: 42/100, Loss: 0.300705\n",
            "Epoch: 43/100, Loss: 0.295652\n",
            "Epoch: 44/100, Loss: 0.290665\n",
            "Epoch: 45/100, Loss: 0.285738\n",
            "Epoch: 46/100, Loss: 0.280877\n",
            "Epoch: 47/100, Loss: 0.276080\n",
            "Epoch: 48/100, Loss: 0.271345\n",
            "Epoch: 49/100, Loss: 0.266671\n",
            "Epoch: 50/100, Loss: 0.262051\n",
            "Epoch: 51/100, Loss: 0.257489\n",
            "Epoch: 52/100, Loss: 0.252985\n",
            "Epoch: 53/100, Loss: 0.248537\n",
            "Epoch: 54/100, Loss: 0.244143\n",
            "Epoch: 55/100, Loss: 0.239804\n",
            "Epoch: 56/100, Loss: 0.235518\n",
            "Epoch: 57/100, Loss: 0.231286\n",
            "Epoch: 58/100, Loss: 0.227107\n",
            "Epoch: 59/100, Loss: 0.222977\n",
            "Epoch: 60/100, Loss: 0.218901\n",
            "Epoch: 61/100, Loss: 0.214878\n",
            "Epoch: 62/100, Loss: 0.210906\n",
            "Epoch: 63/100, Loss: 0.206984\n",
            "Epoch: 64/100, Loss: 0.203112\n",
            "Epoch: 65/100, Loss: 0.199293\n",
            "Epoch: 66/100, Loss: 0.195523\n",
            "Epoch: 67/100, Loss: 0.191806\n",
            "Epoch: 68/100, Loss: 0.188138\n",
            "Epoch: 69/100, Loss: 0.184524\n",
            "Epoch: 70/100, Loss: 0.180954\n",
            "Epoch: 71/100, Loss: 0.177434\n",
            "Epoch: 72/100, Loss: 0.173971\n",
            "Epoch: 73/100, Loss: 0.170573\n",
            "Epoch: 74/100, Loss: 0.167256\n",
            "Epoch: 75/100, Loss: 0.164022\n",
            "Epoch: 76/100, Loss: 0.160832\n",
            "Epoch: 77/100, Loss: 0.157594\n",
            "Epoch: 78/100, Loss: 0.154318\n",
            "Epoch: 79/100, Loss: 0.151090\n",
            "Epoch: 80/100, Loss: 0.147916\n",
            "Epoch: 81/100, Loss: 0.144794\n",
            "Epoch: 82/100, Loss: 0.141745\n",
            "Epoch: 83/100, Loss: 0.138762\n",
            "Epoch: 84/100, Loss: 0.135832\n",
            "Epoch: 85/100, Loss: 0.132942\n",
            "Epoch: 86/100, Loss: 0.130097\n",
            "Epoch: 87/100, Loss: 0.127301\n",
            "Epoch: 88/100, Loss: 0.124549\n",
            "Epoch: 89/100, Loss: 0.121836\n",
            "Epoch: 90/100, Loss: 0.119165\n",
            "Epoch: 91/100, Loss: 0.116543\n",
            "Epoch: 92/100, Loss: 0.113965\n",
            "Epoch: 93/100, Loss: 0.111426\n",
            "Epoch: 94/100, Loss: 0.108925\n",
            "Epoch: 95/100, Loss: 0.106473\n",
            "Epoch: 96/100, Loss: 0.104063\n",
            "Epoch: 97/100, Loss: 0.101703\n",
            "Epoch: 98/100, Loss: 0.099379\n",
            "Epoch: 99/100, Loss: 0.097092\n",
            "Epoch: 100/100, Loss: 0.094837\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6929\n",
            "Normalised mutual info score on k-means on latent space: 0.6108743841045924\n",
            "ARI score on k-means on latent space: 0.5281432689975445\n",
            "K-means cluster error on latent space: 43708.0625\n",
            "K-means silhouette score on latent space: 0.18696441 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7093\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6789588416514838 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5721454338844584 \n",
            "\n",
            "ROUND NUMBER  2 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.916482\n",
            "Epoch: 2/100, Loss: 0.722562\n",
            "Epoch: 3/100, Loss: 0.646359\n",
            "Epoch: 4/100, Loss: 0.609887\n",
            "Epoch: 5/100, Loss: 0.586677\n",
            "Epoch: 6/100, Loss: 0.569252\n",
            "Epoch: 7/100, Loss: 0.554765\n",
            "Epoch: 8/100, Loss: 0.542051\n",
            "Epoch: 9/100, Loss: 0.530516\n",
            "Epoch: 10/100, Loss: 0.519811\n",
            "Epoch: 11/100, Loss: 0.509762\n",
            "Epoch: 12/100, Loss: 0.500231\n",
            "Epoch: 13/100, Loss: 0.491118\n",
            "Epoch: 14/100, Loss: 0.482361\n",
            "Epoch: 15/100, Loss: 0.473900\n",
            "Epoch: 16/100, Loss: 0.465699\n",
            "Epoch: 17/100, Loss: 0.457726\n",
            "Epoch: 18/100, Loss: 0.449966\n",
            "Epoch: 19/100, Loss: 0.442399\n",
            "Epoch: 20/100, Loss: 0.435006\n",
            "Epoch: 21/100, Loss: 0.427778\n",
            "Epoch: 22/100, Loss: 0.420705\n",
            "Epoch: 23/100, Loss: 0.413779\n",
            "Epoch: 24/100, Loss: 0.406988\n",
            "Epoch: 25/100, Loss: 0.400326\n",
            "Epoch: 26/100, Loss: 0.393788\n",
            "Epoch: 27/100, Loss: 0.387362\n",
            "Epoch: 28/100, Loss: 0.381046\n",
            "Epoch: 29/100, Loss: 0.374838\n",
            "Epoch: 30/100, Loss: 0.368732\n",
            "Epoch: 31/100, Loss: 0.362726\n",
            "Epoch: 32/100, Loss: 0.356814\n",
            "Epoch: 33/100, Loss: 0.350993\n",
            "Epoch: 34/100, Loss: 0.345255\n",
            "Epoch: 35/100, Loss: 0.339604\n",
            "Epoch: 36/100, Loss: 0.334029\n",
            "Epoch: 37/100, Loss: 0.328535\n",
            "Epoch: 38/100, Loss: 0.323119\n",
            "Epoch: 39/100, Loss: 0.317776\n",
            "Epoch: 40/100, Loss: 0.312508\n",
            "Epoch: 41/100, Loss: 0.307311\n",
            "Epoch: 42/100, Loss: 0.302184\n",
            "Epoch: 43/100, Loss: 0.297123\n",
            "Epoch: 44/100, Loss: 0.292125\n",
            "Epoch: 45/100, Loss: 0.287192\n",
            "Epoch: 46/100, Loss: 0.282320\n",
            "Epoch: 47/100, Loss: 0.277512\n",
            "Epoch: 48/100, Loss: 0.272765\n",
            "Epoch: 49/100, Loss: 0.268081\n",
            "Epoch: 50/100, Loss: 0.263452\n",
            "Epoch: 51/100, Loss: 0.258881\n",
            "Epoch: 52/100, Loss: 0.254367\n",
            "Epoch: 53/100, Loss: 0.249908\n",
            "Epoch: 54/100, Loss: 0.245505\n",
            "Epoch: 55/100, Loss: 0.241160\n",
            "Epoch: 56/100, Loss: 0.236867\n",
            "Epoch: 57/100, Loss: 0.232635\n",
            "Epoch: 58/100, Loss: 0.228453\n",
            "Epoch: 59/100, Loss: 0.224324\n",
            "Epoch: 60/100, Loss: 0.220245\n",
            "Epoch: 61/100, Loss: 0.216216\n",
            "Epoch: 62/100, Loss: 0.212236\n",
            "Epoch: 63/100, Loss: 0.208308\n",
            "Epoch: 64/100, Loss: 0.204433\n",
            "Epoch: 65/100, Loss: 0.200605\n",
            "Epoch: 66/100, Loss: 0.196827\n",
            "Epoch: 67/100, Loss: 0.193096\n",
            "Epoch: 68/100, Loss: 0.189416\n",
            "Epoch: 69/100, Loss: 0.185782\n",
            "Epoch: 70/100, Loss: 0.182195\n",
            "Epoch: 71/100, Loss: 0.178657\n",
            "Epoch: 72/100, Loss: 0.175168\n",
            "Epoch: 73/100, Loss: 0.171729\n",
            "Epoch: 74/100, Loss: 0.168332\n",
            "Epoch: 75/100, Loss: 0.164983\n",
            "Epoch: 76/100, Loss: 0.161680\n",
            "Epoch: 77/100, Loss: 0.158425\n",
            "Epoch: 78/100, Loss: 0.155213\n",
            "Epoch: 79/100, Loss: 0.152050\n",
            "Epoch: 80/100, Loss: 0.148931\n",
            "Epoch: 81/100, Loss: 0.145864\n",
            "Epoch: 82/100, Loss: 0.142846\n",
            "Epoch: 83/100, Loss: 0.139876\n",
            "Epoch: 84/100, Loss: 0.136946\n",
            "Epoch: 85/100, Loss: 0.134060\n",
            "Epoch: 86/100, Loss: 0.131226\n",
            "Epoch: 87/100, Loss: 0.128442\n",
            "Epoch: 88/100, Loss: 0.125693\n",
            "Epoch: 89/100, Loss: 0.122958\n",
            "Epoch: 90/100, Loss: 0.120252\n",
            "Epoch: 91/100, Loss: 0.117582\n",
            "Epoch: 92/100, Loss: 0.114974\n",
            "Epoch: 93/100, Loss: 0.112437\n",
            "Epoch: 94/100, Loss: 0.109959\n",
            "Epoch: 95/100, Loss: 0.107541\n",
            "Epoch: 96/100, Loss: 0.105172\n",
            "Epoch: 97/100, Loss: 0.102836\n",
            "Epoch: 98/100, Loss: 0.100503\n",
            "Epoch: 99/100, Loss: 0.098183\n",
            "Epoch: 100/100, Loss: 0.095892\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6596\n",
            "Normalised mutual info score on k-means on latent space: 0.5940312579678532\n",
            "ARI score on k-means on latent space: 0.4856427836904703\n",
            "K-means cluster error on latent space: 40560.62109375\n",
            "K-means silhouette score on latent space: 0.18053757 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7102\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6810766682942276 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5471093702072052 \n",
            "\n",
            "ROUND NUMBER  3 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.904985\n",
            "Epoch: 2/100, Loss: 0.719509\n",
            "Epoch: 3/100, Loss: 0.645763\n",
            "Epoch: 4/100, Loss: 0.607385\n",
            "Epoch: 5/100, Loss: 0.583617\n",
            "Epoch: 6/100, Loss: 0.566188\n",
            "Epoch: 7/100, Loss: 0.551755\n",
            "Epoch: 8/100, Loss: 0.539145\n",
            "Epoch: 9/100, Loss: 0.527752\n",
            "Epoch: 10/100, Loss: 0.517212\n",
            "Epoch: 11/100, Loss: 0.507305\n",
            "Epoch: 12/100, Loss: 0.497902\n",
            "Epoch: 13/100, Loss: 0.488899\n",
            "Epoch: 14/100, Loss: 0.480229\n",
            "Epoch: 15/100, Loss: 0.471856\n",
            "Epoch: 16/100, Loss: 0.463735\n",
            "Epoch: 17/100, Loss: 0.455839\n",
            "Epoch: 18/100, Loss: 0.448149\n",
            "Epoch: 19/100, Loss: 0.440650\n",
            "Epoch: 20/100, Loss: 0.433325\n",
            "Epoch: 21/100, Loss: 0.426164\n",
            "Epoch: 22/100, Loss: 0.419149\n",
            "Epoch: 23/100, Loss: 0.412279\n",
            "Epoch: 24/100, Loss: 0.405540\n",
            "Epoch: 25/100, Loss: 0.398926\n",
            "Epoch: 26/100, Loss: 0.392431\n",
            "Epoch: 27/100, Loss: 0.386047\n",
            "Epoch: 28/100, Loss: 0.379770\n",
            "Epoch: 29/100, Loss: 0.373600\n",
            "Epoch: 30/100, Loss: 0.367525\n",
            "Epoch: 31/100, Loss: 0.361549\n",
            "Epoch: 32/100, Loss: 0.355663\n",
            "Epoch: 33/100, Loss: 0.349868\n",
            "Epoch: 34/100, Loss: 0.344157\n",
            "Epoch: 35/100, Loss: 0.338525\n",
            "Epoch: 36/100, Loss: 0.332976\n",
            "Epoch: 37/100, Loss: 0.327505\n",
            "Epoch: 38/100, Loss: 0.322108\n",
            "Epoch: 39/100, Loss: 0.316782\n",
            "Epoch: 40/100, Loss: 0.311531\n",
            "Epoch: 41/100, Loss: 0.306348\n",
            "Epoch: 42/100, Loss: 0.301231\n",
            "Epoch: 43/100, Loss: 0.296184\n",
            "Epoch: 44/100, Loss: 0.291205\n",
            "Epoch: 45/100, Loss: 0.286286\n",
            "Epoch: 46/100, Loss: 0.281426\n",
            "Epoch: 47/100, Loss: 0.276632\n",
            "Epoch: 48/100, Loss: 0.271896\n",
            "Epoch: 49/100, Loss: 0.267218\n",
            "Epoch: 50/100, Loss: 0.262600\n",
            "Epoch: 51/100, Loss: 0.258042\n",
            "Epoch: 52/100, Loss: 0.253540\n",
            "Epoch: 53/100, Loss: 0.249089\n",
            "Epoch: 54/100, Loss: 0.244697\n",
            "Epoch: 55/100, Loss: 0.240356\n",
            "Epoch: 56/100, Loss: 0.236073\n",
            "Epoch: 57/100, Loss: 0.231842\n",
            "Epoch: 58/100, Loss: 0.227664\n",
            "Epoch: 59/100, Loss: 0.223541\n",
            "Epoch: 60/100, Loss: 0.219468\n",
            "Epoch: 61/100, Loss: 0.215446\n",
            "Epoch: 62/100, Loss: 0.211474\n",
            "Epoch: 63/100, Loss: 0.207552\n",
            "Epoch: 64/100, Loss: 0.203682\n",
            "Epoch: 65/100, Loss: 0.199860\n",
            "Epoch: 66/100, Loss: 0.196088\n",
            "Epoch: 67/100, Loss: 0.192366\n",
            "Epoch: 68/100, Loss: 0.188693\n",
            "Epoch: 69/100, Loss: 0.185065\n",
            "Epoch: 70/100, Loss: 0.181488\n",
            "Epoch: 71/100, Loss: 0.177954\n",
            "Epoch: 72/100, Loss: 0.174469\n",
            "Epoch: 73/100, Loss: 0.171031\n",
            "Epoch: 74/100, Loss: 0.167639\n",
            "Epoch: 75/100, Loss: 0.164293\n",
            "Epoch: 76/100, Loss: 0.160996\n",
            "Epoch: 77/100, Loss: 0.157744\n",
            "Epoch: 78/100, Loss: 0.154545\n",
            "Epoch: 79/100, Loss: 0.151397\n",
            "Epoch: 80/100, Loss: 0.148304\n",
            "Epoch: 81/100, Loss: 0.145267\n",
            "Epoch: 82/100, Loss: 0.142285\n",
            "Epoch: 83/100, Loss: 0.139341\n",
            "Epoch: 84/100, Loss: 0.136436\n",
            "Epoch: 85/100, Loss: 0.133533\n",
            "Epoch: 86/100, Loss: 0.130627\n",
            "Epoch: 87/100, Loss: 0.127741\n",
            "Epoch: 88/100, Loss: 0.124921\n",
            "Epoch: 89/100, Loss: 0.122169\n",
            "Epoch: 90/100, Loss: 0.119467\n",
            "Epoch: 91/100, Loss: 0.116819\n",
            "Epoch: 92/100, Loss: 0.114210\n",
            "Epoch: 93/100, Loss: 0.111647\n",
            "Epoch: 94/100, Loss: 0.109132\n",
            "Epoch: 95/100, Loss: 0.106662\n",
            "Epoch: 96/100, Loss: 0.104239\n",
            "Epoch: 97/100, Loss: 0.101849\n",
            "Epoch: 98/100, Loss: 0.099500\n",
            "Epoch: 99/100, Loss: 0.097187\n",
            "Epoch: 100/100, Loss: 0.094916\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.7\n",
            "Normalised mutual info score on k-means on latent space: 0.6009598076497853\n",
            "ARI score on k-means on latent space: 0.5163416829702385\n",
            "K-means cluster error on latent space: 45315.671875\n",
            "K-means silhouette score on latent space: 0.1828285 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7369\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.7116856977787767 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5917493807792001 \n",
            "\n",
            "ROUND NUMBER  4 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.942447\n",
            "Epoch: 2/100, Loss: 0.745081\n",
            "Epoch: 3/100, Loss: 0.665911\n",
            "Epoch: 4/100, Loss: 0.624982\n",
            "Epoch: 5/100, Loss: 0.597478\n",
            "Epoch: 6/100, Loss: 0.576751\n",
            "Epoch: 7/100, Loss: 0.560215\n",
            "Epoch: 8/100, Loss: 0.546289\n",
            "Epoch: 9/100, Loss: 0.533992\n",
            "Epoch: 10/100, Loss: 0.522773\n",
            "Epoch: 11/100, Loss: 0.512343\n",
            "Epoch: 12/100, Loss: 0.502516\n",
            "Epoch: 13/100, Loss: 0.493168\n",
            "Epoch: 14/100, Loss: 0.484211\n",
            "Epoch: 15/100, Loss: 0.475585\n",
            "Epoch: 16/100, Loss: 0.467241\n",
            "Epoch: 17/100, Loss: 0.459138\n",
            "Epoch: 18/100, Loss: 0.451262\n",
            "Epoch: 19/100, Loss: 0.443588\n",
            "Epoch: 20/100, Loss: 0.436104\n",
            "Epoch: 21/100, Loss: 0.428793\n",
            "Epoch: 22/100, Loss: 0.421647\n",
            "Epoch: 23/100, Loss: 0.414654\n",
            "Epoch: 24/100, Loss: 0.407803\n",
            "Epoch: 25/100, Loss: 0.401089\n",
            "Epoch: 26/100, Loss: 0.394499\n",
            "Epoch: 27/100, Loss: 0.388030\n",
            "Epoch: 28/100, Loss: 0.381675\n",
            "Epoch: 29/100, Loss: 0.375430\n",
            "Epoch: 30/100, Loss: 0.369297\n",
            "Epoch: 31/100, Loss: 0.363266\n",
            "Epoch: 32/100, Loss: 0.357332\n",
            "Epoch: 33/100, Loss: 0.351489\n",
            "Epoch: 34/100, Loss: 0.345737\n",
            "Epoch: 35/100, Loss: 0.340067\n",
            "Epoch: 36/100, Loss: 0.334485\n",
            "Epoch: 37/100, Loss: 0.328982\n",
            "Epoch: 38/100, Loss: 0.323558\n",
            "Epoch: 39/100, Loss: 0.318209\n",
            "Epoch: 40/100, Loss: 0.312932\n",
            "Epoch: 41/100, Loss: 0.307727\n",
            "Epoch: 42/100, Loss: 0.302593\n",
            "Epoch: 43/100, Loss: 0.297525\n",
            "Epoch: 44/100, Loss: 0.292526\n",
            "Epoch: 45/100, Loss: 0.287586\n",
            "Epoch: 46/100, Loss: 0.282713\n",
            "Epoch: 47/100, Loss: 0.277900\n",
            "Epoch: 48/100, Loss: 0.273149\n",
            "Epoch: 49/100, Loss: 0.268455\n",
            "Epoch: 50/100, Loss: 0.263819\n",
            "Epoch: 51/100, Loss: 0.259245\n",
            "Epoch: 52/100, Loss: 0.254728\n",
            "Epoch: 53/100, Loss: 0.250265\n",
            "Epoch: 54/100, Loss: 0.245858\n",
            "Epoch: 55/100, Loss: 0.241506\n",
            "Epoch: 56/100, Loss: 0.237209\n",
            "Epoch: 57/100, Loss: 0.232965\n",
            "Epoch: 58/100, Loss: 0.228770\n",
            "Epoch: 59/100, Loss: 0.224632\n",
            "Epoch: 60/100, Loss: 0.220544\n",
            "Epoch: 61/100, Loss: 0.216507\n",
            "Epoch: 62/100, Loss: 0.212521\n",
            "Epoch: 63/100, Loss: 0.208585\n",
            "Epoch: 64/100, Loss: 0.204701\n",
            "Epoch: 65/100, Loss: 0.200865\n",
            "Epoch: 66/100, Loss: 0.197083\n",
            "Epoch: 67/100, Loss: 0.193345\n",
            "Epoch: 68/100, Loss: 0.189661\n",
            "Epoch: 69/100, Loss: 0.186023\n",
            "Epoch: 70/100, Loss: 0.182431\n",
            "Epoch: 71/100, Loss: 0.178885\n",
            "Epoch: 72/100, Loss: 0.175389\n",
            "Epoch: 73/100, Loss: 0.171939\n",
            "Epoch: 74/100, Loss: 0.168536\n",
            "Epoch: 75/100, Loss: 0.165179\n",
            "Epoch: 76/100, Loss: 0.161868\n",
            "Epoch: 77/100, Loss: 0.158603\n",
            "Epoch: 78/100, Loss: 0.155383\n",
            "Epoch: 79/100, Loss: 0.152213\n",
            "Epoch: 80/100, Loss: 0.149086\n",
            "Epoch: 81/100, Loss: 0.146004\n",
            "Epoch: 82/100, Loss: 0.142970\n",
            "Epoch: 83/100, Loss: 0.139981\n",
            "Epoch: 84/100, Loss: 0.137038\n",
            "Epoch: 85/100, Loss: 0.134154\n",
            "Epoch: 86/100, Loss: 0.131315\n",
            "Epoch: 87/100, Loss: 0.128532\n",
            "Epoch: 88/100, Loss: 0.125809\n",
            "Epoch: 89/100, Loss: 0.123117\n",
            "Epoch: 90/100, Loss: 0.120439\n",
            "Epoch: 91/100, Loss: 0.117787\n",
            "Epoch: 92/100, Loss: 0.115171\n",
            "Epoch: 93/100, Loss: 0.112612\n",
            "Epoch: 94/100, Loss: 0.110096\n",
            "Epoch: 95/100, Loss: 0.107638\n",
            "Epoch: 96/100, Loss: 0.105228\n",
            "Epoch: 97/100, Loss: 0.102865\n",
            "Epoch: 98/100, Loss: 0.100548\n",
            "Epoch: 99/100, Loss: 0.098247\n",
            "Epoch: 100/100, Loss: 0.095957\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.658\n",
            "Normalised mutual info score on k-means on latent space: 0.5746180558933951\n",
            "ARI score on k-means on latent space: 0.47003992370425873\n",
            "K-means cluster error on latent space: 43816.234375\n",
            "K-means silhouette score on latent space: 0.17818277 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6288\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6438755517605085 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.49881250372846564 \n",
            "\n",
            "ROUND NUMBER  5 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.937584\n",
            "Epoch: 2/100, Loss: 0.724115\n",
            "Epoch: 3/100, Loss: 0.647950\n",
            "Epoch: 4/100, Loss: 0.611744\n",
            "Epoch: 5/100, Loss: 0.589029\n",
            "Epoch: 6/100, Loss: 0.571468\n",
            "Epoch: 7/100, Loss: 0.556632\n",
            "Epoch: 8/100, Loss: 0.543576\n",
            "Epoch: 9/100, Loss: 0.531753\n",
            "Epoch: 10/100, Loss: 0.520822\n",
            "Epoch: 11/100, Loss: 0.510577\n",
            "Epoch: 12/100, Loss: 0.500876\n",
            "Epoch: 13/100, Loss: 0.491621\n",
            "Epoch: 14/100, Loss: 0.482742\n",
            "Epoch: 15/100, Loss: 0.474187\n",
            "Epoch: 16/100, Loss: 0.465914\n",
            "Epoch: 17/100, Loss: 0.457880\n",
            "Epoch: 18/100, Loss: 0.450063\n",
            "Epoch: 19/100, Loss: 0.442450\n",
            "Epoch: 20/100, Loss: 0.435016\n",
            "Epoch: 21/100, Loss: 0.427746\n",
            "Epoch: 22/100, Loss: 0.420640\n",
            "Epoch: 23/100, Loss: 0.413684\n",
            "Epoch: 24/100, Loss: 0.406872\n",
            "Epoch: 25/100, Loss: 0.400191\n",
            "Epoch: 26/100, Loss: 0.393638\n",
            "Epoch: 27/100, Loss: 0.387203\n",
            "Epoch: 28/100, Loss: 0.380883\n",
            "Epoch: 29/100, Loss: 0.374668\n",
            "Epoch: 30/100, Loss: 0.368555\n",
            "Epoch: 31/100, Loss: 0.362539\n",
            "Epoch: 32/100, Loss: 0.356616\n",
            "Epoch: 33/100, Loss: 0.350782\n",
            "Epoch: 34/100, Loss: 0.345035\n",
            "Epoch: 35/100, Loss: 0.339371\n",
            "Epoch: 36/100, Loss: 0.333789\n",
            "Epoch: 37/100, Loss: 0.328285\n",
            "Epoch: 38/100, Loss: 0.322859\n",
            "Epoch: 39/100, Loss: 0.317508\n",
            "Epoch: 40/100, Loss: 0.312230\n",
            "Epoch: 41/100, Loss: 0.307018\n",
            "Epoch: 42/100, Loss: 0.301877\n",
            "Epoch: 43/100, Loss: 0.296803\n",
            "Epoch: 44/100, Loss: 0.291798\n",
            "Epoch: 45/100, Loss: 0.286856\n",
            "Epoch: 46/100, Loss: 0.281975\n",
            "Epoch: 47/100, Loss: 0.277155\n",
            "Epoch: 48/100, Loss: 0.272392\n",
            "Epoch: 49/100, Loss: 0.267693\n",
            "Epoch: 50/100, Loss: 0.263054\n",
            "Epoch: 51/100, Loss: 0.258470\n",
            "Epoch: 52/100, Loss: 0.253947\n",
            "Epoch: 53/100, Loss: 0.249481\n",
            "Epoch: 54/100, Loss: 0.245069\n",
            "Epoch: 55/100, Loss: 0.240710\n",
            "Epoch: 56/100, Loss: 0.236403\n",
            "Epoch: 57/100, Loss: 0.232152\n",
            "Epoch: 58/100, Loss: 0.227955\n",
            "Epoch: 59/100, Loss: 0.223811\n",
            "Epoch: 60/100, Loss: 0.219720\n",
            "Epoch: 61/100, Loss: 0.215679\n",
            "Epoch: 62/100, Loss: 0.211689\n",
            "Epoch: 63/100, Loss: 0.207749\n",
            "Epoch: 64/100, Loss: 0.203863\n",
            "Epoch: 65/100, Loss: 0.200024\n",
            "Epoch: 66/100, Loss: 0.196235\n",
            "Epoch: 67/100, Loss: 0.192496\n",
            "Epoch: 68/100, Loss: 0.188805\n",
            "Epoch: 69/100, Loss: 0.185162\n",
            "Epoch: 70/100, Loss: 0.181568\n",
            "Epoch: 71/100, Loss: 0.178021\n",
            "Epoch: 72/100, Loss: 0.174521\n",
            "Epoch: 73/100, Loss: 0.171067\n",
            "Epoch: 74/100, Loss: 0.167663\n",
            "Epoch: 75/100, Loss: 0.164307\n",
            "Epoch: 76/100, Loss: 0.160999\n",
            "Epoch: 77/100, Loss: 0.157741\n",
            "Epoch: 78/100, Loss: 0.154533\n",
            "Epoch: 79/100, Loss: 0.151377\n",
            "Epoch: 80/100, Loss: 0.148278\n",
            "Epoch: 81/100, Loss: 0.145246\n",
            "Epoch: 82/100, Loss: 0.142247\n",
            "Epoch: 83/100, Loss: 0.139256\n",
            "Epoch: 84/100, Loss: 0.136275\n",
            "Epoch: 85/100, Loss: 0.133349\n",
            "Epoch: 86/100, Loss: 0.130456\n",
            "Epoch: 87/100, Loss: 0.127608\n",
            "Epoch: 88/100, Loss: 0.124812\n",
            "Epoch: 89/100, Loss: 0.122063\n",
            "Epoch: 90/100, Loss: 0.119371\n",
            "Epoch: 91/100, Loss: 0.116745\n",
            "Epoch: 92/100, Loss: 0.114168\n",
            "Epoch: 93/100, Loss: 0.111638\n",
            "Epoch: 94/100, Loss: 0.109151\n",
            "Epoch: 95/100, Loss: 0.106706\n",
            "Epoch: 96/100, Loss: 0.104310\n",
            "Epoch: 97/100, Loss: 0.101977\n",
            "Epoch: 98/100, Loss: 0.099702\n",
            "Epoch: 99/100, Loss: 0.097471\n",
            "Epoch: 100/100, Loss: 0.095267\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6566\n",
            "Normalised mutual info score on k-means on latent space: 0.5971977285711503\n",
            "ARI score on k-means on latent space: 0.4883608686219556\n",
            "K-means cluster error on latent space: 42072.69140625\n",
            "K-means silhouette score on latent space: 0.18660496 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7157\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6895163438710905 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5664575874850962 \n",
            "\n",
            "ROUND NUMBER  6 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.933784\n",
            "Epoch: 2/100, Loss: 0.740624\n",
            "Epoch: 3/100, Loss: 0.661758\n",
            "Epoch: 4/100, Loss: 0.620245\n",
            "Epoch: 5/100, Loss: 0.592885\n",
            "Epoch: 6/100, Loss: 0.573143\n",
            "Epoch: 7/100, Loss: 0.557462\n",
            "Epoch: 8/100, Loss: 0.544092\n",
            "Epoch: 9/100, Loss: 0.532173\n",
            "Epoch: 10/100, Loss: 0.521268\n",
            "Epoch: 11/100, Loss: 0.511077\n",
            "Epoch: 12/100, Loss: 0.501433\n",
            "Epoch: 13/100, Loss: 0.492234\n",
            "Epoch: 14/100, Loss: 0.483389\n",
            "Epoch: 15/100, Loss: 0.474850\n",
            "Epoch: 16/100, Loss: 0.466579\n",
            "Epoch: 17/100, Loss: 0.458550\n",
            "Epoch: 18/100, Loss: 0.450736\n",
            "Epoch: 19/100, Loss: 0.443118\n",
            "Epoch: 20/100, Loss: 0.435684\n",
            "Epoch: 21/100, Loss: 0.428416\n",
            "Epoch: 22/100, Loss: 0.421300\n",
            "Epoch: 23/100, Loss: 0.414338\n",
            "Epoch: 24/100, Loss: 0.407517\n",
            "Epoch: 25/100, Loss: 0.400828\n",
            "Epoch: 26/100, Loss: 0.394270\n",
            "Epoch: 27/100, Loss: 0.387829\n",
            "Epoch: 28/100, Loss: 0.381498\n",
            "Epoch: 29/100, Loss: 0.375273\n",
            "Epoch: 30/100, Loss: 0.369155\n",
            "Epoch: 31/100, Loss: 0.363132\n",
            "Epoch: 32/100, Loss: 0.357206\n",
            "Epoch: 33/100, Loss: 0.351375\n",
            "Epoch: 34/100, Loss: 0.345628\n",
            "Epoch: 35/100, Loss: 0.339963\n",
            "Epoch: 36/100, Loss: 0.334384\n",
            "Epoch: 37/100, Loss: 0.328881\n",
            "Epoch: 38/100, Loss: 0.323452\n",
            "Epoch: 39/100, Loss: 0.318097\n",
            "Epoch: 40/100, Loss: 0.312815\n",
            "Epoch: 41/100, Loss: 0.307599\n",
            "Epoch: 42/100, Loss: 0.302455\n",
            "Epoch: 43/100, Loss: 0.297382\n",
            "Epoch: 44/100, Loss: 0.292373\n",
            "Epoch: 45/100, Loss: 0.287424\n",
            "Epoch: 46/100, Loss: 0.282537\n",
            "Epoch: 47/100, Loss: 0.277718\n",
            "Epoch: 48/100, Loss: 0.272959\n",
            "Epoch: 49/100, Loss: 0.268259\n",
            "Epoch: 50/100, Loss: 0.263614\n",
            "Epoch: 51/100, Loss: 0.259027\n",
            "Epoch: 52/100, Loss: 0.254499\n",
            "Epoch: 53/100, Loss: 0.250024\n",
            "Epoch: 54/100, Loss: 0.245610\n",
            "Epoch: 55/100, Loss: 0.241250\n",
            "Epoch: 56/100, Loss: 0.236940\n",
            "Epoch: 57/100, Loss: 0.232685\n",
            "Epoch: 58/100, Loss: 0.228482\n",
            "Epoch: 59/100, Loss: 0.224337\n",
            "Epoch: 60/100, Loss: 0.220241\n",
            "Epoch: 61/100, Loss: 0.216198\n",
            "Epoch: 62/100, Loss: 0.212206\n",
            "Epoch: 63/100, Loss: 0.208266\n",
            "Epoch: 64/100, Loss: 0.204372\n",
            "Epoch: 65/100, Loss: 0.200531\n",
            "Epoch: 66/100, Loss: 0.196740\n",
            "Epoch: 67/100, Loss: 0.193000\n",
            "Epoch: 68/100, Loss: 0.189307\n",
            "Epoch: 69/100, Loss: 0.185661\n",
            "Epoch: 70/100, Loss: 0.182067\n",
            "Epoch: 71/100, Loss: 0.178521\n",
            "Epoch: 72/100, Loss: 0.175025\n",
            "Epoch: 73/100, Loss: 0.171572\n",
            "Epoch: 74/100, Loss: 0.168168\n",
            "Epoch: 75/100, Loss: 0.164812\n",
            "Epoch: 76/100, Loss: 0.161504\n",
            "Epoch: 77/100, Loss: 0.158244\n",
            "Epoch: 78/100, Loss: 0.155025\n",
            "Epoch: 79/100, Loss: 0.151852\n",
            "Epoch: 80/100, Loss: 0.148732\n",
            "Epoch: 81/100, Loss: 0.145658\n",
            "Epoch: 82/100, Loss: 0.142632\n",
            "Epoch: 83/100, Loss: 0.139665\n",
            "Epoch: 84/100, Loss: 0.136755\n",
            "Epoch: 85/100, Loss: 0.133900\n",
            "Epoch: 86/100, Loss: 0.131091\n",
            "Epoch: 87/100, Loss: 0.128322\n",
            "Epoch: 88/100, Loss: 0.125608\n",
            "Epoch: 89/100, Loss: 0.122921\n",
            "Epoch: 90/100, Loss: 0.120219\n",
            "Epoch: 91/100, Loss: 0.117567\n",
            "Epoch: 92/100, Loss: 0.114977\n",
            "Epoch: 93/100, Loss: 0.112453\n",
            "Epoch: 94/100, Loss: 0.109984\n",
            "Epoch: 95/100, Loss: 0.107587\n",
            "Epoch: 96/100, Loss: 0.105240\n",
            "Epoch: 97/100, Loss: 0.102896\n",
            "Epoch: 98/100, Loss: 0.100572\n",
            "Epoch: 99/100, Loss: 0.098240\n",
            "Epoch: 100/100, Loss: 0.095924\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.7045\n",
            "Normalised mutual info score on k-means on latent space: 0.6179509573935439\n",
            "ARI score on k-means on latent space: 0.5372716413331748\n",
            "K-means cluster error on latent space: 42026.44921875\n",
            "K-means silhouette score on latent space: 0.1894188 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6473\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6626971346802785 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5145938258547222 \n",
            "\n",
            "ROUND NUMBER  7 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.924190\n",
            "Epoch: 2/100, Loss: 0.729493\n",
            "Epoch: 3/100, Loss: 0.650994\n",
            "Epoch: 4/100, Loss: 0.613580\n",
            "Epoch: 5/100, Loss: 0.589529\n",
            "Epoch: 6/100, Loss: 0.571495\n",
            "Epoch: 7/100, Loss: 0.556644\n",
            "Epoch: 8/100, Loss: 0.543678\n",
            "Epoch: 9/100, Loss: 0.531965\n",
            "Epoch: 10/100, Loss: 0.521146\n",
            "Epoch: 11/100, Loss: 0.510992\n",
            "Epoch: 12/100, Loss: 0.501361\n",
            "Epoch: 13/100, Loss: 0.492157\n",
            "Epoch: 14/100, Loss: 0.483317\n",
            "Epoch: 15/100, Loss: 0.474790\n",
            "Epoch: 16/100, Loss: 0.466532\n",
            "Epoch: 17/100, Loss: 0.458522\n",
            "Epoch: 18/100, Loss: 0.450728\n",
            "Epoch: 19/100, Loss: 0.443138\n",
            "Epoch: 20/100, Loss: 0.435728\n",
            "Epoch: 21/100, Loss: 0.428485\n",
            "Epoch: 22/100, Loss: 0.421399\n",
            "Epoch: 23/100, Loss: 0.414461\n",
            "Epoch: 24/100, Loss: 0.407661\n",
            "Epoch: 25/100, Loss: 0.400987\n",
            "Epoch: 26/100, Loss: 0.394435\n",
            "Epoch: 27/100, Loss: 0.387998\n",
            "Epoch: 28/100, Loss: 0.381673\n",
            "Epoch: 29/100, Loss: 0.375459\n",
            "Epoch: 30/100, Loss: 0.369345\n",
            "Epoch: 31/100, Loss: 0.363332\n",
            "Epoch: 32/100, Loss: 0.357409\n",
            "Epoch: 33/100, Loss: 0.351575\n",
            "Epoch: 34/100, Loss: 0.345828\n",
            "Epoch: 35/100, Loss: 0.340166\n",
            "Epoch: 36/100, Loss: 0.334581\n",
            "Epoch: 37/100, Loss: 0.329075\n",
            "Epoch: 38/100, Loss: 0.323649\n",
            "Epoch: 39/100, Loss: 0.318297\n",
            "Epoch: 40/100, Loss: 0.313012\n",
            "Epoch: 41/100, Loss: 0.307801\n",
            "Epoch: 42/100, Loss: 0.302658\n",
            "Epoch: 43/100, Loss: 0.297580\n",
            "Epoch: 44/100, Loss: 0.292571\n",
            "Epoch: 45/100, Loss: 0.287624\n",
            "Epoch: 46/100, Loss: 0.282738\n",
            "Epoch: 47/100, Loss: 0.277917\n",
            "Epoch: 48/100, Loss: 0.273157\n",
            "Epoch: 49/100, Loss: 0.268455\n",
            "Epoch: 50/100, Loss: 0.263810\n",
            "Epoch: 51/100, Loss: 0.259223\n",
            "Epoch: 52/100, Loss: 0.254698\n",
            "Epoch: 53/100, Loss: 0.250222\n",
            "Epoch: 54/100, Loss: 0.245803\n",
            "Epoch: 55/100, Loss: 0.241443\n",
            "Epoch: 56/100, Loss: 0.237134\n",
            "Epoch: 57/100, Loss: 0.232881\n",
            "Epoch: 58/100, Loss: 0.228681\n",
            "Epoch: 59/100, Loss: 0.224534\n",
            "Epoch: 60/100, Loss: 0.220443\n",
            "Epoch: 61/100, Loss: 0.216403\n",
            "Epoch: 62/100, Loss: 0.212414\n",
            "Epoch: 63/100, Loss: 0.208473\n",
            "Epoch: 64/100, Loss: 0.204583\n",
            "Epoch: 65/100, Loss: 0.200744\n",
            "Epoch: 66/100, Loss: 0.196951\n",
            "Epoch: 67/100, Loss: 0.193210\n",
            "Epoch: 68/100, Loss: 0.189516\n",
            "Epoch: 69/100, Loss: 0.185870\n",
            "Epoch: 70/100, Loss: 0.182272\n",
            "Epoch: 71/100, Loss: 0.178722\n",
            "Epoch: 72/100, Loss: 0.175220\n",
            "Epoch: 73/100, Loss: 0.171768\n",
            "Epoch: 74/100, Loss: 0.168360\n",
            "Epoch: 75/100, Loss: 0.165002\n",
            "Epoch: 76/100, Loss: 0.161691\n",
            "Epoch: 77/100, Loss: 0.158424\n",
            "Epoch: 78/100, Loss: 0.155207\n",
            "Epoch: 79/100, Loss: 0.152036\n",
            "Epoch: 80/100, Loss: 0.148916\n",
            "Epoch: 81/100, Loss: 0.145845\n",
            "Epoch: 82/100, Loss: 0.142823\n",
            "Epoch: 83/100, Loss: 0.139864\n",
            "Epoch: 84/100, Loss: 0.136962\n",
            "Epoch: 85/100, Loss: 0.134118\n",
            "Epoch: 86/100, Loss: 0.131343\n",
            "Epoch: 87/100, Loss: 0.128622\n",
            "Epoch: 88/100, Loss: 0.125898\n",
            "Epoch: 89/100, Loss: 0.123203\n",
            "Epoch: 90/100, Loss: 0.120502\n",
            "Epoch: 91/100, Loss: 0.117792\n",
            "Epoch: 92/100, Loss: 0.115146\n",
            "Epoch: 93/100, Loss: 0.112547\n",
            "Epoch: 94/100, Loss: 0.109989\n",
            "Epoch: 95/100, Loss: 0.107468\n",
            "Epoch: 96/100, Loss: 0.104995\n",
            "Epoch: 97/100, Loss: 0.102560\n",
            "Epoch: 98/100, Loss: 0.100188\n",
            "Epoch: 99/100, Loss: 0.097867\n",
            "Epoch: 100/100, Loss: 0.095607\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6929\n",
            "Normalised mutual info score on k-means on latent space: 0.6012411636330718\n",
            "ARI score on k-means on latent space: 0.5191419350289018\n",
            "K-means cluster error on latent space: 41857.0234375\n",
            "K-means silhouette score on latent space: 0.17612548 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.736\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.7003157730319118 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.597243134769545 \n",
            "\n",
            "ROUND NUMBER  8 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.930753\n",
            "Epoch: 2/100, Loss: 0.742497\n",
            "Epoch: 3/100, Loss: 0.658750\n",
            "Epoch: 4/100, Loss: 0.615482\n",
            "Epoch: 5/100, Loss: 0.590276\n",
            "Epoch: 6/100, Loss: 0.572128\n",
            "Epoch: 7/100, Loss: 0.557295\n",
            "Epoch: 8/100, Loss: 0.544329\n",
            "Epoch: 9/100, Loss: 0.532594\n",
            "Epoch: 10/100, Loss: 0.521735\n",
            "Epoch: 11/100, Loss: 0.511540\n",
            "Epoch: 12/100, Loss: 0.501872\n",
            "Epoch: 13/100, Loss: 0.492630\n",
            "Epoch: 14/100, Loss: 0.483740\n",
            "Epoch: 15/100, Loss: 0.475165\n",
            "Epoch: 16/100, Loss: 0.466856\n",
            "Epoch: 17/100, Loss: 0.458792\n",
            "Epoch: 18/100, Loss: 0.450941\n",
            "Epoch: 19/100, Loss: 0.443296\n",
            "Epoch: 20/100, Loss: 0.435835\n",
            "Epoch: 21/100, Loss: 0.428541\n",
            "Epoch: 22/100, Loss: 0.421411\n",
            "Epoch: 23/100, Loss: 0.414428\n",
            "Epoch: 24/100, Loss: 0.407585\n",
            "Epoch: 25/100, Loss: 0.400872\n",
            "Epoch: 26/100, Loss: 0.394288\n",
            "Epoch: 27/100, Loss: 0.387827\n",
            "Epoch: 28/100, Loss: 0.381479\n",
            "Epoch: 29/100, Loss: 0.375238\n",
            "Epoch: 30/100, Loss: 0.369106\n",
            "Epoch: 31/100, Loss: 0.363075\n",
            "Epoch: 32/100, Loss: 0.357143\n",
            "Epoch: 33/100, Loss: 0.351302\n",
            "Epoch: 34/100, Loss: 0.345550\n",
            "Epoch: 35/100, Loss: 0.339883\n",
            "Epoch: 36/100, Loss: 0.334296\n",
            "Epoch: 37/100, Loss: 0.328787\n",
            "Epoch: 38/100, Loss: 0.323357\n",
            "Epoch: 39/100, Loss: 0.318002\n",
            "Epoch: 40/100, Loss: 0.312716\n",
            "Epoch: 41/100, Loss: 0.307506\n",
            "Epoch: 42/100, Loss: 0.302364\n",
            "Epoch: 43/100, Loss: 0.297291\n",
            "Epoch: 44/100, Loss: 0.292282\n",
            "Epoch: 45/100, Loss: 0.287338\n",
            "Epoch: 46/100, Loss: 0.282457\n",
            "Epoch: 47/100, Loss: 0.277635\n",
            "Epoch: 48/100, Loss: 0.272879\n",
            "Epoch: 49/100, Loss: 0.268181\n",
            "Epoch: 50/100, Loss: 0.263540\n",
            "Epoch: 51/100, Loss: 0.258959\n",
            "Epoch: 52/100, Loss: 0.254435\n",
            "Epoch: 53/100, Loss: 0.249966\n",
            "Epoch: 54/100, Loss: 0.245555\n",
            "Epoch: 55/100, Loss: 0.241198\n",
            "Epoch: 56/100, Loss: 0.236895\n",
            "Epoch: 57/100, Loss: 0.232645\n",
            "Epoch: 58/100, Loss: 0.228450\n",
            "Epoch: 59/100, Loss: 0.224308\n",
            "Epoch: 60/100, Loss: 0.220217\n",
            "Epoch: 61/100, Loss: 0.216179\n",
            "Epoch: 62/100, Loss: 0.212190\n",
            "Epoch: 63/100, Loss: 0.208251\n",
            "Epoch: 64/100, Loss: 0.204362\n",
            "Epoch: 65/100, Loss: 0.200525\n",
            "Epoch: 66/100, Loss: 0.196739\n",
            "Epoch: 67/100, Loss: 0.192998\n",
            "Epoch: 68/100, Loss: 0.189304\n",
            "Epoch: 69/100, Loss: 0.185660\n",
            "Epoch: 70/100, Loss: 0.182064\n",
            "Epoch: 71/100, Loss: 0.178520\n",
            "Epoch: 72/100, Loss: 0.175022\n",
            "Epoch: 73/100, Loss: 0.171571\n",
            "Epoch: 74/100, Loss: 0.168165\n",
            "Epoch: 75/100, Loss: 0.164809\n",
            "Epoch: 76/100, Loss: 0.161507\n",
            "Epoch: 77/100, Loss: 0.158253\n",
            "Epoch: 78/100, Loss: 0.155052\n",
            "Epoch: 79/100, Loss: 0.151902\n",
            "Epoch: 80/100, Loss: 0.148806\n",
            "Epoch: 81/100, Loss: 0.145757\n",
            "Epoch: 82/100, Loss: 0.142771\n",
            "Epoch: 83/100, Loss: 0.139849\n",
            "Epoch: 84/100, Loss: 0.136967\n",
            "Epoch: 85/100, Loss: 0.134073\n",
            "Epoch: 86/100, Loss: 0.131167\n",
            "Epoch: 87/100, Loss: 0.128287\n",
            "Epoch: 88/100, Loss: 0.125457\n",
            "Epoch: 89/100, Loss: 0.122688\n",
            "Epoch: 90/100, Loss: 0.119970\n",
            "Epoch: 91/100, Loss: 0.117318\n",
            "Epoch: 92/100, Loss: 0.114727\n",
            "Epoch: 93/100, Loss: 0.112164\n",
            "Epoch: 94/100, Loss: 0.109652\n",
            "Epoch: 95/100, Loss: 0.107177\n",
            "Epoch: 96/100, Loss: 0.104727\n",
            "Epoch: 97/100, Loss: 0.102326\n",
            "Epoch: 98/100, Loss: 0.099968\n",
            "Epoch: 99/100, Loss: 0.097649\n",
            "Epoch: 100/100, Loss: 0.095368\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6627\n",
            "Normalised mutual info score on k-means on latent space: 0.601057441071968\n",
            "ARI score on k-means on latent space: 0.4923374726388357\n",
            "K-means cluster error on latent space: 44151.03125\n",
            "K-means silhouette score on latent space: 0.17387056 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.729\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6918120216985424 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5976805549945123 \n",
            "\n",
            "ROUND NUMBER  9 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.929180\n",
            "Epoch: 2/100, Loss: 0.732019\n",
            "Epoch: 3/100, Loss: 0.655485\n",
            "Epoch: 4/100, Loss: 0.615135\n",
            "Epoch: 5/100, Loss: 0.589320\n",
            "Epoch: 6/100, Loss: 0.570699\n",
            "Epoch: 7/100, Loss: 0.555599\n",
            "Epoch: 8/100, Loss: 0.542490\n",
            "Epoch: 9/100, Loss: 0.530767\n",
            "Epoch: 10/100, Loss: 0.519981\n",
            "Epoch: 11/100, Loss: 0.509874\n",
            "Epoch: 12/100, Loss: 0.500310\n",
            "Epoch: 13/100, Loss: 0.491171\n",
            "Epoch: 14/100, Loss: 0.482377\n",
            "Epoch: 15/100, Loss: 0.473890\n",
            "Epoch: 16/100, Loss: 0.465672\n",
            "Epoch: 17/100, Loss: 0.457697\n",
            "Epoch: 18/100, Loss: 0.449930\n",
            "Epoch: 19/100, Loss: 0.442356\n",
            "Epoch: 20/100, Loss: 0.434963\n",
            "Epoch: 21/100, Loss: 0.427731\n",
            "Epoch: 22/100, Loss: 0.420654\n",
            "Epoch: 23/100, Loss: 0.413718\n",
            "Epoch: 24/100, Loss: 0.406919\n",
            "Epoch: 25/100, Loss: 0.400251\n",
            "Epoch: 26/100, Loss: 0.393700\n",
            "Epoch: 27/100, Loss: 0.387268\n",
            "Epoch: 28/100, Loss: 0.380945\n",
            "Epoch: 29/100, Loss: 0.374724\n",
            "Epoch: 30/100, Loss: 0.368604\n",
            "Epoch: 31/100, Loss: 0.362586\n",
            "Epoch: 32/100, Loss: 0.356658\n",
            "Epoch: 33/100, Loss: 0.350828\n",
            "Epoch: 34/100, Loss: 0.345081\n",
            "Epoch: 35/100, Loss: 0.339424\n",
            "Epoch: 36/100, Loss: 0.333847\n",
            "Epoch: 37/100, Loss: 0.328351\n",
            "Epoch: 38/100, Loss: 0.322931\n",
            "Epoch: 39/100, Loss: 0.317585\n",
            "Epoch: 40/100, Loss: 0.312312\n",
            "Epoch: 41/100, Loss: 0.307111\n",
            "Epoch: 42/100, Loss: 0.301976\n",
            "Epoch: 43/100, Loss: 0.296912\n",
            "Epoch: 44/100, Loss: 0.291913\n",
            "Epoch: 45/100, Loss: 0.286975\n",
            "Epoch: 46/100, Loss: 0.282101\n",
            "Epoch: 47/100, Loss: 0.277290\n",
            "Epoch: 48/100, Loss: 0.272538\n",
            "Epoch: 49/100, Loss: 0.267847\n",
            "Epoch: 50/100, Loss: 0.263213\n",
            "Epoch: 51/100, Loss: 0.258637\n",
            "Epoch: 52/100, Loss: 0.254119\n",
            "Epoch: 53/100, Loss: 0.249657\n",
            "Epoch: 54/100, Loss: 0.245248\n",
            "Epoch: 55/100, Loss: 0.240893\n",
            "Epoch: 56/100, Loss: 0.236594\n",
            "Epoch: 57/100, Loss: 0.232350\n",
            "Epoch: 58/100, Loss: 0.228159\n",
            "Epoch: 59/100, Loss: 0.224022\n",
            "Epoch: 60/100, Loss: 0.219935\n",
            "Epoch: 61/100, Loss: 0.215900\n",
            "Epoch: 62/100, Loss: 0.211919\n",
            "Epoch: 63/100, Loss: 0.207986\n",
            "Epoch: 64/100, Loss: 0.204102\n",
            "Epoch: 65/100, Loss: 0.200268\n",
            "Epoch: 66/100, Loss: 0.196483\n",
            "Epoch: 67/100, Loss: 0.192748\n",
            "Epoch: 68/100, Loss: 0.189062\n",
            "Epoch: 69/100, Loss: 0.185425\n",
            "Epoch: 70/100, Loss: 0.181835\n",
            "Epoch: 71/100, Loss: 0.178294\n",
            "Epoch: 72/100, Loss: 0.174798\n",
            "Epoch: 73/100, Loss: 0.171347\n",
            "Epoch: 74/100, Loss: 0.167943\n",
            "Epoch: 75/100, Loss: 0.164584\n",
            "Epoch: 76/100, Loss: 0.161274\n",
            "Epoch: 77/100, Loss: 0.158011\n",
            "Epoch: 78/100, Loss: 0.154793\n",
            "Epoch: 79/100, Loss: 0.151622\n",
            "Epoch: 80/100, Loss: 0.148498\n",
            "Epoch: 81/100, Loss: 0.145417\n",
            "Epoch: 82/100, Loss: 0.142383\n",
            "Epoch: 83/100, Loss: 0.139401\n",
            "Epoch: 84/100, Loss: 0.136471\n",
            "Epoch: 85/100, Loss: 0.133597\n",
            "Epoch: 86/100, Loss: 0.130774\n",
            "Epoch: 87/100, Loss: 0.127994\n",
            "Epoch: 88/100, Loss: 0.125235\n",
            "Epoch: 89/100, Loss: 0.122492\n",
            "Epoch: 90/100, Loss: 0.119789\n",
            "Epoch: 91/100, Loss: 0.117142\n",
            "Epoch: 92/100, Loss: 0.114557\n",
            "Epoch: 93/100, Loss: 0.112011\n",
            "Epoch: 94/100, Loss: 0.109503\n",
            "Epoch: 95/100, Loss: 0.107018\n",
            "Epoch: 96/100, Loss: 0.104562\n",
            "Epoch: 97/100, Loss: 0.102146\n",
            "Epoch: 98/100, Loss: 0.099797\n",
            "Epoch: 99/100, Loss: 0.097511\n",
            "Epoch: 100/100, Loss: 0.095278\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6603\n",
            "Normalised mutual info score on k-means on latent space: 0.5828149143261255\n",
            "ARI score on k-means on latent space: 0.4797848625376256\n",
            "K-means cluster error on latent space: 42589.73046875\n",
            "K-means silhouette score on latent space: 0.18557 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.721\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.7055237353756553 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.592719041233122 \n",
            "\n",
            "ROUND NUMBER  10 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.902583\n",
            "Epoch: 2/100, Loss: 0.710973\n",
            "Epoch: 3/100, Loss: 0.643588\n",
            "Epoch: 4/100, Loss: 0.608179\n",
            "Epoch: 5/100, Loss: 0.585374\n",
            "Epoch: 6/100, Loss: 0.568022\n",
            "Epoch: 7/100, Loss: 0.553651\n",
            "Epoch: 8/100, Loss: 0.541086\n",
            "Epoch: 9/100, Loss: 0.529667\n",
            "Epoch: 10/100, Loss: 0.519069\n",
            "Epoch: 11/100, Loss: 0.509098\n",
            "Epoch: 12/100, Loss: 0.499607\n",
            "Epoch: 13/100, Loss: 0.490523\n",
            "Epoch: 14/100, Loss: 0.481779\n",
            "Epoch: 15/100, Loss: 0.473330\n",
            "Epoch: 16/100, Loss: 0.465141\n",
            "Epoch: 17/100, Loss: 0.457189\n",
            "Epoch: 18/100, Loss: 0.449445\n",
            "Epoch: 19/100, Loss: 0.441893\n",
            "Epoch: 20/100, Loss: 0.434520\n",
            "Epoch: 21/100, Loss: 0.427306\n",
            "Epoch: 22/100, Loss: 0.420250\n",
            "Epoch: 23/100, Loss: 0.413335\n",
            "Epoch: 24/100, Loss: 0.406554\n",
            "Epoch: 25/100, Loss: 0.399905\n",
            "Epoch: 26/100, Loss: 0.393376\n",
            "Epoch: 27/100, Loss: 0.386968\n",
            "Epoch: 28/100, Loss: 0.380671\n",
            "Epoch: 29/100, Loss: 0.374478\n",
            "Epoch: 30/100, Loss: 0.368386\n",
            "Epoch: 31/100, Loss: 0.362388\n",
            "Epoch: 32/100, Loss: 0.356488\n",
            "Epoch: 33/100, Loss: 0.350677\n",
            "Epoch: 34/100, Loss: 0.344946\n",
            "Epoch: 35/100, Loss: 0.339302\n",
            "Epoch: 36/100, Loss: 0.333736\n",
            "Epoch: 37/100, Loss: 0.328251\n",
            "Epoch: 38/100, Loss: 0.322839\n",
            "Epoch: 39/100, Loss: 0.317504\n",
            "Epoch: 40/100, Loss: 0.312242\n",
            "Epoch: 41/100, Loss: 0.307045\n",
            "Epoch: 42/100, Loss: 0.301920\n",
            "Epoch: 43/100, Loss: 0.296857\n",
            "Epoch: 44/100, Loss: 0.291864\n",
            "Epoch: 45/100, Loss: 0.286932\n",
            "Epoch: 46/100, Loss: 0.282064\n",
            "Epoch: 47/100, Loss: 0.277259\n",
            "Epoch: 48/100, Loss: 0.272513\n",
            "Epoch: 49/100, Loss: 0.267826\n",
            "Epoch: 50/100, Loss: 0.263198\n",
            "Epoch: 51/100, Loss: 0.258629\n",
            "Epoch: 52/100, Loss: 0.254116\n",
            "Epoch: 53/100, Loss: 0.249657\n",
            "Epoch: 54/100, Loss: 0.245251\n",
            "Epoch: 55/100, Loss: 0.240896\n",
            "Epoch: 56/100, Loss: 0.236602\n",
            "Epoch: 57/100, Loss: 0.232360\n",
            "Epoch: 58/100, Loss: 0.228171\n",
            "Epoch: 59/100, Loss: 0.224033\n",
            "Epoch: 60/100, Loss: 0.219951\n",
            "Epoch: 61/100, Loss: 0.215916\n",
            "Epoch: 62/100, Loss: 0.211933\n",
            "Epoch: 63/100, Loss: 0.208002\n",
            "Epoch: 64/100, Loss: 0.204122\n",
            "Epoch: 65/100, Loss: 0.200291\n",
            "Epoch: 66/100, Loss: 0.196509\n",
            "Epoch: 67/100, Loss: 0.192776\n",
            "Epoch: 68/100, Loss: 0.189095\n",
            "Epoch: 69/100, Loss: 0.185463\n",
            "Epoch: 70/100, Loss: 0.181876\n",
            "Epoch: 71/100, Loss: 0.178337\n",
            "Epoch: 72/100, Loss: 0.174850\n",
            "Epoch: 73/100, Loss: 0.171407\n",
            "Epoch: 74/100, Loss: 0.168011\n",
            "Epoch: 75/100, Loss: 0.164659\n",
            "Epoch: 76/100, Loss: 0.161357\n",
            "Epoch: 77/100, Loss: 0.158100\n",
            "Epoch: 78/100, Loss: 0.154895\n",
            "Epoch: 79/100, Loss: 0.151739\n",
            "Epoch: 80/100, Loss: 0.148635\n",
            "Epoch: 81/100, Loss: 0.145592\n",
            "Epoch: 82/100, Loss: 0.142612\n",
            "Epoch: 83/100, Loss: 0.139694\n",
            "Epoch: 84/100, Loss: 0.136810\n",
            "Epoch: 85/100, Loss: 0.133946\n",
            "Epoch: 86/100, Loss: 0.131073\n",
            "Epoch: 87/100, Loss: 0.128192\n",
            "Epoch: 88/100, Loss: 0.125353\n",
            "Epoch: 89/100, Loss: 0.122570\n",
            "Epoch: 90/100, Loss: 0.119842\n",
            "Epoch: 91/100, Loss: 0.117175\n",
            "Epoch: 92/100, Loss: 0.114565\n",
            "Epoch: 93/100, Loss: 0.112015\n",
            "Epoch: 94/100, Loss: 0.109513\n",
            "Epoch: 95/100, Loss: 0.107061\n",
            "Epoch: 96/100, Loss: 0.104652\n",
            "Epoch: 97/100, Loss: 0.102288\n",
            "Epoch: 98/100, Loss: 0.099958\n",
            "Epoch: 99/100, Loss: 0.097661\n",
            "Epoch: 100/100, Loss: 0.095383\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.7174\n",
            "Normalised mutual info score on k-means on latent space: 0.6347919787231426\n",
            "ARI score on k-means on latent space: 0.558712485017823\n",
            "K-means cluster error on latent space: 41525.9765625\n",
            "K-means silhouette score on latent space: 0.19228506 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7373\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6929569792586399 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.57952871575167 \n",
            "\n",
            "Average k-means accuracy score at latent space: 0.68049 \n",
            "\n",
            "Average k-means NMI score at latent space: 0.6015537689334629 \n",
            "\n",
            "Average k-means ARI score at latent space: 0.5075776924540829 \n",
            "\n",
            "Average agglomerative clustering accuracy score at latent space: 0.7071500000000001 \n",
            "\n",
            "Average agglomerative clustering NMI score at latent space: 0.6858418747401115 \n",
            "\n",
            "Average agglomerative clustering ARI score at latent space: 0.5658039548687998 \n",
            "\n",
            "Average k-means silhouette score on latent space: 0.18323881030082703 \n",
            "\n",
            "Average k-means cluster error on latent space: 42762.34921875 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_for_k_60 = run_experiment(60, dataloader, 784, datapoints = images, datapoints_reshaped = images_reshaped, labels = labels, epochs=100, hl1_neurons=32, hl2_neurons=64, hl3_neurons=128, latent_dimension=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssu74yY07yhP",
        "outputId": "1dd01b86-8b9b-4a66-a897-843f4a16156d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment results for k-means with k = 60 clusters:\n",
            "\n",
            "Running k-means algorithm in order to get our pseudolabels: \n",
            "\n",
            "K_means greedy accuracy score (initial space): 0.8073\n",
            "Normalised mutual info score (initial space): 0.6974304113284331\n",
            "ARI (initial space): 0.6467915497396858 \n",
            "\n",
            "Using the autoencoder model on our data: \n",
            "\n",
            "ROUND NUMBER  1 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.912483\n",
            "Epoch: 2/100, Loss: 0.724748\n",
            "Epoch: 3/100, Loss: 0.650582\n",
            "Epoch: 4/100, Loss: 0.612820\n",
            "Epoch: 5/100, Loss: 0.588240\n",
            "Epoch: 6/100, Loss: 0.569890\n",
            "Epoch: 7/100, Loss: 0.554844\n",
            "Epoch: 8/100, Loss: 0.541785\n",
            "Epoch: 9/100, Loss: 0.530055\n",
            "Epoch: 10/100, Loss: 0.519258\n",
            "Epoch: 11/100, Loss: 0.509152\n",
            "Epoch: 12/100, Loss: 0.499569\n",
            "Epoch: 13/100, Loss: 0.490407\n",
            "Epoch: 14/100, Loss: 0.481601\n",
            "Epoch: 15/100, Loss: 0.473114\n",
            "Epoch: 16/100, Loss: 0.464890\n",
            "Epoch: 17/100, Loss: 0.456904\n",
            "Epoch: 18/100, Loss: 0.449136\n",
            "Epoch: 19/100, Loss: 0.441567\n",
            "Epoch: 20/100, Loss: 0.434176\n",
            "Epoch: 21/100, Loss: 0.426946\n",
            "Epoch: 22/100, Loss: 0.419871\n",
            "Epoch: 23/100, Loss: 0.412946\n",
            "Epoch: 24/100, Loss: 0.406159\n",
            "Epoch: 25/100, Loss: 0.399500\n",
            "Epoch: 26/100, Loss: 0.392966\n",
            "Epoch: 27/100, Loss: 0.386546\n",
            "Epoch: 28/100, Loss: 0.380241\n",
            "Epoch: 29/100, Loss: 0.374041\n",
            "Epoch: 30/100, Loss: 0.367942\n",
            "Epoch: 31/100, Loss: 0.361940\n",
            "Epoch: 32/100, Loss: 0.356038\n",
            "Epoch: 33/100, Loss: 0.350224\n",
            "Epoch: 34/100, Loss: 0.344495\n",
            "Epoch: 35/100, Loss: 0.338853\n",
            "Epoch: 36/100, Loss: 0.333290\n",
            "Epoch: 37/100, Loss: 0.327810\n",
            "Epoch: 38/100, Loss: 0.322401\n",
            "Epoch: 39/100, Loss: 0.317070\n",
            "Epoch: 40/100, Loss: 0.311811\n",
            "Epoch: 41/100, Loss: 0.306622\n",
            "Epoch: 42/100, Loss: 0.301500\n",
            "Epoch: 43/100, Loss: 0.296443\n",
            "Epoch: 44/100, Loss: 0.291452\n",
            "Epoch: 45/100, Loss: 0.286526\n",
            "Epoch: 46/100, Loss: 0.281661\n",
            "Epoch: 47/100, Loss: 0.276858\n",
            "Epoch: 48/100, Loss: 0.272113\n",
            "Epoch: 49/100, Loss: 0.267429\n",
            "Epoch: 50/100, Loss: 0.262805\n",
            "Epoch: 51/100, Loss: 0.258237\n",
            "Epoch: 52/100, Loss: 0.253727\n",
            "Epoch: 53/100, Loss: 0.249273\n",
            "Epoch: 54/100, Loss: 0.244870\n",
            "Epoch: 55/100, Loss: 0.240523\n",
            "Epoch: 56/100, Loss: 0.236232\n",
            "Epoch: 57/100, Loss: 0.231996\n",
            "Epoch: 58/100, Loss: 0.227813\n",
            "Epoch: 59/100, Loss: 0.223680\n",
            "Epoch: 60/100, Loss: 0.219603\n",
            "Epoch: 61/100, Loss: 0.215575\n",
            "Epoch: 62/100, Loss: 0.211595\n",
            "Epoch: 63/100, Loss: 0.207665\n",
            "Epoch: 64/100, Loss: 0.203786\n",
            "Epoch: 65/100, Loss: 0.199961\n",
            "Epoch: 66/100, Loss: 0.196184\n",
            "Epoch: 67/100, Loss: 0.192455\n",
            "Epoch: 68/100, Loss: 0.188775\n",
            "Epoch: 69/100, Loss: 0.185144\n",
            "Epoch: 70/100, Loss: 0.181566\n",
            "Epoch: 71/100, Loss: 0.178036\n",
            "Epoch: 72/100, Loss: 0.174557\n",
            "Epoch: 73/100, Loss: 0.171122\n",
            "Epoch: 74/100, Loss: 0.167735\n",
            "Epoch: 75/100, Loss: 0.164394\n",
            "Epoch: 76/100, Loss: 0.161098\n",
            "Epoch: 77/100, Loss: 0.157848\n",
            "Epoch: 78/100, Loss: 0.154646\n",
            "Epoch: 79/100, Loss: 0.151497\n",
            "Epoch: 80/100, Loss: 0.148403\n",
            "Epoch: 81/100, Loss: 0.145379\n",
            "Epoch: 82/100, Loss: 0.142392\n",
            "Epoch: 83/100, Loss: 0.139462\n",
            "Epoch: 84/100, Loss: 0.136595\n",
            "Epoch: 85/100, Loss: 0.133756\n",
            "Epoch: 86/100, Loss: 0.130899\n",
            "Epoch: 87/100, Loss: 0.128073\n",
            "Epoch: 88/100, Loss: 0.125274\n",
            "Epoch: 89/100, Loss: 0.122502\n",
            "Epoch: 90/100, Loss: 0.119770\n",
            "Epoch: 91/100, Loss: 0.117119\n",
            "Epoch: 92/100, Loss: 0.114539\n",
            "Epoch: 93/100, Loss: 0.112021\n",
            "Epoch: 94/100, Loss: 0.109565\n",
            "Epoch: 95/100, Loss: 0.107180\n",
            "Epoch: 96/100, Loss: 0.104834\n",
            "Epoch: 97/100, Loss: 0.102486\n",
            "Epoch: 98/100, Loss: 0.100148\n",
            "Epoch: 99/100, Loss: 0.097812\n",
            "Epoch: 100/100, Loss: 0.095472\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.665\n",
            "Normalised mutual info score on k-means on latent space: 0.5961883310340801\n",
            "ARI score on k-means on latent space: 0.49612048734441866\n",
            "K-means cluster error on latent space: 42594.4296875\n",
            "K-means silhouette score on latent space: 0.1888435 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7324\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.7089027818450206 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.6208724644793477 \n",
            "\n",
            "ROUND NUMBER  2 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.938321\n",
            "Epoch: 2/100, Loss: 0.739467\n",
            "Epoch: 3/100, Loss: 0.653837\n",
            "Epoch: 4/100, Loss: 0.613374\n",
            "Epoch: 5/100, Loss: 0.588798\n",
            "Epoch: 6/100, Loss: 0.570692\n",
            "Epoch: 7/100, Loss: 0.555874\n",
            "Epoch: 8/100, Loss: 0.542965\n",
            "Epoch: 9/100, Loss: 0.531276\n",
            "Epoch: 10/100, Loss: 0.520460\n",
            "Epoch: 11/100, Loss: 0.510327\n",
            "Epoch: 12/100, Loss: 0.500728\n",
            "Epoch: 13/100, Loss: 0.491557\n",
            "Epoch: 14/100, Loss: 0.482746\n",
            "Epoch: 15/100, Loss: 0.474248\n",
            "Epoch: 16/100, Loss: 0.466019\n",
            "Epoch: 17/100, Loss: 0.458024\n",
            "Epoch: 18/100, Loss: 0.450244\n",
            "Epoch: 19/100, Loss: 0.442655\n",
            "Epoch: 20/100, Loss: 0.435244\n",
            "Epoch: 21/100, Loss: 0.427993\n",
            "Epoch: 22/100, Loss: 0.420893\n",
            "Epoch: 23/100, Loss: 0.413941\n",
            "Epoch: 24/100, Loss: 0.407129\n",
            "Epoch: 25/100, Loss: 0.400447\n",
            "Epoch: 26/100, Loss: 0.393892\n",
            "Epoch: 27/100, Loss: 0.387454\n",
            "Epoch: 28/100, Loss: 0.381125\n",
            "Epoch: 29/100, Loss: 0.374906\n",
            "Epoch: 30/100, Loss: 0.368789\n",
            "Epoch: 31/100, Loss: 0.362770\n",
            "Epoch: 32/100, Loss: 0.356845\n",
            "Epoch: 33/100, Loss: 0.351015\n",
            "Epoch: 34/100, Loss: 0.345265\n",
            "Epoch: 35/100, Loss: 0.339602\n",
            "Epoch: 36/100, Loss: 0.334024\n",
            "Epoch: 37/100, Loss: 0.328525\n",
            "Epoch: 38/100, Loss: 0.323099\n",
            "Epoch: 39/100, Loss: 0.317753\n",
            "Epoch: 40/100, Loss: 0.312480\n",
            "Epoch: 41/100, Loss: 0.307278\n",
            "Epoch: 42/100, Loss: 0.302142\n",
            "Epoch: 43/100, Loss: 0.297070\n",
            "Epoch: 44/100, Loss: 0.292068\n",
            "Epoch: 45/100, Loss: 0.287132\n",
            "Epoch: 46/100, Loss: 0.282258\n",
            "Epoch: 47/100, Loss: 0.277444\n",
            "Epoch: 48/100, Loss: 0.272688\n",
            "Epoch: 49/100, Loss: 0.267992\n",
            "Epoch: 50/100, Loss: 0.263354\n",
            "Epoch: 51/100, Loss: 0.258773\n",
            "Epoch: 52/100, Loss: 0.254254\n",
            "Epoch: 53/100, Loss: 0.249788\n",
            "Epoch: 54/100, Loss: 0.245377\n",
            "Epoch: 55/100, Loss: 0.241022\n",
            "Epoch: 56/100, Loss: 0.236720\n",
            "Epoch: 57/100, Loss: 0.232475\n",
            "Epoch: 58/100, Loss: 0.228279\n",
            "Epoch: 59/100, Loss: 0.224134\n",
            "Epoch: 60/100, Loss: 0.220043\n",
            "Epoch: 61/100, Loss: 0.216004\n",
            "Epoch: 62/100, Loss: 0.212013\n",
            "Epoch: 63/100, Loss: 0.208075\n",
            "Epoch: 64/100, Loss: 0.204187\n",
            "Epoch: 65/100, Loss: 0.200352\n",
            "Epoch: 66/100, Loss: 0.196564\n",
            "Epoch: 67/100, Loss: 0.192825\n",
            "Epoch: 68/100, Loss: 0.189139\n",
            "Epoch: 69/100, Loss: 0.185501\n",
            "Epoch: 70/100, Loss: 0.181910\n",
            "Epoch: 71/100, Loss: 0.178369\n",
            "Epoch: 72/100, Loss: 0.174878\n",
            "Epoch: 73/100, Loss: 0.171430\n",
            "Epoch: 74/100, Loss: 0.168034\n",
            "Epoch: 75/100, Loss: 0.164682\n",
            "Epoch: 76/100, Loss: 0.161381\n",
            "Epoch: 77/100, Loss: 0.158126\n",
            "Epoch: 78/100, Loss: 0.154919\n",
            "Epoch: 79/100, Loss: 0.151762\n",
            "Epoch: 80/100, Loss: 0.148654\n",
            "Epoch: 81/100, Loss: 0.145604\n",
            "Epoch: 82/100, Loss: 0.142603\n",
            "Epoch: 83/100, Loss: 0.139641\n",
            "Epoch: 84/100, Loss: 0.136715\n",
            "Epoch: 85/100, Loss: 0.133852\n",
            "Epoch: 86/100, Loss: 0.131036\n",
            "Epoch: 87/100, Loss: 0.128187\n",
            "Epoch: 88/100, Loss: 0.125344\n",
            "Epoch: 89/100, Loss: 0.122566\n",
            "Epoch: 90/100, Loss: 0.119876\n",
            "Epoch: 91/100, Loss: 0.117254\n",
            "Epoch: 92/100, Loss: 0.114679\n",
            "Epoch: 93/100, Loss: 0.112153\n",
            "Epoch: 94/100, Loss: 0.109684\n",
            "Epoch: 95/100, Loss: 0.107264\n",
            "Epoch: 96/100, Loss: 0.104847\n",
            "Epoch: 97/100, Loss: 0.102432\n",
            "Epoch: 98/100, Loss: 0.100057\n",
            "Epoch: 99/100, Loss: 0.097721\n",
            "Epoch: 100/100, Loss: 0.095424\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6427\n",
            "Normalised mutual info score on k-means on latent space: 0.5717921652403659\n",
            "ARI score on k-means on latent space: 0.4664168139685152\n",
            "K-means cluster error on latent space: 45184.2421875\n",
            "K-means silhouette score on latent space: 0.18224166 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7368\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.7064107482128877 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.6021861616412744 \n",
            "\n",
            "ROUND NUMBER  3 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.935463\n",
            "Epoch: 2/100, Loss: 0.746942\n",
            "Epoch: 3/100, Loss: 0.667418\n",
            "Epoch: 4/100, Loss: 0.621715\n",
            "Epoch: 5/100, Loss: 0.593592\n",
            "Epoch: 6/100, Loss: 0.573552\n",
            "Epoch: 7/100, Loss: 0.557410\n",
            "Epoch: 8/100, Loss: 0.543818\n",
            "Epoch: 9/100, Loss: 0.531814\n",
            "Epoch: 10/100, Loss: 0.520842\n",
            "Epoch: 11/100, Loss: 0.510615\n",
            "Epoch: 12/100, Loss: 0.500958\n",
            "Epoch: 13/100, Loss: 0.491763\n",
            "Epoch: 14/100, Loss: 0.482945\n",
            "Epoch: 15/100, Loss: 0.474436\n",
            "Epoch: 16/100, Loss: 0.466205\n",
            "Epoch: 17/100, Loss: 0.458214\n",
            "Epoch: 18/100, Loss: 0.450437\n",
            "Epoch: 19/100, Loss: 0.442856\n",
            "Epoch: 20/100, Loss: 0.435453\n",
            "Epoch: 21/100, Loss: 0.428215\n",
            "Epoch: 22/100, Loss: 0.421129\n",
            "Epoch: 23/100, Loss: 0.414186\n",
            "Epoch: 24/100, Loss: 0.407381\n",
            "Epoch: 25/100, Loss: 0.400713\n",
            "Epoch: 26/100, Loss: 0.394170\n",
            "Epoch: 27/100, Loss: 0.387742\n",
            "Epoch: 28/100, Loss: 0.381425\n",
            "Epoch: 29/100, Loss: 0.375217\n",
            "Epoch: 30/100, Loss: 0.369113\n",
            "Epoch: 31/100, Loss: 0.363104\n",
            "Epoch: 32/100, Loss: 0.357187\n",
            "Epoch: 33/100, Loss: 0.351364\n",
            "Epoch: 34/100, Loss: 0.345627\n",
            "Epoch: 35/100, Loss: 0.339974\n",
            "Epoch: 36/100, Loss: 0.334401\n",
            "Epoch: 37/100, Loss: 0.328906\n",
            "Epoch: 38/100, Loss: 0.323488\n",
            "Epoch: 39/100, Loss: 0.318144\n",
            "Epoch: 40/100, Loss: 0.312872\n",
            "Epoch: 41/100, Loss: 0.307670\n",
            "Epoch: 42/100, Loss: 0.302539\n",
            "Epoch: 43/100, Loss: 0.297473\n",
            "Epoch: 44/100, Loss: 0.292472\n",
            "Epoch: 45/100, Loss: 0.287537\n",
            "Epoch: 46/100, Loss: 0.282663\n",
            "Epoch: 47/100, Loss: 0.277853\n",
            "Epoch: 48/100, Loss: 0.273102\n",
            "Epoch: 49/100, Loss: 0.268413\n",
            "Epoch: 50/100, Loss: 0.263780\n",
            "Epoch: 51/100, Loss: 0.259204\n",
            "Epoch: 52/100, Loss: 0.254686\n",
            "Epoch: 53/100, Loss: 0.250226\n",
            "Epoch: 54/100, Loss: 0.245816\n",
            "Epoch: 55/100, Loss: 0.241463\n",
            "Epoch: 56/100, Loss: 0.237167\n",
            "Epoch: 57/100, Loss: 0.232923\n",
            "Epoch: 58/100, Loss: 0.228733\n",
            "Epoch: 59/100, Loss: 0.224597\n",
            "Epoch: 60/100, Loss: 0.220512\n",
            "Epoch: 61/100, Loss: 0.216473\n",
            "Epoch: 62/100, Loss: 0.212489\n",
            "Epoch: 63/100, Loss: 0.208553\n",
            "Epoch: 64/100, Loss: 0.204666\n",
            "Epoch: 65/100, Loss: 0.200833\n",
            "Epoch: 66/100, Loss: 0.197047\n",
            "Epoch: 67/100, Loss: 0.193313\n",
            "Epoch: 68/100, Loss: 0.189622\n",
            "Epoch: 69/100, Loss: 0.185982\n",
            "Epoch: 70/100, Loss: 0.182392\n",
            "Epoch: 71/100, Loss: 0.178850\n",
            "Epoch: 72/100, Loss: 0.175351\n",
            "Epoch: 73/100, Loss: 0.171901\n",
            "Epoch: 74/100, Loss: 0.168498\n",
            "Epoch: 75/100, Loss: 0.165141\n",
            "Epoch: 76/100, Loss: 0.161825\n",
            "Epoch: 77/100, Loss: 0.158557\n",
            "Epoch: 78/100, Loss: 0.155336\n",
            "Epoch: 79/100, Loss: 0.152161\n",
            "Epoch: 80/100, Loss: 0.149031\n",
            "Epoch: 81/100, Loss: 0.145946\n",
            "Epoch: 82/100, Loss: 0.142907\n",
            "Epoch: 83/100, Loss: 0.139917\n",
            "Epoch: 84/100, Loss: 0.136970\n",
            "Epoch: 85/100, Loss: 0.134078\n",
            "Epoch: 86/100, Loss: 0.131229\n",
            "Epoch: 87/100, Loss: 0.128430\n",
            "Epoch: 88/100, Loss: 0.125681\n",
            "Epoch: 89/100, Loss: 0.122995\n",
            "Epoch: 90/100, Loss: 0.120367\n",
            "Epoch: 91/100, Loss: 0.117789\n",
            "Epoch: 92/100, Loss: 0.115264\n",
            "Epoch: 93/100, Loss: 0.112720\n",
            "Epoch: 94/100, Loss: 0.110163\n",
            "Epoch: 95/100, Loss: 0.107662\n",
            "Epoch: 96/100, Loss: 0.105210\n",
            "Epoch: 97/100, Loss: 0.102819\n",
            "Epoch: 98/100, Loss: 0.100476\n",
            "Epoch: 99/100, Loss: 0.098176\n",
            "Epoch: 100/100, Loss: 0.095886\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6444\n",
            "Normalised mutual info score on k-means on latent space: 0.5809167542955739\n",
            "ARI score on k-means on latent space: 0.4731614259151035\n",
            "K-means cluster error on latent space: 41696.95703125\n",
            "K-means silhouette score on latent space: 0.19722094 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6959\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6692682434393186 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5414189675597949 \n",
            "\n",
            "ROUND NUMBER  4 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.914349\n",
            "Epoch: 2/100, Loss: 0.719270\n",
            "Epoch: 3/100, Loss: 0.645197\n",
            "Epoch: 4/100, Loss: 0.609808\n",
            "Epoch: 5/100, Loss: 0.587188\n",
            "Epoch: 6/100, Loss: 0.569797\n",
            "Epoch: 7/100, Loss: 0.555301\n",
            "Epoch: 8/100, Loss: 0.542571\n",
            "Epoch: 9/100, Loss: 0.530992\n",
            "Epoch: 10/100, Loss: 0.520236\n",
            "Epoch: 11/100, Loss: 0.510118\n",
            "Epoch: 12/100, Loss: 0.500502\n",
            "Epoch: 13/100, Loss: 0.491310\n",
            "Epoch: 14/100, Loss: 0.482470\n",
            "Epoch: 15/100, Loss: 0.473944\n",
            "Epoch: 16/100, Loss: 0.465690\n",
            "Epoch: 17/100, Loss: 0.457680\n",
            "Epoch: 18/100, Loss: 0.449887\n",
            "Epoch: 19/100, Loss: 0.442293\n",
            "Epoch: 20/100, Loss: 0.434880\n",
            "Epoch: 21/100, Loss: 0.427635\n",
            "Epoch: 22/100, Loss: 0.420547\n",
            "Epoch: 23/100, Loss: 0.413606\n",
            "Epoch: 24/100, Loss: 0.406810\n",
            "Epoch: 25/100, Loss: 0.400142\n",
            "Epoch: 26/100, Loss: 0.393600\n",
            "Epoch: 27/100, Loss: 0.387175\n",
            "Epoch: 28/100, Loss: 0.380856\n",
            "Epoch: 29/100, Loss: 0.374645\n",
            "Epoch: 30/100, Loss: 0.368538\n",
            "Epoch: 31/100, Loss: 0.362527\n",
            "Epoch: 32/100, Loss: 0.356609\n",
            "Epoch: 33/100, Loss: 0.350783\n",
            "Epoch: 34/100, Loss: 0.345039\n",
            "Epoch: 35/100, Loss: 0.339379\n",
            "Epoch: 36/100, Loss: 0.333805\n",
            "Epoch: 37/100, Loss: 0.328308\n",
            "Epoch: 38/100, Loss: 0.322886\n",
            "Epoch: 39/100, Loss: 0.317533\n",
            "Epoch: 40/100, Loss: 0.312254\n",
            "Epoch: 41/100, Loss: 0.307044\n",
            "Epoch: 42/100, Loss: 0.301907\n",
            "Epoch: 43/100, Loss: 0.296836\n",
            "Epoch: 44/100, Loss: 0.291836\n",
            "Epoch: 45/100, Loss: 0.286892\n",
            "Epoch: 46/100, Loss: 0.282014\n",
            "Epoch: 47/100, Loss: 0.277197\n",
            "Epoch: 48/100, Loss: 0.272442\n",
            "Epoch: 49/100, Loss: 0.267746\n",
            "Epoch: 50/100, Loss: 0.263107\n",
            "Epoch: 51/100, Loss: 0.258524\n",
            "Epoch: 52/100, Loss: 0.254002\n",
            "Epoch: 53/100, Loss: 0.249536\n",
            "Epoch: 54/100, Loss: 0.245126\n",
            "Epoch: 55/100, Loss: 0.240771\n",
            "Epoch: 56/100, Loss: 0.236469\n",
            "Epoch: 57/100, Loss: 0.232221\n",
            "Epoch: 58/100, Loss: 0.228029\n",
            "Epoch: 59/100, Loss: 0.223887\n",
            "Epoch: 60/100, Loss: 0.219794\n",
            "Epoch: 61/100, Loss: 0.215756\n",
            "Epoch: 62/100, Loss: 0.211771\n",
            "Epoch: 63/100, Loss: 0.207839\n",
            "Epoch: 64/100, Loss: 0.203956\n",
            "Epoch: 65/100, Loss: 0.200121\n",
            "Epoch: 66/100, Loss: 0.196336\n",
            "Epoch: 67/100, Loss: 0.192598\n",
            "Epoch: 68/100, Loss: 0.188909\n",
            "Epoch: 69/100, Loss: 0.185270\n",
            "Epoch: 70/100, Loss: 0.181674\n",
            "Epoch: 71/100, Loss: 0.178127\n",
            "Epoch: 72/100, Loss: 0.174633\n",
            "Epoch: 73/100, Loss: 0.171184\n",
            "Epoch: 74/100, Loss: 0.167785\n",
            "Epoch: 75/100, Loss: 0.164433\n",
            "Epoch: 76/100, Loss: 0.161129\n",
            "Epoch: 77/100, Loss: 0.157872\n",
            "Epoch: 78/100, Loss: 0.154658\n",
            "Epoch: 79/100, Loss: 0.151498\n",
            "Epoch: 80/100, Loss: 0.148384\n",
            "Epoch: 81/100, Loss: 0.145323\n",
            "Epoch: 82/100, Loss: 0.142312\n",
            "Epoch: 83/100, Loss: 0.139341\n",
            "Epoch: 84/100, Loss: 0.136415\n",
            "Epoch: 85/100, Loss: 0.133537\n",
            "Epoch: 86/100, Loss: 0.130705\n",
            "Epoch: 87/100, Loss: 0.127922\n",
            "Epoch: 88/100, Loss: 0.125202\n",
            "Epoch: 89/100, Loss: 0.122511\n",
            "Epoch: 90/100, Loss: 0.119761\n",
            "Epoch: 91/100, Loss: 0.117036\n",
            "Epoch: 92/100, Loss: 0.114391\n",
            "Epoch: 93/100, Loss: 0.111814\n",
            "Epoch: 94/100, Loss: 0.109293\n",
            "Epoch: 95/100, Loss: 0.106827\n",
            "Epoch: 96/100, Loss: 0.104406\n",
            "Epoch: 97/100, Loss: 0.102027\n",
            "Epoch: 98/100, Loss: 0.099694\n",
            "Epoch: 99/100, Loss: 0.097410\n",
            "Epoch: 100/100, Loss: 0.095167\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6831\n",
            "Normalised mutual info score on k-means on latent space: 0.621040144815251\n",
            "ARI score on k-means on latent space: 0.5137077180513264\n",
            "K-means cluster error on latent space: 44092.84765625\n",
            "K-means silhouette score on latent space: 0.18335155 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7693\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.7393338715285889 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.6483951322270867 \n",
            "\n",
            "ROUND NUMBER  5 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.916554\n",
            "Epoch: 2/100, Loss: 0.717363\n",
            "Epoch: 3/100, Loss: 0.645410\n",
            "Epoch: 4/100, Loss: 0.608991\n",
            "Epoch: 5/100, Loss: 0.585421\n",
            "Epoch: 6/100, Loss: 0.567751\n",
            "Epoch: 7/100, Loss: 0.553193\n",
            "Epoch: 8/100, Loss: 0.540468\n",
            "Epoch: 9/100, Loss: 0.528951\n",
            "Epoch: 10/100, Loss: 0.518280\n",
            "Epoch: 11/100, Loss: 0.508263\n",
            "Epoch: 12/100, Loss: 0.498754\n",
            "Epoch: 13/100, Loss: 0.489655\n",
            "Epoch: 14/100, Loss: 0.480896\n",
            "Epoch: 15/100, Loss: 0.472435\n",
            "Epoch: 16/100, Loss: 0.464242\n",
            "Epoch: 17/100, Loss: 0.456283\n",
            "Epoch: 18/100, Loss: 0.448539\n",
            "Epoch: 19/100, Loss: 0.440984\n",
            "Epoch: 20/100, Loss: 0.433610\n",
            "Epoch: 21/100, Loss: 0.426403\n",
            "Epoch: 22/100, Loss: 0.419349\n",
            "Epoch: 23/100, Loss: 0.412444\n",
            "Epoch: 24/100, Loss: 0.405670\n",
            "Epoch: 25/100, Loss: 0.399024\n",
            "Epoch: 26/100, Loss: 0.392503\n",
            "Epoch: 27/100, Loss: 0.386100\n",
            "Epoch: 28/100, Loss: 0.379807\n",
            "Epoch: 29/100, Loss: 0.373617\n",
            "Epoch: 30/100, Loss: 0.367529\n",
            "Epoch: 31/100, Loss: 0.361539\n",
            "Epoch: 32/100, Loss: 0.355642\n",
            "Epoch: 33/100, Loss: 0.349837\n",
            "Epoch: 34/100, Loss: 0.344115\n",
            "Epoch: 35/100, Loss: 0.338479\n",
            "Epoch: 36/100, Loss: 0.332923\n",
            "Epoch: 37/100, Loss: 0.327443\n",
            "Epoch: 38/100, Loss: 0.322042\n",
            "Epoch: 39/100, Loss: 0.316713\n",
            "Epoch: 40/100, Loss: 0.311459\n",
            "Epoch: 41/100, Loss: 0.306271\n",
            "Epoch: 42/100, Loss: 0.301150\n",
            "Epoch: 43/100, Loss: 0.296097\n",
            "Epoch: 44/100, Loss: 0.291112\n",
            "Epoch: 45/100, Loss: 0.286187\n",
            "Epoch: 46/100, Loss: 0.281323\n",
            "Epoch: 47/100, Loss: 0.276523\n",
            "Epoch: 48/100, Loss: 0.271780\n",
            "Epoch: 49/100, Loss: 0.267099\n",
            "Epoch: 50/100, Loss: 0.262474\n",
            "Epoch: 51/100, Loss: 0.257906\n",
            "Epoch: 52/100, Loss: 0.253397\n",
            "Epoch: 53/100, Loss: 0.248943\n",
            "Epoch: 54/100, Loss: 0.244543\n",
            "Epoch: 55/100, Loss: 0.240197\n",
            "Epoch: 56/100, Loss: 0.235907\n",
            "Epoch: 57/100, Loss: 0.231670\n",
            "Epoch: 58/100, Loss: 0.227484\n",
            "Epoch: 59/100, Loss: 0.223351\n",
            "Epoch: 60/100, Loss: 0.219271\n",
            "Epoch: 61/100, Loss: 0.215242\n",
            "Epoch: 62/100, Loss: 0.211266\n",
            "Epoch: 63/100, Loss: 0.207340\n",
            "Epoch: 64/100, Loss: 0.203465\n",
            "Epoch: 65/100, Loss: 0.199640\n",
            "Epoch: 66/100, Loss: 0.195862\n",
            "Epoch: 67/100, Loss: 0.192131\n",
            "Epoch: 68/100, Loss: 0.188452\n",
            "Epoch: 69/100, Loss: 0.184823\n",
            "Epoch: 70/100, Loss: 0.181237\n",
            "Epoch: 71/100, Loss: 0.177702\n",
            "Epoch: 72/100, Loss: 0.174215\n",
            "Epoch: 73/100, Loss: 0.170778\n",
            "Epoch: 74/100, Loss: 0.167386\n",
            "Epoch: 75/100, Loss: 0.164047\n",
            "Epoch: 76/100, Loss: 0.160761\n",
            "Epoch: 77/100, Loss: 0.157522\n",
            "Epoch: 78/100, Loss: 0.154326\n",
            "Epoch: 79/100, Loss: 0.151177\n",
            "Epoch: 80/100, Loss: 0.148088\n",
            "Epoch: 81/100, Loss: 0.145040\n",
            "Epoch: 82/100, Loss: 0.142022\n",
            "Epoch: 83/100, Loss: 0.139025\n",
            "Epoch: 84/100, Loss: 0.136060\n",
            "Epoch: 85/100, Loss: 0.133146\n",
            "Epoch: 86/100, Loss: 0.130275\n",
            "Epoch: 87/100, Loss: 0.127457\n",
            "Epoch: 88/100, Loss: 0.124700\n",
            "Epoch: 89/100, Loss: 0.121985\n",
            "Epoch: 90/100, Loss: 0.119301\n",
            "Epoch: 91/100, Loss: 0.116647\n",
            "Epoch: 92/100, Loss: 0.114039\n",
            "Epoch: 93/100, Loss: 0.111493\n",
            "Epoch: 94/100, Loss: 0.108986\n",
            "Epoch: 95/100, Loss: 0.106514\n",
            "Epoch: 96/100, Loss: 0.104101\n",
            "Epoch: 97/100, Loss: 0.101745\n",
            "Epoch: 98/100, Loss: 0.099451\n",
            "Epoch: 99/100, Loss: 0.097215\n",
            "Epoch: 100/100, Loss: 0.095034\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.66\n",
            "Normalised mutual info score on k-means on latent space: 0.5863320655931522\n",
            "ARI score on k-means on latent space: 0.4844861021922563\n",
            "K-means cluster error on latent space: 43789.9453125\n",
            "K-means silhouette score on latent space: 0.1861782 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.664\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6463572380215106 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5212175202901346 \n",
            "\n",
            "ROUND NUMBER  6 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.899184\n",
            "Epoch: 2/100, Loss: 0.703770\n",
            "Epoch: 3/100, Loss: 0.637190\n",
            "Epoch: 4/100, Loss: 0.604309\n",
            "Epoch: 5/100, Loss: 0.582895\n",
            "Epoch: 6/100, Loss: 0.566327\n",
            "Epoch: 7/100, Loss: 0.552324\n",
            "Epoch: 8/100, Loss: 0.539911\n",
            "Epoch: 9/100, Loss: 0.528574\n",
            "Epoch: 10/100, Loss: 0.518005\n",
            "Epoch: 11/100, Loss: 0.508043\n",
            "Epoch: 12/100, Loss: 0.498562\n",
            "Epoch: 13/100, Loss: 0.489489\n",
            "Epoch: 14/100, Loss: 0.480758\n",
            "Epoch: 15/100, Loss: 0.472322\n",
            "Epoch: 16/100, Loss: 0.464154\n",
            "Epoch: 17/100, Loss: 0.456207\n",
            "Epoch: 18/100, Loss: 0.448476\n",
            "Epoch: 19/100, Loss: 0.440945\n",
            "Epoch: 20/100, Loss: 0.433589\n",
            "Epoch: 21/100, Loss: 0.426399\n",
            "Epoch: 22/100, Loss: 0.419366\n",
            "Epoch: 23/100, Loss: 0.412476\n",
            "Epoch: 24/100, Loss: 0.405720\n",
            "Epoch: 25/100, Loss: 0.399091\n",
            "Epoch: 26/100, Loss: 0.392587\n",
            "Epoch: 27/100, Loss: 0.386192\n",
            "Epoch: 28/100, Loss: 0.379906\n",
            "Epoch: 29/100, Loss: 0.373726\n",
            "Epoch: 30/100, Loss: 0.367652\n",
            "Epoch: 31/100, Loss: 0.361670\n",
            "Epoch: 32/100, Loss: 0.355780\n",
            "Epoch: 33/100, Loss: 0.349978\n",
            "Epoch: 34/100, Loss: 0.344266\n",
            "Epoch: 35/100, Loss: 0.338632\n",
            "Epoch: 36/100, Loss: 0.333082\n",
            "Epoch: 37/100, Loss: 0.327614\n",
            "Epoch: 38/100, Loss: 0.322220\n",
            "Epoch: 39/100, Loss: 0.316899\n",
            "Epoch: 40/100, Loss: 0.311645\n",
            "Epoch: 41/100, Loss: 0.306464\n",
            "Epoch: 42/100, Loss: 0.301349\n",
            "Epoch: 43/100, Loss: 0.296304\n",
            "Epoch: 44/100, Loss: 0.291320\n",
            "Epoch: 45/100, Loss: 0.286398\n",
            "Epoch: 46/100, Loss: 0.281543\n",
            "Epoch: 47/100, Loss: 0.276747\n",
            "Epoch: 48/100, Loss: 0.272015\n",
            "Epoch: 49/100, Loss: 0.267341\n",
            "Epoch: 50/100, Loss: 0.262722\n",
            "Epoch: 51/100, Loss: 0.258163\n",
            "Epoch: 52/100, Loss: 0.253661\n",
            "Epoch: 53/100, Loss: 0.249217\n",
            "Epoch: 54/100, Loss: 0.244826\n",
            "Epoch: 55/100, Loss: 0.240490\n",
            "Epoch: 56/100, Loss: 0.236205\n",
            "Epoch: 57/100, Loss: 0.231973\n",
            "Epoch: 58/100, Loss: 0.227793\n",
            "Epoch: 59/100, Loss: 0.223667\n",
            "Epoch: 60/100, Loss: 0.219594\n",
            "Epoch: 61/100, Loss: 0.215565\n",
            "Epoch: 62/100, Loss: 0.211588\n",
            "Epoch: 63/100, Loss: 0.207664\n",
            "Epoch: 64/100, Loss: 0.203790\n",
            "Epoch: 65/100, Loss: 0.199965\n",
            "Epoch: 66/100, Loss: 0.196193\n",
            "Epoch: 67/100, Loss: 0.192471\n",
            "Epoch: 68/100, Loss: 0.188794\n",
            "Epoch: 69/100, Loss: 0.185165\n",
            "Epoch: 70/100, Loss: 0.181583\n",
            "Epoch: 71/100, Loss: 0.178048\n",
            "Epoch: 72/100, Loss: 0.174565\n",
            "Epoch: 73/100, Loss: 0.171129\n",
            "Epoch: 74/100, Loss: 0.167735\n",
            "Epoch: 75/100, Loss: 0.164394\n",
            "Epoch: 76/100, Loss: 0.161102\n",
            "Epoch: 77/100, Loss: 0.157859\n",
            "Epoch: 78/100, Loss: 0.154660\n",
            "Epoch: 79/100, Loss: 0.151518\n",
            "Epoch: 80/100, Loss: 0.148417\n",
            "Epoch: 81/100, Loss: 0.145369\n",
            "Epoch: 82/100, Loss: 0.142360\n",
            "Epoch: 83/100, Loss: 0.139390\n",
            "Epoch: 84/100, Loss: 0.136468\n",
            "Epoch: 85/100, Loss: 0.133598\n",
            "Epoch: 86/100, Loss: 0.130752\n",
            "Epoch: 87/100, Loss: 0.127903\n",
            "Epoch: 88/100, Loss: 0.125081\n",
            "Epoch: 89/100, Loss: 0.122321\n",
            "Epoch: 90/100, Loss: 0.119629\n",
            "Epoch: 91/100, Loss: 0.116979\n",
            "Epoch: 92/100, Loss: 0.114371\n",
            "Epoch: 93/100, Loss: 0.111811\n",
            "Epoch: 94/100, Loss: 0.109309\n",
            "Epoch: 95/100, Loss: 0.106845\n",
            "Epoch: 96/100, Loss: 0.104423\n",
            "Epoch: 97/100, Loss: 0.102056\n",
            "Epoch: 98/100, Loss: 0.099736\n",
            "Epoch: 99/100, Loss: 0.097458\n",
            "Epoch: 100/100, Loss: 0.095207\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6524\n",
            "Normalised mutual info score on k-means on latent space: 0.5945963461144825\n",
            "ARI score on k-means on latent space: 0.48462115359469665\n",
            "K-means cluster error on latent space: 44444.87109375\n",
            "K-means silhouette score on latent space: 0.19292258 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7014\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6860559973336466 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5599057763305104 \n",
            "\n",
            "ROUND NUMBER  7 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.932213\n",
            "Epoch: 2/100, Loss: 0.731520\n",
            "Epoch: 3/100, Loss: 0.651351\n",
            "Epoch: 4/100, Loss: 0.612353\n",
            "Epoch: 5/100, Loss: 0.587959\n",
            "Epoch: 6/100, Loss: 0.569721\n",
            "Epoch: 7/100, Loss: 0.554701\n",
            "Epoch: 8/100, Loss: 0.541666\n",
            "Epoch: 9/100, Loss: 0.529943\n",
            "Epoch: 10/100, Loss: 0.519125\n",
            "Epoch: 11/100, Loss: 0.508977\n",
            "Epoch: 12/100, Loss: 0.499367\n",
            "Epoch: 13/100, Loss: 0.490188\n",
            "Epoch: 14/100, Loss: 0.481363\n",
            "Epoch: 15/100, Loss: 0.472851\n",
            "Epoch: 16/100, Loss: 0.464605\n",
            "Epoch: 17/100, Loss: 0.456601\n",
            "Epoch: 18/100, Loss: 0.448812\n",
            "Epoch: 19/100, Loss: 0.441222\n",
            "Epoch: 20/100, Loss: 0.433811\n",
            "Epoch: 21/100, Loss: 0.426569\n",
            "Epoch: 22/100, Loss: 0.419486\n",
            "Epoch: 23/100, Loss: 0.412554\n",
            "Epoch: 24/100, Loss: 0.405755\n",
            "Epoch: 25/100, Loss: 0.399089\n",
            "Epoch: 26/100, Loss: 0.392541\n",
            "Epoch: 27/100, Loss: 0.386116\n",
            "Epoch: 28/100, Loss: 0.379799\n",
            "Epoch: 29/100, Loss: 0.373590\n",
            "Epoch: 30/100, Loss: 0.367481\n",
            "Epoch: 31/100, Loss: 0.361473\n",
            "Epoch: 32/100, Loss: 0.355559\n",
            "Epoch: 33/100, Loss: 0.349739\n",
            "Epoch: 34/100, Loss: 0.344006\n",
            "Epoch: 35/100, Loss: 0.338357\n",
            "Epoch: 36/100, Loss: 0.332793\n",
            "Epoch: 37/100, Loss: 0.327306\n",
            "Epoch: 38/100, Loss: 0.321897\n",
            "Epoch: 39/100, Loss: 0.316562\n",
            "Epoch: 40/100, Loss: 0.311298\n",
            "Epoch: 41/100, Loss: 0.306102\n",
            "Epoch: 42/100, Loss: 0.300977\n",
            "Epoch: 43/100, Loss: 0.295917\n",
            "Epoch: 44/100, Loss: 0.290925\n",
            "Epoch: 45/100, Loss: 0.285990\n",
            "Epoch: 46/100, Loss: 0.281123\n",
            "Epoch: 47/100, Loss: 0.276319\n",
            "Epoch: 48/100, Loss: 0.271573\n",
            "Epoch: 49/100, Loss: 0.266889\n",
            "Epoch: 50/100, Loss: 0.262265\n",
            "Epoch: 51/100, Loss: 0.257698\n",
            "Epoch: 52/100, Loss: 0.253186\n",
            "Epoch: 53/100, Loss: 0.248730\n",
            "Epoch: 54/100, Loss: 0.244331\n",
            "Epoch: 55/100, Loss: 0.239983\n",
            "Epoch: 56/100, Loss: 0.235691\n",
            "Epoch: 57/100, Loss: 0.231449\n",
            "Epoch: 58/100, Loss: 0.227262\n",
            "Epoch: 59/100, Loss: 0.223127\n",
            "Epoch: 60/100, Loss: 0.219045\n",
            "Epoch: 61/100, Loss: 0.215016\n",
            "Epoch: 62/100, Loss: 0.211033\n",
            "Epoch: 63/100, Loss: 0.207101\n",
            "Epoch: 64/100, Loss: 0.203222\n",
            "Epoch: 65/100, Loss: 0.199391\n",
            "Epoch: 66/100, Loss: 0.195607\n",
            "Epoch: 67/100, Loss: 0.191876\n",
            "Epoch: 68/100, Loss: 0.188194\n",
            "Epoch: 69/100, Loss: 0.184559\n",
            "Epoch: 70/100, Loss: 0.180971\n",
            "Epoch: 71/100, Loss: 0.177431\n",
            "Epoch: 72/100, Loss: 0.173944\n",
            "Epoch: 73/100, Loss: 0.170501\n",
            "Epoch: 74/100, Loss: 0.167105\n",
            "Epoch: 75/100, Loss: 0.163757\n",
            "Epoch: 76/100, Loss: 0.160454\n",
            "Epoch: 77/100, Loss: 0.157197\n",
            "Epoch: 78/100, Loss: 0.153988\n",
            "Epoch: 79/100, Loss: 0.150834\n",
            "Epoch: 80/100, Loss: 0.147732\n",
            "Epoch: 81/100, Loss: 0.144690\n",
            "Epoch: 82/100, Loss: 0.141705\n",
            "Epoch: 83/100, Loss: 0.138772\n",
            "Epoch: 84/100, Loss: 0.135899\n",
            "Epoch: 85/100, Loss: 0.133060\n",
            "Epoch: 86/100, Loss: 0.130229\n",
            "Epoch: 87/100, Loss: 0.127385\n",
            "Epoch: 88/100, Loss: 0.124557\n",
            "Epoch: 89/100, Loss: 0.121781\n",
            "Epoch: 90/100, Loss: 0.119071\n",
            "Epoch: 91/100, Loss: 0.116418\n",
            "Epoch: 92/100, Loss: 0.113831\n",
            "Epoch: 93/100, Loss: 0.111302\n",
            "Epoch: 94/100, Loss: 0.108801\n",
            "Epoch: 95/100, Loss: 0.106344\n",
            "Epoch: 96/100, Loss: 0.103946\n",
            "Epoch: 97/100, Loss: 0.101582\n",
            "Epoch: 98/100, Loss: 0.099228\n",
            "Epoch: 99/100, Loss: 0.096903\n",
            "Epoch: 100/100, Loss: 0.094625\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.7096\n",
            "Normalised mutual info score on k-means on latent space: 0.626200678717853\n",
            "ARI score on k-means on latent space: 0.5401715613698879\n",
            "K-means cluster error on latent space: 45043.671875\n",
            "K-means silhouette score on latent space: 0.19465952 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7093\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6804096616585841 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5579231614791482 \n",
            "\n",
            "ROUND NUMBER  8 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.906854\n",
            "Epoch: 2/100, Loss: 0.722981\n",
            "Epoch: 3/100, Loss: 0.647821\n",
            "Epoch: 4/100, Loss: 0.610578\n",
            "Epoch: 5/100, Loss: 0.586988\n",
            "Epoch: 6/100, Loss: 0.569351\n",
            "Epoch: 7/100, Loss: 0.554882\n",
            "Epoch: 8/100, Loss: 0.542170\n",
            "Epoch: 9/100, Loss: 0.530638\n",
            "Epoch: 10/100, Loss: 0.519951\n",
            "Epoch: 11/100, Loss: 0.509910\n",
            "Epoch: 12/100, Loss: 0.500371\n",
            "Epoch: 13/100, Loss: 0.491234\n",
            "Epoch: 14/100, Loss: 0.482437\n",
            "Epoch: 15/100, Loss: 0.473939\n",
            "Epoch: 16/100, Loss: 0.465707\n",
            "Epoch: 17/100, Loss: 0.457712\n",
            "Epoch: 18/100, Loss: 0.449940\n",
            "Epoch: 19/100, Loss: 0.442352\n",
            "Epoch: 20/100, Loss: 0.434946\n",
            "Epoch: 21/100, Loss: 0.427702\n",
            "Epoch: 22/100, Loss: 0.420626\n",
            "Epoch: 23/100, Loss: 0.413694\n",
            "Epoch: 24/100, Loss: 0.406892\n",
            "Epoch: 25/100, Loss: 0.400223\n",
            "Epoch: 26/100, Loss: 0.393677\n",
            "Epoch: 27/100, Loss: 0.387246\n",
            "Epoch: 28/100, Loss: 0.380934\n",
            "Epoch: 29/100, Loss: 0.374724\n",
            "Epoch: 30/100, Loss: 0.368617\n",
            "Epoch: 31/100, Loss: 0.362605\n",
            "Epoch: 32/100, Loss: 0.356684\n",
            "Epoch: 33/100, Loss: 0.350856\n",
            "Epoch: 34/100, Loss: 0.345115\n",
            "Epoch: 35/100, Loss: 0.339461\n",
            "Epoch: 36/100, Loss: 0.333886\n",
            "Epoch: 37/100, Loss: 0.328391\n",
            "Epoch: 38/100, Loss: 0.322970\n",
            "Epoch: 39/100, Loss: 0.317625\n",
            "Epoch: 40/100, Loss: 0.312353\n",
            "Epoch: 41/100, Loss: 0.307152\n",
            "Epoch: 42/100, Loss: 0.302019\n",
            "Epoch: 43/100, Loss: 0.296957\n",
            "Epoch: 44/100, Loss: 0.291959\n",
            "Epoch: 45/100, Loss: 0.287025\n",
            "Epoch: 46/100, Loss: 0.282154\n",
            "Epoch: 47/100, Loss: 0.277346\n",
            "Epoch: 48/100, Loss: 0.272597\n",
            "Epoch: 49/100, Loss: 0.267908\n",
            "Epoch: 50/100, Loss: 0.263277\n",
            "Epoch: 51/100, Loss: 0.258708\n",
            "Epoch: 52/100, Loss: 0.254193\n",
            "Epoch: 53/100, Loss: 0.249734\n",
            "Epoch: 54/100, Loss: 0.245328\n",
            "Epoch: 55/100, Loss: 0.240981\n",
            "Epoch: 56/100, Loss: 0.236690\n",
            "Epoch: 57/100, Loss: 0.232452\n",
            "Epoch: 58/100, Loss: 0.228266\n",
            "Epoch: 59/100, Loss: 0.224135\n",
            "Epoch: 60/100, Loss: 0.220053\n",
            "Epoch: 61/100, Loss: 0.216024\n",
            "Epoch: 62/100, Loss: 0.212044\n",
            "Epoch: 63/100, Loss: 0.208115\n",
            "Epoch: 64/100, Loss: 0.204237\n",
            "Epoch: 65/100, Loss: 0.200407\n",
            "Epoch: 66/100, Loss: 0.196631\n",
            "Epoch: 67/100, Loss: 0.192900\n",
            "Epoch: 68/100, Loss: 0.189219\n",
            "Epoch: 69/100, Loss: 0.185586\n",
            "Epoch: 70/100, Loss: 0.181999\n",
            "Epoch: 71/100, Loss: 0.178459\n",
            "Epoch: 72/100, Loss: 0.174968\n",
            "Epoch: 73/100, Loss: 0.171527\n",
            "Epoch: 74/100, Loss: 0.168130\n",
            "Epoch: 75/100, Loss: 0.164785\n",
            "Epoch: 76/100, Loss: 0.161487\n",
            "Epoch: 77/100, Loss: 0.158233\n",
            "Epoch: 78/100, Loss: 0.155025\n",
            "Epoch: 79/100, Loss: 0.151870\n",
            "Epoch: 80/100, Loss: 0.148762\n",
            "Epoch: 81/100, Loss: 0.145701\n",
            "Epoch: 82/100, Loss: 0.142685\n",
            "Epoch: 83/100, Loss: 0.139715\n",
            "Epoch: 84/100, Loss: 0.136787\n",
            "Epoch: 85/100, Loss: 0.133914\n",
            "Epoch: 86/100, Loss: 0.131101\n",
            "Epoch: 87/100, Loss: 0.128309\n",
            "Epoch: 88/100, Loss: 0.125509\n",
            "Epoch: 89/100, Loss: 0.122724\n",
            "Epoch: 90/100, Loss: 0.120004\n",
            "Epoch: 91/100, Loss: 0.117341\n",
            "Epoch: 92/100, Loss: 0.114717\n",
            "Epoch: 93/100, Loss: 0.112147\n",
            "Epoch: 94/100, Loss: 0.109628\n",
            "Epoch: 95/100, Loss: 0.107147\n",
            "Epoch: 96/100, Loss: 0.104715\n",
            "Epoch: 97/100, Loss: 0.102346\n",
            "Epoch: 98/100, Loss: 0.100030\n",
            "Epoch: 99/100, Loss: 0.097756\n",
            "Epoch: 100/100, Loss: 0.095520\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6756\n",
            "Normalised mutual info score on k-means on latent space: 0.5964304449409579\n",
            "ARI score on k-means on latent space: 0.49803006047880605\n",
            "K-means cluster error on latent space: 43337.8515625\n",
            "K-means silhouette score on latent space: 0.19013128 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6892\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.721085405333127 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5729941726923696 \n",
            "\n",
            "ROUND NUMBER  9 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.922616\n",
            "Epoch: 2/100, Loss: 0.719496\n",
            "Epoch: 3/100, Loss: 0.644399\n",
            "Epoch: 4/100, Loss: 0.609485\n",
            "Epoch: 5/100, Loss: 0.586787\n",
            "Epoch: 6/100, Loss: 0.569189\n",
            "Epoch: 7/100, Loss: 0.554407\n",
            "Epoch: 8/100, Loss: 0.541496\n",
            "Epoch: 9/100, Loss: 0.529851\n",
            "Epoch: 10/100, Loss: 0.519088\n",
            "Epoch: 11/100, Loss: 0.508990\n",
            "Epoch: 12/100, Loss: 0.499410\n",
            "Epoch: 13/100, Loss: 0.490244\n",
            "Epoch: 14/100, Loss: 0.481430\n",
            "Epoch: 15/100, Loss: 0.472923\n",
            "Epoch: 16/100, Loss: 0.464683\n",
            "Epoch: 17/100, Loss: 0.456684\n",
            "Epoch: 18/100, Loss: 0.448901\n",
            "Epoch: 19/100, Loss: 0.441315\n",
            "Epoch: 20/100, Loss: 0.433911\n",
            "Epoch: 21/100, Loss: 0.426678\n",
            "Epoch: 22/100, Loss: 0.419609\n",
            "Epoch: 23/100, Loss: 0.412684\n",
            "Epoch: 24/100, Loss: 0.405902\n",
            "Epoch: 25/100, Loss: 0.399246\n",
            "Epoch: 26/100, Loss: 0.392718\n",
            "Epoch: 27/100, Loss: 0.386307\n",
            "Epoch: 28/100, Loss: 0.380006\n",
            "Epoch: 29/100, Loss: 0.373810\n",
            "Epoch: 30/100, Loss: 0.367717\n",
            "Epoch: 31/100, Loss: 0.361722\n",
            "Epoch: 32/100, Loss: 0.355817\n",
            "Epoch: 33/100, Loss: 0.350000\n",
            "Epoch: 34/100, Loss: 0.344274\n",
            "Epoch: 35/100, Loss: 0.338632\n",
            "Epoch: 36/100, Loss: 0.333075\n",
            "Epoch: 37/100, Loss: 0.327599\n",
            "Epoch: 38/100, Loss: 0.322195\n",
            "Epoch: 39/100, Loss: 0.316863\n",
            "Epoch: 40/100, Loss: 0.311603\n",
            "Epoch: 41/100, Loss: 0.306413\n",
            "Epoch: 42/100, Loss: 0.301291\n",
            "Epoch: 43/100, Loss: 0.296236\n",
            "Epoch: 44/100, Loss: 0.291248\n",
            "Epoch: 45/100, Loss: 0.286326\n",
            "Epoch: 46/100, Loss: 0.281468\n",
            "Epoch: 47/100, Loss: 0.276671\n",
            "Epoch: 48/100, Loss: 0.271936\n",
            "Epoch: 49/100, Loss: 0.267259\n",
            "Epoch: 50/100, Loss: 0.262638\n",
            "Epoch: 51/100, Loss: 0.258076\n",
            "Epoch: 52/100, Loss: 0.253570\n",
            "Epoch: 53/100, Loss: 0.249120\n",
            "Epoch: 54/100, Loss: 0.244728\n",
            "Epoch: 55/100, Loss: 0.240392\n",
            "Epoch: 56/100, Loss: 0.236107\n",
            "Epoch: 57/100, Loss: 0.231875\n",
            "Epoch: 58/100, Loss: 0.227697\n",
            "Epoch: 59/100, Loss: 0.223574\n",
            "Epoch: 60/100, Loss: 0.219499\n",
            "Epoch: 61/100, Loss: 0.215476\n",
            "Epoch: 62/100, Loss: 0.211506\n",
            "Epoch: 63/100, Loss: 0.207587\n",
            "Epoch: 64/100, Loss: 0.203716\n",
            "Epoch: 65/100, Loss: 0.199894\n",
            "Epoch: 66/100, Loss: 0.196122\n",
            "Epoch: 67/100, Loss: 0.192396\n",
            "Epoch: 68/100, Loss: 0.188721\n",
            "Epoch: 69/100, Loss: 0.185088\n",
            "Epoch: 70/100, Loss: 0.181505\n",
            "Epoch: 71/100, Loss: 0.177971\n",
            "Epoch: 72/100, Loss: 0.174481\n",
            "Epoch: 73/100, Loss: 0.171041\n",
            "Epoch: 74/100, Loss: 0.167648\n",
            "Epoch: 75/100, Loss: 0.164304\n",
            "Epoch: 76/100, Loss: 0.161004\n",
            "Epoch: 77/100, Loss: 0.157750\n",
            "Epoch: 78/100, Loss: 0.154543\n",
            "Epoch: 79/100, Loss: 0.151380\n",
            "Epoch: 80/100, Loss: 0.148261\n",
            "Epoch: 81/100, Loss: 0.145189\n",
            "Epoch: 82/100, Loss: 0.142167\n",
            "Epoch: 83/100, Loss: 0.139199\n",
            "Epoch: 84/100, Loss: 0.136282\n",
            "Epoch: 85/100, Loss: 0.133420\n",
            "Epoch: 86/100, Loss: 0.130605\n",
            "Epoch: 87/100, Loss: 0.127834\n",
            "Epoch: 88/100, Loss: 0.125082\n",
            "Epoch: 89/100, Loss: 0.122344\n",
            "Epoch: 90/100, Loss: 0.119606\n",
            "Epoch: 91/100, Loss: 0.116902\n",
            "Epoch: 92/100, Loss: 0.114269\n",
            "Epoch: 93/100, Loss: 0.111697\n",
            "Epoch: 94/100, Loss: 0.109164\n",
            "Epoch: 95/100, Loss: 0.106673\n",
            "Epoch: 96/100, Loss: 0.104229\n",
            "Epoch: 97/100, Loss: 0.101845\n",
            "Epoch: 98/100, Loss: 0.099502\n",
            "Epoch: 99/100, Loss: 0.097195\n",
            "Epoch: 100/100, Loss: 0.094910\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.7045\n",
            "Normalised mutual info score on k-means on latent space: 0.6354165136865797\n",
            "ARI score on k-means on latent space: 0.5481035449942538\n",
            "K-means cluster error on latent space: 43001.0625\n",
            "K-means silhouette score on latent space: 0.18266971 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7247\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6951380699576813 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5817341137418918 \n",
            "\n",
            "ROUND NUMBER  10 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.917426\n",
            "Epoch: 2/100, Loss: 0.727110\n",
            "Epoch: 3/100, Loss: 0.650031\n",
            "Epoch: 4/100, Loss: 0.610810\n",
            "Epoch: 5/100, Loss: 0.586426\n",
            "Epoch: 6/100, Loss: 0.568411\n",
            "Epoch: 7/100, Loss: 0.553714\n",
            "Epoch: 8/100, Loss: 0.540964\n",
            "Epoch: 9/100, Loss: 0.529464\n",
            "Epoch: 10/100, Loss: 0.518823\n",
            "Epoch: 11/100, Loss: 0.508811\n",
            "Epoch: 12/100, Loss: 0.499293\n",
            "Epoch: 13/100, Loss: 0.490183\n",
            "Epoch: 14/100, Loss: 0.481419\n",
            "Epoch: 15/100, Loss: 0.472955\n",
            "Epoch: 16/100, Loss: 0.464760\n",
            "Epoch: 17/100, Loss: 0.456806\n",
            "Epoch: 18/100, Loss: 0.449065\n",
            "Epoch: 19/100, Loss: 0.441520\n",
            "Epoch: 20/100, Loss: 0.434156\n",
            "Epoch: 21/100, Loss: 0.426961\n",
            "Epoch: 22/100, Loss: 0.419925\n",
            "Epoch: 23/100, Loss: 0.413031\n",
            "Epoch: 24/100, Loss: 0.406276\n",
            "Epoch: 25/100, Loss: 0.399650\n",
            "Epoch: 26/100, Loss: 0.393147\n",
            "Epoch: 27/100, Loss: 0.386757\n",
            "Epoch: 28/100, Loss: 0.380480\n",
            "Epoch: 29/100, Loss: 0.374305\n",
            "Epoch: 30/100, Loss: 0.368230\n",
            "Epoch: 31/100, Loss: 0.362252\n",
            "Epoch: 32/100, Loss: 0.356367\n",
            "Epoch: 33/100, Loss: 0.350568\n",
            "Epoch: 34/100, Loss: 0.344856\n",
            "Epoch: 35/100, Loss: 0.339227\n",
            "Epoch: 36/100, Loss: 0.333677\n",
            "Epoch: 37/100, Loss: 0.328205\n",
            "Epoch: 38/100, Loss: 0.322809\n",
            "Epoch: 39/100, Loss: 0.317488\n",
            "Epoch: 40/100, Loss: 0.312236\n",
            "Epoch: 41/100, Loss: 0.307052\n",
            "Epoch: 42/100, Loss: 0.301941\n",
            "Epoch: 43/100, Loss: 0.296891\n",
            "Epoch: 44/100, Loss: 0.291912\n",
            "Epoch: 45/100, Loss: 0.286998\n",
            "Epoch: 46/100, Loss: 0.282143\n",
            "Epoch: 47/100, Loss: 0.277350\n",
            "Epoch: 48/100, Loss: 0.272618\n",
            "Epoch: 49/100, Loss: 0.267941\n",
            "Epoch: 50/100, Loss: 0.263323\n",
            "Epoch: 51/100, Loss: 0.258764\n",
            "Epoch: 52/100, Loss: 0.254260\n",
            "Epoch: 53/100, Loss: 0.249812\n",
            "Epoch: 54/100, Loss: 0.245420\n",
            "Epoch: 55/100, Loss: 0.241082\n",
            "Epoch: 56/100, Loss: 0.236799\n",
            "Epoch: 57/100, Loss: 0.232567\n",
            "Epoch: 58/100, Loss: 0.228389\n",
            "Epoch: 59/100, Loss: 0.224263\n",
            "Epoch: 60/100, Loss: 0.220187\n",
            "Epoch: 61/100, Loss: 0.216160\n",
            "Epoch: 62/100, Loss: 0.212190\n",
            "Epoch: 63/100, Loss: 0.208267\n",
            "Epoch: 64/100, Loss: 0.204396\n",
            "Epoch: 65/100, Loss: 0.200571\n",
            "Epoch: 66/100, Loss: 0.196794\n",
            "Epoch: 67/100, Loss: 0.193066\n",
            "Epoch: 68/100, Loss: 0.189386\n",
            "Epoch: 69/100, Loss: 0.185754\n",
            "Epoch: 70/100, Loss: 0.182173\n",
            "Epoch: 71/100, Loss: 0.178638\n",
            "Epoch: 72/100, Loss: 0.175149\n",
            "Epoch: 73/100, Loss: 0.171705\n",
            "Epoch: 74/100, Loss: 0.168312\n",
            "Epoch: 75/100, Loss: 0.164962\n",
            "Epoch: 76/100, Loss: 0.161659\n",
            "Epoch: 77/100, Loss: 0.158403\n",
            "Epoch: 78/100, Loss: 0.155190\n",
            "Epoch: 79/100, Loss: 0.152022\n",
            "Epoch: 80/100, Loss: 0.148900\n",
            "Epoch: 81/100, Loss: 0.145822\n",
            "Epoch: 82/100, Loss: 0.142789\n",
            "Epoch: 83/100, Loss: 0.139804\n",
            "Epoch: 84/100, Loss: 0.136867\n",
            "Epoch: 85/100, Loss: 0.133979\n",
            "Epoch: 86/100, Loss: 0.131143\n",
            "Epoch: 87/100, Loss: 0.128364\n",
            "Epoch: 88/100, Loss: 0.125644\n",
            "Epoch: 89/100, Loss: 0.122994\n",
            "Epoch: 90/100, Loss: 0.120400\n",
            "Epoch: 91/100, Loss: 0.117791\n",
            "Epoch: 92/100, Loss: 0.115143\n",
            "Epoch: 93/100, Loss: 0.112516\n",
            "Epoch: 94/100, Loss: 0.109942\n",
            "Epoch: 95/100, Loss: 0.107431\n",
            "Epoch: 96/100, Loss: 0.104996\n",
            "Epoch: 97/100, Loss: 0.102611\n",
            "Epoch: 98/100, Loss: 0.100262\n",
            "Epoch: 99/100, Loss: 0.097953\n",
            "Epoch: 100/100, Loss: 0.095678\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6575\n",
            "Normalised mutual info score on k-means on latent space: 0.587147201145989\n",
            "ARI score on k-means on latent space: 0.4819820174720019\n",
            "K-means cluster error on latent space: 43826.79296875\n",
            "K-means silhouette score on latent space: 0.19934821 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6943\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6653333736539565 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5349786068097897 \n",
            "\n",
            "Average k-means accuracy score at latent space: 0.6694800000000001 \n",
            "\n",
            "Average k-means NMI score at latent space: 0.5996060645584285 \n",
            "\n",
            "Average k-means ARI score at latent space: 0.4986800885381267 \n",
            "\n",
            "Average agglomerative clustering accuracy score at latent space: 0.71173 \n",
            "\n",
            "Average agglomerative clustering NMI score at latent space: 0.6918295390984321 \n",
            "\n",
            "Average agglomerative clustering ARI score at latent space: 0.5741626077251347 \n",
            "\n",
            "Average k-means silhouette score on latent space: 0.18975671529769897 \n",
            "\n",
            "Average k-means cluster error on latent space: 43701.2671875 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_for_k_70 = run_experiment(70, dataloader, 784, datapoints = images, datapoints_reshaped = images_reshaped, labels = labels, epochs=100, hl1_neurons=32, hl2_neurons=64, hl3_neurons=128, latent_dimension=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAi9YbAH7yj-",
        "outputId": "db6af579-fbc0-4cae-c1db-a528e8e9f1fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment results for k-means with k = 70 clusters:\n",
            "\n",
            "Running k-means algorithm in order to get our pseudolabels: \n",
            "\n",
            "K_means greedy accuracy score (initial space): 0.8354\n",
            "Normalised mutual info score (initial space): 0.7232658655637275\n",
            "ARI (initial space): 0.6894906370911381 \n",
            "\n",
            "Using the autoencoder model on our data: \n",
            "\n",
            "ROUND NUMBER  1 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.957514\n",
            "Epoch: 2/100, Loss: 0.747950\n",
            "Epoch: 3/100, Loss: 0.661419\n",
            "Epoch: 4/100, Loss: 0.617895\n",
            "Epoch: 5/100, Loss: 0.591756\n",
            "Epoch: 6/100, Loss: 0.573061\n",
            "Epoch: 7/100, Loss: 0.557853\n",
            "Epoch: 8/100, Loss: 0.544593\n",
            "Epoch: 9/100, Loss: 0.532611\n",
            "Epoch: 10/100, Loss: 0.521542\n",
            "Epoch: 11/100, Loss: 0.511191\n",
            "Epoch: 12/100, Loss: 0.501411\n",
            "Epoch: 13/100, Loss: 0.492092\n",
            "Epoch: 14/100, Loss: 0.483168\n",
            "Epoch: 15/100, Loss: 0.474573\n",
            "Epoch: 16/100, Loss: 0.466263\n",
            "Epoch: 17/100, Loss: 0.458199\n",
            "Epoch: 18/100, Loss: 0.450361\n",
            "Epoch: 19/100, Loss: 0.442728\n",
            "Epoch: 20/100, Loss: 0.435281\n",
            "Epoch: 21/100, Loss: 0.428001\n",
            "Epoch: 22/100, Loss: 0.420881\n",
            "Epoch: 23/100, Loss: 0.413908\n",
            "Epoch: 24/100, Loss: 0.407081\n",
            "Epoch: 25/100, Loss: 0.400383\n",
            "Epoch: 26/100, Loss: 0.393810\n",
            "Epoch: 27/100, Loss: 0.387360\n",
            "Epoch: 28/100, Loss: 0.381021\n",
            "Epoch: 29/100, Loss: 0.374788\n",
            "Epoch: 30/100, Loss: 0.368662\n",
            "Epoch: 31/100, Loss: 0.362634\n",
            "Epoch: 32/100, Loss: 0.356706\n",
            "Epoch: 33/100, Loss: 0.350871\n",
            "Epoch: 34/100, Loss: 0.345119\n",
            "Epoch: 35/100, Loss: 0.339452\n",
            "Epoch: 36/100, Loss: 0.333870\n",
            "Epoch: 37/100, Loss: 0.328370\n",
            "Epoch: 38/100, Loss: 0.322946\n",
            "Epoch: 39/100, Loss: 0.317599\n",
            "Epoch: 40/100, Loss: 0.312318\n",
            "Epoch: 41/100, Loss: 0.307111\n",
            "Epoch: 42/100, Loss: 0.301969\n",
            "Epoch: 43/100, Loss: 0.296898\n",
            "Epoch: 44/100, Loss: 0.291891\n",
            "Epoch: 45/100, Loss: 0.286952\n",
            "Epoch: 46/100, Loss: 0.282073\n",
            "Epoch: 47/100, Loss: 0.277257\n",
            "Epoch: 48/100, Loss: 0.272502\n",
            "Epoch: 49/100, Loss: 0.267807\n",
            "Epoch: 50/100, Loss: 0.263168\n",
            "Epoch: 51/100, Loss: 0.258591\n",
            "Epoch: 52/100, Loss: 0.254070\n",
            "Epoch: 53/100, Loss: 0.249606\n",
            "Epoch: 54/100, Loss: 0.245195\n",
            "Epoch: 55/100, Loss: 0.240844\n",
            "Epoch: 56/100, Loss: 0.236544\n",
            "Epoch: 57/100, Loss: 0.232301\n",
            "Epoch: 58/100, Loss: 0.228109\n",
            "Epoch: 59/100, Loss: 0.223967\n",
            "Epoch: 60/100, Loss: 0.219880\n",
            "Epoch: 61/100, Loss: 0.215842\n",
            "Epoch: 62/100, Loss: 0.211858\n",
            "Epoch: 63/100, Loss: 0.207924\n",
            "Epoch: 64/100, Loss: 0.204042\n",
            "Epoch: 65/100, Loss: 0.200208\n",
            "Epoch: 66/100, Loss: 0.196420\n",
            "Epoch: 67/100, Loss: 0.192682\n",
            "Epoch: 68/100, Loss: 0.188992\n",
            "Epoch: 69/100, Loss: 0.185353\n",
            "Epoch: 70/100, Loss: 0.181762\n",
            "Epoch: 71/100, Loss: 0.178220\n",
            "Epoch: 72/100, Loss: 0.174725\n",
            "Epoch: 73/100, Loss: 0.171278\n",
            "Epoch: 74/100, Loss: 0.167877\n",
            "Epoch: 75/100, Loss: 0.164524\n",
            "Epoch: 76/100, Loss: 0.161218\n",
            "Epoch: 77/100, Loss: 0.157956\n",
            "Epoch: 78/100, Loss: 0.154743\n",
            "Epoch: 79/100, Loss: 0.151575\n",
            "Epoch: 80/100, Loss: 0.148462\n",
            "Epoch: 81/100, Loss: 0.145399\n",
            "Epoch: 82/100, Loss: 0.142382\n",
            "Epoch: 83/100, Loss: 0.139413\n",
            "Epoch: 84/100, Loss: 0.136496\n",
            "Epoch: 85/100, Loss: 0.133635\n",
            "Epoch: 86/100, Loss: 0.130815\n",
            "Epoch: 87/100, Loss: 0.127994\n",
            "Epoch: 88/100, Loss: 0.125202\n",
            "Epoch: 89/100, Loss: 0.122464\n",
            "Epoch: 90/100, Loss: 0.119810\n",
            "Epoch: 91/100, Loss: 0.117224\n",
            "Epoch: 92/100, Loss: 0.114711\n",
            "Epoch: 93/100, Loss: 0.112242\n",
            "Epoch: 94/100, Loss: 0.109770\n",
            "Epoch: 95/100, Loss: 0.107280\n",
            "Epoch: 96/100, Loss: 0.104806\n",
            "Epoch: 97/100, Loss: 0.102354\n",
            "Epoch: 98/100, Loss: 0.099934\n",
            "Epoch: 99/100, Loss: 0.097555\n",
            "Epoch: 100/100, Loss: 0.095215\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6801\n",
            "Normalised mutual info score on k-means on latent space: 0.612472822732192\n",
            "ARI score on k-means on latent space: 0.5103056872765634\n",
            "K-means cluster error on latent space: 44477.74609375\n",
            "K-means silhouette score on latent space: 0.19628604 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7791\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.7349437876354935 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.6522048669214319 \n",
            "\n",
            "ROUND NUMBER  2 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.897542\n",
            "Epoch: 2/100, Loss: 0.704395\n",
            "Epoch: 3/100, Loss: 0.637554\n",
            "Epoch: 4/100, Loss: 0.604638\n",
            "Epoch: 5/100, Loss: 0.583141\n",
            "Epoch: 6/100, Loss: 0.566600\n",
            "Epoch: 7/100, Loss: 0.552655\n",
            "Epoch: 8/100, Loss: 0.540292\n",
            "Epoch: 9/100, Loss: 0.529011\n",
            "Epoch: 10/100, Loss: 0.518506\n",
            "Epoch: 11/100, Loss: 0.508587\n",
            "Epoch: 12/100, Loss: 0.499139\n",
            "Epoch: 13/100, Loss: 0.490081\n",
            "Epoch: 14/100, Loss: 0.481351\n",
            "Epoch: 15/100, Loss: 0.472913\n",
            "Epoch: 16/100, Loss: 0.464735\n",
            "Epoch: 17/100, Loss: 0.456778\n",
            "Epoch: 18/100, Loss: 0.449029\n",
            "Epoch: 19/100, Loss: 0.441473\n",
            "Epoch: 20/100, Loss: 0.434087\n",
            "Epoch: 21/100, Loss: 0.426867\n",
            "Epoch: 22/100, Loss: 0.419804\n",
            "Epoch: 23/100, Loss: 0.412881\n",
            "Epoch: 24/100, Loss: 0.406097\n",
            "Epoch: 25/100, Loss: 0.399447\n",
            "Epoch: 26/100, Loss: 0.392917\n",
            "Epoch: 27/100, Loss: 0.386500\n",
            "Epoch: 28/100, Loss: 0.380195\n",
            "Epoch: 29/100, Loss: 0.373997\n",
            "Epoch: 30/100, Loss: 0.367902\n",
            "Epoch: 31/100, Loss: 0.361903\n",
            "Epoch: 32/100, Loss: 0.356000\n",
            "Epoch: 33/100, Loss: 0.350187\n",
            "Epoch: 34/100, Loss: 0.344462\n",
            "Epoch: 35/100, Loss: 0.338820\n",
            "Epoch: 36/100, Loss: 0.333257\n",
            "Epoch: 37/100, Loss: 0.327775\n",
            "Epoch: 38/100, Loss: 0.322371\n",
            "Epoch: 39/100, Loss: 0.317041\n",
            "Epoch: 40/100, Loss: 0.311780\n",
            "Epoch: 41/100, Loss: 0.306591\n",
            "Epoch: 42/100, Loss: 0.301470\n",
            "Epoch: 43/100, Loss: 0.296417\n",
            "Epoch: 44/100, Loss: 0.291429\n",
            "Epoch: 45/100, Loss: 0.286503\n",
            "Epoch: 46/100, Loss: 0.281640\n",
            "Epoch: 47/100, Loss: 0.276842\n",
            "Epoch: 48/100, Loss: 0.272106\n",
            "Epoch: 49/100, Loss: 0.267425\n",
            "Epoch: 50/100, Loss: 0.262803\n",
            "Epoch: 51/100, Loss: 0.258239\n",
            "Epoch: 52/100, Loss: 0.253732\n",
            "Epoch: 53/100, Loss: 0.249278\n",
            "Epoch: 54/100, Loss: 0.244882\n",
            "Epoch: 55/100, Loss: 0.240539\n",
            "Epoch: 56/100, Loss: 0.236251\n",
            "Epoch: 57/100, Loss: 0.232015\n",
            "Epoch: 58/100, Loss: 0.227836\n",
            "Epoch: 59/100, Loss: 0.223702\n",
            "Epoch: 60/100, Loss: 0.219623\n",
            "Epoch: 61/100, Loss: 0.215593\n",
            "Epoch: 62/100, Loss: 0.211617\n",
            "Epoch: 63/100, Loss: 0.207696\n",
            "Epoch: 64/100, Loss: 0.203822\n",
            "Epoch: 65/100, Loss: 0.199998\n",
            "Epoch: 66/100, Loss: 0.196220\n",
            "Epoch: 67/100, Loss: 0.192493\n",
            "Epoch: 68/100, Loss: 0.188812\n",
            "Epoch: 69/100, Loss: 0.185180\n",
            "Epoch: 70/100, Loss: 0.181596\n",
            "Epoch: 71/100, Loss: 0.178058\n",
            "Epoch: 72/100, Loss: 0.174569\n",
            "Epoch: 73/100, Loss: 0.171124\n",
            "Epoch: 74/100, Loss: 0.167728\n",
            "Epoch: 75/100, Loss: 0.164375\n",
            "Epoch: 76/100, Loss: 0.161072\n",
            "Epoch: 77/100, Loss: 0.157812\n",
            "Epoch: 78/100, Loss: 0.154602\n",
            "Epoch: 79/100, Loss: 0.151436\n",
            "Epoch: 80/100, Loss: 0.148317\n",
            "Epoch: 81/100, Loss: 0.145240\n",
            "Epoch: 82/100, Loss: 0.142209\n",
            "Epoch: 83/100, Loss: 0.139232\n",
            "Epoch: 84/100, Loss: 0.136303\n",
            "Epoch: 85/100, Loss: 0.133422\n",
            "Epoch: 86/100, Loss: 0.130589\n",
            "Epoch: 87/100, Loss: 0.127803\n",
            "Epoch: 88/100, Loss: 0.125077\n",
            "Epoch: 89/100, Loss: 0.122399\n",
            "Epoch: 90/100, Loss: 0.119739\n",
            "Epoch: 91/100, Loss: 0.117109\n",
            "Epoch: 92/100, Loss: 0.114511\n",
            "Epoch: 93/100, Loss: 0.111927\n",
            "Epoch: 94/100, Loss: 0.109381\n",
            "Epoch: 95/100, Loss: 0.106889\n",
            "Epoch: 96/100, Loss: 0.104444\n",
            "Epoch: 97/100, Loss: 0.102048\n",
            "Epoch: 98/100, Loss: 0.099702\n",
            "Epoch: 99/100, Loss: 0.097388\n",
            "Epoch: 100/100, Loss: 0.095120\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6404\n",
            "Normalised mutual info score on k-means on latent space: 0.5608963338222202\n",
            "ARI score on k-means on latent space: 0.45699164060799624\n",
            "K-means cluster error on latent space: 42610.83984375\n",
            "K-means silhouette score on latent space: 0.17924476 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6964\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6952943077609214 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.575819490422705 \n",
            "\n",
            "ROUND NUMBER  3 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.944857\n",
            "Epoch: 2/100, Loss: 0.748303\n",
            "Epoch: 3/100, Loss: 0.660497\n",
            "Epoch: 4/100, Loss: 0.615460\n",
            "Epoch: 5/100, Loss: 0.588509\n",
            "Epoch: 6/100, Loss: 0.569492\n",
            "Epoch: 7/100, Loss: 0.554590\n",
            "Epoch: 8/100, Loss: 0.541762\n",
            "Epoch: 9/100, Loss: 0.530217\n",
            "Epoch: 10/100, Loss: 0.519545\n",
            "Epoch: 11/100, Loss: 0.509514\n",
            "Epoch: 12/100, Loss: 0.499983\n",
            "Epoch: 13/100, Loss: 0.490859\n",
            "Epoch: 14/100, Loss: 0.482082\n",
            "Epoch: 15/100, Loss: 0.473596\n",
            "Epoch: 16/100, Loss: 0.465376\n",
            "Epoch: 17/100, Loss: 0.457381\n",
            "Epoch: 18/100, Loss: 0.449602\n",
            "Epoch: 19/100, Loss: 0.442014\n",
            "Epoch: 20/100, Loss: 0.434607\n",
            "Epoch: 21/100, Loss: 0.427364\n",
            "Epoch: 22/100, Loss: 0.420275\n",
            "Epoch: 23/100, Loss: 0.413335\n",
            "Epoch: 24/100, Loss: 0.406532\n",
            "Epoch: 25/100, Loss: 0.399858\n",
            "Epoch: 26/100, Loss: 0.393316\n",
            "Epoch: 27/100, Loss: 0.386888\n",
            "Epoch: 28/100, Loss: 0.380574\n",
            "Epoch: 29/100, Loss: 0.374364\n",
            "Epoch: 30/100, Loss: 0.368259\n",
            "Epoch: 31/100, Loss: 0.362249\n",
            "Epoch: 32/100, Loss: 0.356336\n",
            "Epoch: 33/100, Loss: 0.350514\n",
            "Epoch: 34/100, Loss: 0.344782\n",
            "Epoch: 35/100, Loss: 0.339133\n",
            "Epoch: 36/100, Loss: 0.333559\n",
            "Epoch: 37/100, Loss: 0.328070\n",
            "Epoch: 38/100, Loss: 0.322653\n",
            "Epoch: 39/100, Loss: 0.317312\n",
            "Epoch: 40/100, Loss: 0.312044\n",
            "Epoch: 41/100, Loss: 0.306848\n",
            "Epoch: 42/100, Loss: 0.301723\n",
            "Epoch: 43/100, Loss: 0.296663\n",
            "Epoch: 44/100, Loss: 0.291670\n",
            "Epoch: 45/100, Loss: 0.286740\n",
            "Epoch: 46/100, Loss: 0.281873\n",
            "Epoch: 47/100, Loss: 0.277065\n",
            "Epoch: 48/100, Loss: 0.272318\n",
            "Epoch: 49/100, Loss: 0.267629\n",
            "Epoch: 50/100, Loss: 0.263000\n",
            "Epoch: 51/100, Loss: 0.258426\n",
            "Epoch: 52/100, Loss: 0.253912\n",
            "Epoch: 53/100, Loss: 0.249454\n",
            "Epoch: 54/100, Loss: 0.245047\n",
            "Epoch: 55/100, Loss: 0.240698\n",
            "Epoch: 56/100, Loss: 0.236400\n",
            "Epoch: 57/100, Loss: 0.232156\n",
            "Epoch: 58/100, Loss: 0.227969\n",
            "Epoch: 59/100, Loss: 0.223834\n",
            "Epoch: 60/100, Loss: 0.219748\n",
            "Epoch: 61/100, Loss: 0.215716\n",
            "Epoch: 62/100, Loss: 0.211733\n",
            "Epoch: 63/100, Loss: 0.207798\n",
            "Epoch: 64/100, Loss: 0.203914\n",
            "Epoch: 65/100, Loss: 0.200080\n",
            "Epoch: 66/100, Loss: 0.196299\n",
            "Epoch: 67/100, Loss: 0.192566\n",
            "Epoch: 68/100, Loss: 0.188879\n",
            "Epoch: 69/100, Loss: 0.185242\n",
            "Epoch: 70/100, Loss: 0.181654\n",
            "Epoch: 71/100, Loss: 0.178112\n",
            "Epoch: 72/100, Loss: 0.174616\n",
            "Epoch: 73/100, Loss: 0.171168\n",
            "Epoch: 74/100, Loss: 0.167765\n",
            "Epoch: 75/100, Loss: 0.164410\n",
            "Epoch: 76/100, Loss: 0.161099\n",
            "Epoch: 77/100, Loss: 0.157839\n",
            "Epoch: 78/100, Loss: 0.154623\n",
            "Epoch: 79/100, Loss: 0.151454\n",
            "Epoch: 80/100, Loss: 0.148328\n",
            "Epoch: 81/100, Loss: 0.145251\n",
            "Epoch: 82/100, Loss: 0.142222\n",
            "Epoch: 83/100, Loss: 0.139242\n",
            "Epoch: 84/100, Loss: 0.136308\n",
            "Epoch: 85/100, Loss: 0.133431\n",
            "Epoch: 86/100, Loss: 0.130603\n",
            "Epoch: 87/100, Loss: 0.127814\n",
            "Epoch: 88/100, Loss: 0.125067\n",
            "Epoch: 89/100, Loss: 0.122369\n",
            "Epoch: 90/100, Loss: 0.119727\n",
            "Epoch: 91/100, Loss: 0.117121\n",
            "Epoch: 92/100, Loss: 0.114551\n",
            "Epoch: 93/100, Loss: 0.111980\n",
            "Epoch: 94/100, Loss: 0.109430\n",
            "Epoch: 95/100, Loss: 0.106930\n",
            "Epoch: 96/100, Loss: 0.104492\n",
            "Epoch: 97/100, Loss: 0.102112\n",
            "Epoch: 98/100, Loss: 0.099786\n",
            "Epoch: 99/100, Loss: 0.097498\n",
            "Epoch: 100/100, Loss: 0.095279\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6459\n",
            "Normalised mutual info score on k-means on latent space: 0.5897460156877734\n",
            "ARI score on k-means on latent space: 0.4756568335115929\n",
            "K-means cluster error on latent space: 44006.1171875\n",
            "K-means silhouette score on latent space: 0.1896617 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.5915\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6256861938488761 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.4603753943781476 \n",
            "\n",
            "ROUND NUMBER  4 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.932797\n",
            "Epoch: 2/100, Loss: 0.730206\n",
            "Epoch: 3/100, Loss: 0.649977\n",
            "Epoch: 4/100, Loss: 0.611598\n",
            "Epoch: 5/100, Loss: 0.587400\n",
            "Epoch: 6/100, Loss: 0.569474\n",
            "Epoch: 7/100, Loss: 0.554719\n",
            "Epoch: 8/100, Loss: 0.541861\n",
            "Epoch: 9/100, Loss: 0.530248\n",
            "Epoch: 10/100, Loss: 0.519519\n",
            "Epoch: 11/100, Loss: 0.509447\n",
            "Epoch: 12/100, Loss: 0.499893\n",
            "Epoch: 13/100, Loss: 0.490762\n",
            "Epoch: 14/100, Loss: 0.481977\n",
            "Epoch: 15/100, Loss: 0.473488\n",
            "Epoch: 16/100, Loss: 0.465269\n",
            "Epoch: 17/100, Loss: 0.457277\n",
            "Epoch: 18/100, Loss: 0.449507\n",
            "Epoch: 19/100, Loss: 0.441934\n",
            "Epoch: 20/100, Loss: 0.434544\n",
            "Epoch: 21/100, Loss: 0.427320\n",
            "Epoch: 22/100, Loss: 0.420255\n",
            "Epoch: 23/100, Loss: 0.413338\n",
            "Epoch: 24/100, Loss: 0.406559\n",
            "Epoch: 25/100, Loss: 0.399918\n",
            "Epoch: 26/100, Loss: 0.393397\n",
            "Epoch: 27/100, Loss: 0.386996\n",
            "Epoch: 28/100, Loss: 0.380705\n",
            "Epoch: 29/100, Loss: 0.374514\n",
            "Epoch: 30/100, Loss: 0.368432\n",
            "Epoch: 31/100, Loss: 0.362443\n",
            "Epoch: 32/100, Loss: 0.356550\n",
            "Epoch: 33/100, Loss: 0.350744\n",
            "Epoch: 34/100, Loss: 0.345029\n",
            "Epoch: 35/100, Loss: 0.339398\n",
            "Epoch: 36/100, Loss: 0.333849\n",
            "Epoch: 37/100, Loss: 0.328370\n",
            "Epoch: 38/100, Loss: 0.322971\n",
            "Epoch: 39/100, Loss: 0.317644\n",
            "Epoch: 40/100, Loss: 0.312388\n",
            "Epoch: 41/100, Loss: 0.307200\n",
            "Epoch: 42/100, Loss: 0.302081\n",
            "Epoch: 43/100, Loss: 0.297029\n",
            "Epoch: 44/100, Loss: 0.292043\n",
            "Epoch: 45/100, Loss: 0.287120\n",
            "Epoch: 46/100, Loss: 0.282258\n",
            "Epoch: 47/100, Loss: 0.277457\n",
            "Epoch: 48/100, Loss: 0.272716\n",
            "Epoch: 49/100, Loss: 0.268033\n",
            "Epoch: 50/100, Loss: 0.263410\n",
            "Epoch: 51/100, Loss: 0.258844\n",
            "Epoch: 52/100, Loss: 0.254331\n",
            "Epoch: 53/100, Loss: 0.249876\n",
            "Epoch: 54/100, Loss: 0.245477\n",
            "Epoch: 55/100, Loss: 0.241133\n",
            "Epoch: 56/100, Loss: 0.236840\n",
            "Epoch: 57/100, Loss: 0.232602\n",
            "Epoch: 58/100, Loss: 0.228415\n",
            "Epoch: 59/100, Loss: 0.224284\n",
            "Epoch: 60/100, Loss: 0.220205\n",
            "Epoch: 61/100, Loss: 0.216178\n",
            "Epoch: 62/100, Loss: 0.212198\n",
            "Epoch: 63/100, Loss: 0.208265\n",
            "Epoch: 64/100, Loss: 0.204384\n",
            "Epoch: 65/100, Loss: 0.200554\n",
            "Epoch: 66/100, Loss: 0.196772\n",
            "Epoch: 67/100, Loss: 0.193038\n",
            "Epoch: 68/100, Loss: 0.189351\n",
            "Epoch: 69/100, Loss: 0.185710\n",
            "Epoch: 70/100, Loss: 0.182119\n",
            "Epoch: 71/100, Loss: 0.178576\n",
            "Epoch: 72/100, Loss: 0.175080\n",
            "Epoch: 73/100, Loss: 0.171635\n",
            "Epoch: 74/100, Loss: 0.168240\n",
            "Epoch: 75/100, Loss: 0.164889\n",
            "Epoch: 76/100, Loss: 0.161585\n",
            "Epoch: 77/100, Loss: 0.158329\n",
            "Epoch: 78/100, Loss: 0.155120\n",
            "Epoch: 79/100, Loss: 0.151955\n",
            "Epoch: 80/100, Loss: 0.148839\n",
            "Epoch: 81/100, Loss: 0.145768\n",
            "Epoch: 82/100, Loss: 0.142751\n",
            "Epoch: 83/100, Loss: 0.139779\n",
            "Epoch: 84/100, Loss: 0.136844\n",
            "Epoch: 85/100, Loss: 0.133959\n",
            "Epoch: 86/100, Loss: 0.131117\n",
            "Epoch: 87/100, Loss: 0.128307\n",
            "Epoch: 88/100, Loss: 0.125529\n",
            "Epoch: 89/100, Loss: 0.122792\n",
            "Epoch: 90/100, Loss: 0.120106\n",
            "Epoch: 91/100, Loss: 0.117462\n",
            "Epoch: 92/100, Loss: 0.114855\n",
            "Epoch: 93/100, Loss: 0.112306\n",
            "Epoch: 94/100, Loss: 0.109823\n",
            "Epoch: 95/100, Loss: 0.107403\n",
            "Epoch: 96/100, Loss: 0.105038\n",
            "Epoch: 97/100, Loss: 0.102723\n",
            "Epoch: 98/100, Loss: 0.100462\n",
            "Epoch: 99/100, Loss: 0.098209\n",
            "Epoch: 100/100, Loss: 0.095906\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.7037\n",
            "Normalised mutual info score on k-means on latent space: 0.6253945543947156\n",
            "ARI score on k-means on latent space: 0.5287726519537281\n",
            "K-means cluster error on latent space: 43051.37109375\n",
            "K-means silhouette score on latent space: 0.1670457 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7778\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.7335133960774738 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.6528561365900469 \n",
            "\n",
            "ROUND NUMBER  5 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.952909\n",
            "Epoch: 2/100, Loss: 0.753575\n",
            "Epoch: 3/100, Loss: 0.664985\n",
            "Epoch: 4/100, Loss: 0.621640\n",
            "Epoch: 5/100, Loss: 0.594999\n",
            "Epoch: 6/100, Loss: 0.575156\n",
            "Epoch: 7/100, Loss: 0.559243\n",
            "Epoch: 8/100, Loss: 0.545650\n",
            "Epoch: 9/100, Loss: 0.533593\n",
            "Epoch: 10/100, Loss: 0.522577\n",
            "Epoch: 11/100, Loss: 0.512295\n",
            "Epoch: 12/100, Loss: 0.502575\n",
            "Epoch: 13/100, Loss: 0.493311\n",
            "Epoch: 14/100, Loss: 0.484423\n",
            "Epoch: 15/100, Loss: 0.475846\n",
            "Epoch: 16/100, Loss: 0.467538\n",
            "Epoch: 17/100, Loss: 0.459468\n",
            "Epoch: 18/100, Loss: 0.451617\n",
            "Epoch: 19/100, Loss: 0.443966\n",
            "Epoch: 20/100, Loss: 0.436510\n",
            "Epoch: 21/100, Loss: 0.429227\n",
            "Epoch: 22/100, Loss: 0.422108\n",
            "Epoch: 23/100, Loss: 0.415140\n",
            "Epoch: 24/100, Loss: 0.408310\n",
            "Epoch: 25/100, Loss: 0.401617\n",
            "Epoch: 26/100, Loss: 0.395048\n",
            "Epoch: 27/100, Loss: 0.388595\n",
            "Epoch: 28/100, Loss: 0.382259\n",
            "Epoch: 29/100, Loss: 0.376031\n",
            "Epoch: 30/100, Loss: 0.369909\n",
            "Epoch: 31/100, Loss: 0.363889\n",
            "Epoch: 32/100, Loss: 0.357963\n",
            "Epoch: 33/100, Loss: 0.352122\n",
            "Epoch: 34/100, Loss: 0.346368\n",
            "Epoch: 35/100, Loss: 0.340703\n",
            "Epoch: 36/100, Loss: 0.335117\n",
            "Epoch: 37/100, Loss: 0.329611\n",
            "Epoch: 38/100, Loss: 0.324182\n",
            "Epoch: 39/100, Loss: 0.318828\n",
            "Epoch: 40/100, Loss: 0.313546\n",
            "Epoch: 41/100, Loss: 0.308333\n",
            "Epoch: 42/100, Loss: 0.303190\n",
            "Epoch: 43/100, Loss: 0.298110\n",
            "Epoch: 44/100, Loss: 0.293093\n",
            "Epoch: 45/100, Loss: 0.288146\n",
            "Epoch: 46/100, Loss: 0.283260\n",
            "Epoch: 47/100, Loss: 0.278436\n",
            "Epoch: 48/100, Loss: 0.273671\n",
            "Epoch: 49/100, Loss: 0.268968\n",
            "Epoch: 50/100, Loss: 0.264322\n",
            "Epoch: 51/100, Loss: 0.259733\n",
            "Epoch: 52/100, Loss: 0.255201\n",
            "Epoch: 53/100, Loss: 0.250725\n",
            "Epoch: 54/100, Loss: 0.246305\n",
            "Epoch: 55/100, Loss: 0.241943\n",
            "Epoch: 56/100, Loss: 0.237632\n",
            "Epoch: 57/100, Loss: 0.233378\n",
            "Epoch: 58/100, Loss: 0.229176\n",
            "Epoch: 59/100, Loss: 0.225022\n",
            "Epoch: 60/100, Loss: 0.220924\n",
            "Epoch: 61/100, Loss: 0.216878\n",
            "Epoch: 62/100, Loss: 0.212880\n",
            "Epoch: 63/100, Loss: 0.208930\n",
            "Epoch: 64/100, Loss: 0.205033\n",
            "Epoch: 65/100, Loss: 0.201187\n",
            "Epoch: 66/100, Loss: 0.197392\n",
            "Epoch: 67/100, Loss: 0.193646\n",
            "Epoch: 68/100, Loss: 0.189949\n",
            "Epoch: 69/100, Loss: 0.186299\n",
            "Epoch: 70/100, Loss: 0.182696\n",
            "Epoch: 71/100, Loss: 0.179138\n",
            "Epoch: 72/100, Loss: 0.175628\n",
            "Epoch: 73/100, Loss: 0.172168\n",
            "Epoch: 74/100, Loss: 0.168754\n",
            "Epoch: 75/100, Loss: 0.165386\n",
            "Epoch: 76/100, Loss: 0.162066\n",
            "Epoch: 77/100, Loss: 0.158791\n",
            "Epoch: 78/100, Loss: 0.155564\n",
            "Epoch: 79/100, Loss: 0.152380\n",
            "Epoch: 80/100, Loss: 0.149245\n",
            "Epoch: 81/100, Loss: 0.146158\n",
            "Epoch: 82/100, Loss: 0.143116\n",
            "Epoch: 83/100, Loss: 0.140129\n",
            "Epoch: 84/100, Loss: 0.137190\n",
            "Epoch: 85/100, Loss: 0.134308\n",
            "Epoch: 86/100, Loss: 0.131478\n",
            "Epoch: 87/100, Loss: 0.128700\n",
            "Epoch: 88/100, Loss: 0.125951\n",
            "Epoch: 89/100, Loss: 0.123227\n",
            "Epoch: 90/100, Loss: 0.120492\n",
            "Epoch: 91/100, Loss: 0.117784\n",
            "Epoch: 92/100, Loss: 0.115145\n",
            "Epoch: 93/100, Loss: 0.112573\n",
            "Epoch: 94/100, Loss: 0.110066\n",
            "Epoch: 95/100, Loss: 0.107600\n",
            "Epoch: 96/100, Loss: 0.105190\n",
            "Epoch: 97/100, Loss: 0.102849\n",
            "Epoch: 98/100, Loss: 0.100586\n",
            "Epoch: 99/100, Loss: 0.098376\n",
            "Epoch: 100/100, Loss: 0.096165\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.662\n",
            "Normalised mutual info score on k-means on latent space: 0.5955092639929951\n",
            "ARI score on k-means on latent space: 0.4950838494395233\n",
            "K-means cluster error on latent space: 42062.59375\n",
            "K-means silhouette score on latent space: 0.18537405 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7163\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.7169214860820166 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5806198588236289 \n",
            "\n",
            "ROUND NUMBER  6 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.905950\n",
            "Epoch: 2/100, Loss: 0.709956\n",
            "Epoch: 3/100, Loss: 0.638594\n",
            "Epoch: 4/100, Loss: 0.604279\n",
            "Epoch: 5/100, Loss: 0.582565\n",
            "Epoch: 6/100, Loss: 0.566172\n",
            "Epoch: 7/100, Loss: 0.552394\n",
            "Epoch: 8/100, Loss: 0.540142\n",
            "Epoch: 9/100, Loss: 0.528916\n",
            "Epoch: 10/100, Loss: 0.518441\n",
            "Epoch: 11/100, Loss: 0.508538\n",
            "Epoch: 12/100, Loss: 0.499084\n",
            "Epoch: 13/100, Loss: 0.490030\n",
            "Epoch: 14/100, Loss: 0.481305\n",
            "Epoch: 15/100, Loss: 0.472862\n",
            "Epoch: 16/100, Loss: 0.464673\n",
            "Epoch: 17/100, Loss: 0.456713\n",
            "Epoch: 18/100, Loss: 0.448961\n",
            "Epoch: 19/100, Loss: 0.441405\n",
            "Epoch: 20/100, Loss: 0.434026\n",
            "Epoch: 21/100, Loss: 0.426810\n",
            "Epoch: 22/100, Loss: 0.419748\n",
            "Epoch: 23/100, Loss: 0.412831\n",
            "Epoch: 24/100, Loss: 0.406051\n",
            "Epoch: 25/100, Loss: 0.399398\n",
            "Epoch: 26/100, Loss: 0.392868\n",
            "Epoch: 27/100, Loss: 0.386451\n",
            "Epoch: 28/100, Loss: 0.380143\n",
            "Epoch: 29/100, Loss: 0.373942\n",
            "Epoch: 30/100, Loss: 0.367838\n",
            "Epoch: 31/100, Loss: 0.361835\n",
            "Epoch: 32/100, Loss: 0.355925\n",
            "Epoch: 33/100, Loss: 0.350106\n",
            "Epoch: 34/100, Loss: 0.344373\n",
            "Epoch: 35/100, Loss: 0.338721\n",
            "Epoch: 36/100, Loss: 0.333149\n",
            "Epoch: 37/100, Loss: 0.327660\n",
            "Epoch: 38/100, Loss: 0.322247\n",
            "Epoch: 39/100, Loss: 0.316908\n",
            "Epoch: 40/100, Loss: 0.311640\n",
            "Epoch: 41/100, Loss: 0.306443\n",
            "Epoch: 42/100, Loss: 0.301316\n",
            "Epoch: 43/100, Loss: 0.296256\n",
            "Epoch: 44/100, Loss: 0.291265\n",
            "Epoch: 45/100, Loss: 0.286334\n",
            "Epoch: 46/100, Loss: 0.281466\n",
            "Epoch: 47/100, Loss: 0.276662\n",
            "Epoch: 48/100, Loss: 0.271918\n",
            "Epoch: 49/100, Loss: 0.267231\n",
            "Epoch: 50/100, Loss: 0.262604\n",
            "Epoch: 51/100, Loss: 0.258038\n",
            "Epoch: 52/100, Loss: 0.253525\n",
            "Epoch: 53/100, Loss: 0.249066\n",
            "Epoch: 54/100, Loss: 0.244667\n",
            "Epoch: 55/100, Loss: 0.240323\n",
            "Epoch: 56/100, Loss: 0.236032\n",
            "Epoch: 57/100, Loss: 0.231797\n",
            "Epoch: 58/100, Loss: 0.227612\n",
            "Epoch: 59/100, Loss: 0.223480\n",
            "Epoch: 60/100, Loss: 0.219399\n",
            "Epoch: 61/100, Loss: 0.215370\n",
            "Epoch: 62/100, Loss: 0.211394\n",
            "Epoch: 63/100, Loss: 0.207467\n",
            "Epoch: 64/100, Loss: 0.203595\n",
            "Epoch: 65/100, Loss: 0.199770\n",
            "Epoch: 66/100, Loss: 0.195990\n",
            "Epoch: 67/100, Loss: 0.192263\n",
            "Epoch: 68/100, Loss: 0.188586\n",
            "Epoch: 69/100, Loss: 0.184955\n",
            "Epoch: 70/100, Loss: 0.181376\n",
            "Epoch: 71/100, Loss: 0.177844\n",
            "Epoch: 72/100, Loss: 0.174360\n",
            "Epoch: 73/100, Loss: 0.170921\n",
            "Epoch: 74/100, Loss: 0.167535\n",
            "Epoch: 75/100, Loss: 0.164198\n",
            "Epoch: 76/100, Loss: 0.160906\n",
            "Epoch: 77/100, Loss: 0.157669\n",
            "Epoch: 78/100, Loss: 0.154478\n",
            "Epoch: 79/100, Loss: 0.151333\n",
            "Epoch: 80/100, Loss: 0.148239\n",
            "Epoch: 81/100, Loss: 0.145203\n",
            "Epoch: 82/100, Loss: 0.142211\n",
            "Epoch: 83/100, Loss: 0.139248\n",
            "Epoch: 84/100, Loss: 0.136321\n",
            "Epoch: 85/100, Loss: 0.133417\n",
            "Epoch: 86/100, Loss: 0.130546\n",
            "Epoch: 87/100, Loss: 0.127715\n",
            "Epoch: 88/100, Loss: 0.124933\n",
            "Epoch: 89/100, Loss: 0.122195\n",
            "Epoch: 90/100, Loss: 0.119494\n",
            "Epoch: 91/100, Loss: 0.116826\n",
            "Epoch: 92/100, Loss: 0.114200\n",
            "Epoch: 93/100, Loss: 0.111626\n",
            "Epoch: 94/100, Loss: 0.109115\n",
            "Epoch: 95/100, Loss: 0.106648\n",
            "Epoch: 96/100, Loss: 0.104212\n",
            "Epoch: 97/100, Loss: 0.101821\n",
            "Epoch: 98/100, Loss: 0.099469\n",
            "Epoch: 99/100, Loss: 0.097160\n",
            "Epoch: 100/100, Loss: 0.094910\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6672\n",
            "Normalised mutual info score on k-means on latent space: 0.5986129575243477\n",
            "ARI score on k-means on latent space: 0.49965025265070373\n",
            "K-means cluster error on latent space: 44237.609375\n",
            "K-means silhouette score on latent space: 0.19769204 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.641\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6235304790315622 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5016729782166923 \n",
            "\n",
            "ROUND NUMBER  7 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.907630\n",
            "Epoch: 2/100, Loss: 0.714342\n",
            "Epoch: 3/100, Loss: 0.646346\n",
            "Epoch: 4/100, Loss: 0.611435\n",
            "Epoch: 5/100, Loss: 0.588045\n",
            "Epoch: 6/100, Loss: 0.569852\n",
            "Epoch: 7/100, Loss: 0.554792\n",
            "Epoch: 8/100, Loss: 0.541741\n",
            "Epoch: 9/100, Loss: 0.530029\n",
            "Epoch: 10/100, Loss: 0.519254\n",
            "Epoch: 11/100, Loss: 0.509139\n",
            "Epoch: 12/100, Loss: 0.499537\n",
            "Epoch: 13/100, Loss: 0.490366\n",
            "Epoch: 14/100, Loss: 0.481554\n",
            "Epoch: 15/100, Loss: 0.473060\n",
            "Epoch: 16/100, Loss: 0.464832\n",
            "Epoch: 17/100, Loss: 0.456853\n",
            "Epoch: 18/100, Loss: 0.449091\n",
            "Epoch: 19/100, Loss: 0.441532\n",
            "Epoch: 20/100, Loss: 0.434152\n",
            "Epoch: 21/100, Loss: 0.426942\n",
            "Epoch: 22/100, Loss: 0.419885\n",
            "Epoch: 23/100, Loss: 0.412980\n",
            "Epoch: 24/100, Loss: 0.406220\n",
            "Epoch: 25/100, Loss: 0.399589\n",
            "Epoch: 26/100, Loss: 0.393078\n",
            "Epoch: 27/100, Loss: 0.386686\n",
            "Epoch: 28/100, Loss: 0.380404\n",
            "Epoch: 29/100, Loss: 0.374222\n",
            "Epoch: 30/100, Loss: 0.368143\n",
            "Epoch: 31/100, Loss: 0.362160\n",
            "Epoch: 32/100, Loss: 0.356274\n",
            "Epoch: 33/100, Loss: 0.350478\n",
            "Epoch: 34/100, Loss: 0.344769\n",
            "Epoch: 35/100, Loss: 0.339142\n",
            "Epoch: 36/100, Loss: 0.333596\n",
            "Epoch: 37/100, Loss: 0.328129\n",
            "Epoch: 38/100, Loss: 0.322736\n",
            "Epoch: 39/100, Loss: 0.317412\n",
            "Epoch: 40/100, Loss: 0.312161\n",
            "Epoch: 41/100, Loss: 0.306985\n",
            "Epoch: 42/100, Loss: 0.301874\n",
            "Epoch: 43/100, Loss: 0.296826\n",
            "Epoch: 44/100, Loss: 0.291845\n",
            "Epoch: 45/100, Loss: 0.286928\n",
            "Epoch: 46/100, Loss: 0.282073\n",
            "Epoch: 47/100, Loss: 0.277278\n",
            "Epoch: 48/100, Loss: 0.272543\n",
            "Epoch: 49/100, Loss: 0.267867\n",
            "Epoch: 50/100, Loss: 0.263249\n",
            "Epoch: 51/100, Loss: 0.258688\n",
            "Epoch: 52/100, Loss: 0.254187\n",
            "Epoch: 53/100, Loss: 0.249742\n",
            "Epoch: 54/100, Loss: 0.245355\n",
            "Epoch: 55/100, Loss: 0.241020\n",
            "Epoch: 56/100, Loss: 0.236740\n",
            "Epoch: 57/100, Loss: 0.232514\n",
            "Epoch: 58/100, Loss: 0.228337\n",
            "Epoch: 59/100, Loss: 0.224214\n",
            "Epoch: 60/100, Loss: 0.220143\n",
            "Epoch: 61/100, Loss: 0.216123\n",
            "Epoch: 62/100, Loss: 0.212151\n",
            "Epoch: 63/100, Loss: 0.208229\n",
            "Epoch: 64/100, Loss: 0.204357\n",
            "Epoch: 65/100, Loss: 0.200534\n",
            "Epoch: 66/100, Loss: 0.196760\n",
            "Epoch: 67/100, Loss: 0.193038\n",
            "Epoch: 68/100, Loss: 0.189365\n",
            "Epoch: 69/100, Loss: 0.185736\n",
            "Epoch: 70/100, Loss: 0.182156\n",
            "Epoch: 71/100, Loss: 0.178621\n",
            "Epoch: 72/100, Loss: 0.175133\n",
            "Epoch: 73/100, Loss: 0.171692\n",
            "Epoch: 74/100, Loss: 0.168300\n",
            "Epoch: 75/100, Loss: 0.164949\n",
            "Epoch: 76/100, Loss: 0.161645\n",
            "Epoch: 77/100, Loss: 0.158384\n",
            "Epoch: 78/100, Loss: 0.155172\n",
            "Epoch: 79/100, Loss: 0.152008\n",
            "Epoch: 80/100, Loss: 0.148887\n",
            "Epoch: 81/100, Loss: 0.145809\n",
            "Epoch: 82/100, Loss: 0.142780\n",
            "Epoch: 83/100, Loss: 0.139797\n",
            "Epoch: 84/100, Loss: 0.136862\n",
            "Epoch: 85/100, Loss: 0.133970\n",
            "Epoch: 86/100, Loss: 0.131122\n",
            "Epoch: 87/100, Loss: 0.128318\n",
            "Epoch: 88/100, Loss: 0.125563\n",
            "Epoch: 89/100, Loss: 0.122843\n",
            "Epoch: 90/100, Loss: 0.120172\n",
            "Epoch: 91/100, Loss: 0.117548\n",
            "Epoch: 92/100, Loss: 0.114962\n",
            "Epoch: 93/100, Loss: 0.112429\n",
            "Epoch: 94/100, Loss: 0.109926\n",
            "Epoch: 95/100, Loss: 0.107455\n",
            "Epoch: 96/100, Loss: 0.104999\n",
            "Epoch: 97/100, Loss: 0.102576\n",
            "Epoch: 98/100, Loss: 0.100195\n",
            "Epoch: 99/100, Loss: 0.097858\n",
            "Epoch: 100/100, Loss: 0.095564\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6487\n",
            "Normalised mutual info score on k-means on latent space: 0.5873027556750368\n",
            "ARI score on k-means on latent space: 0.4785048852603208\n",
            "K-means cluster error on latent space: 42490.60546875\n",
            "K-means silhouette score on latent space: 0.1890189 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6785\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6581043037819366 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.533363086826527 \n",
            "\n",
            "ROUND NUMBER  8 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.924297\n",
            "Epoch: 2/100, Loss: 0.730842\n",
            "Epoch: 3/100, Loss: 0.652194\n",
            "Epoch: 4/100, Loss: 0.613460\n",
            "Epoch: 5/100, Loss: 0.588947\n",
            "Epoch: 6/100, Loss: 0.570697\n",
            "Epoch: 7/100, Loss: 0.555482\n",
            "Epoch: 8/100, Loss: 0.542156\n",
            "Epoch: 9/100, Loss: 0.530180\n",
            "Epoch: 10/100, Loss: 0.519172\n",
            "Epoch: 11/100, Loss: 0.508889\n",
            "Epoch: 12/100, Loss: 0.499193\n",
            "Epoch: 13/100, Loss: 0.489960\n",
            "Epoch: 14/100, Loss: 0.481106\n",
            "Epoch: 15/100, Loss: 0.472573\n",
            "Epoch: 16/100, Loss: 0.464322\n",
            "Epoch: 17/100, Loss: 0.456323\n",
            "Epoch: 18/100, Loss: 0.448550\n",
            "Epoch: 19/100, Loss: 0.440985\n",
            "Epoch: 20/100, Loss: 0.433602\n",
            "Epoch: 21/100, Loss: 0.426392\n",
            "Epoch: 22/100, Loss: 0.419339\n",
            "Epoch: 23/100, Loss: 0.412439\n",
            "Epoch: 24/100, Loss: 0.405682\n",
            "Epoch: 25/100, Loss: 0.399050\n",
            "Epoch: 26/100, Loss: 0.392547\n",
            "Epoch: 27/100, Loss: 0.386160\n",
            "Epoch: 28/100, Loss: 0.379881\n",
            "Epoch: 29/100, Loss: 0.373706\n",
            "Epoch: 30/100, Loss: 0.367632\n",
            "Epoch: 31/100, Loss: 0.361656\n",
            "Epoch: 32/100, Loss: 0.355775\n",
            "Epoch: 33/100, Loss: 0.349982\n",
            "Epoch: 34/100, Loss: 0.344276\n",
            "Epoch: 35/100, Loss: 0.338653\n",
            "Epoch: 36/100, Loss: 0.333109\n",
            "Epoch: 37/100, Loss: 0.327644\n",
            "Epoch: 38/100, Loss: 0.322251\n",
            "Epoch: 39/100, Loss: 0.316933\n",
            "Epoch: 40/100, Loss: 0.311687\n",
            "Epoch: 41/100, Loss: 0.306506\n",
            "Epoch: 42/100, Loss: 0.301394\n",
            "Epoch: 43/100, Loss: 0.296347\n",
            "Epoch: 44/100, Loss: 0.291365\n",
            "Epoch: 45/100, Loss: 0.286447\n",
            "Epoch: 46/100, Loss: 0.281591\n",
            "Epoch: 47/100, Loss: 0.276797\n",
            "Epoch: 48/100, Loss: 0.272061\n",
            "Epoch: 49/100, Loss: 0.267385\n",
            "Epoch: 50/100, Loss: 0.262766\n",
            "Epoch: 51/100, Loss: 0.258207\n",
            "Epoch: 52/100, Loss: 0.253702\n",
            "Epoch: 53/100, Loss: 0.249250\n",
            "Epoch: 54/100, Loss: 0.244853\n",
            "Epoch: 55/100, Loss: 0.240509\n",
            "Epoch: 56/100, Loss: 0.236219\n",
            "Epoch: 57/100, Loss: 0.231986\n",
            "Epoch: 58/100, Loss: 0.227802\n",
            "Epoch: 59/100, Loss: 0.223672\n",
            "Epoch: 60/100, Loss: 0.219593\n",
            "Epoch: 61/100, Loss: 0.215567\n",
            "Epoch: 62/100, Loss: 0.211591\n",
            "Epoch: 63/100, Loss: 0.207665\n",
            "Epoch: 64/100, Loss: 0.203790\n",
            "Epoch: 65/100, Loss: 0.199963\n",
            "Epoch: 66/100, Loss: 0.196185\n",
            "Epoch: 67/100, Loss: 0.192456\n",
            "Epoch: 68/100, Loss: 0.188775\n",
            "Epoch: 69/100, Loss: 0.185139\n",
            "Epoch: 70/100, Loss: 0.181555\n",
            "Epoch: 71/100, Loss: 0.178017\n",
            "Epoch: 72/100, Loss: 0.174528\n",
            "Epoch: 73/100, Loss: 0.171084\n",
            "Epoch: 74/100, Loss: 0.167687\n",
            "Epoch: 75/100, Loss: 0.164335\n",
            "Epoch: 76/100, Loss: 0.161031\n",
            "Epoch: 77/100, Loss: 0.157776\n",
            "Epoch: 78/100, Loss: 0.154563\n",
            "Epoch: 79/100, Loss: 0.151396\n",
            "Epoch: 80/100, Loss: 0.148274\n",
            "Epoch: 81/100, Loss: 0.145195\n",
            "Epoch: 82/100, Loss: 0.142164\n",
            "Epoch: 83/100, Loss: 0.139182\n",
            "Epoch: 84/100, Loss: 0.136246\n",
            "Epoch: 85/100, Loss: 0.133362\n",
            "Epoch: 86/100, Loss: 0.130522\n",
            "Epoch: 87/100, Loss: 0.127740\n",
            "Epoch: 88/100, Loss: 0.125014\n",
            "Epoch: 89/100, Loss: 0.122341\n",
            "Epoch: 90/100, Loss: 0.119689\n",
            "Epoch: 91/100, Loss: 0.117053\n",
            "Epoch: 92/100, Loss: 0.114430\n",
            "Epoch: 93/100, Loss: 0.111813\n",
            "Epoch: 94/100, Loss: 0.109230\n",
            "Epoch: 95/100, Loss: 0.106704\n",
            "Epoch: 96/100, Loss: 0.104246\n",
            "Epoch: 97/100, Loss: 0.101840\n",
            "Epoch: 98/100, Loss: 0.099478\n",
            "Epoch: 99/100, Loss: 0.097161\n",
            "Epoch: 100/100, Loss: 0.094901\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6838\n",
            "Normalised mutual info score on k-means on latent space: 0.6069488953982695\n",
            "ARI score on k-means on latent space: 0.5093412214253276\n",
            "K-means cluster error on latent space: 42882.76171875\n",
            "K-means silhouette score on latent space: 0.19594233 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7001\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6725434876018889 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5485948928029515 \n",
            "\n",
            "ROUND NUMBER  9 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.926871\n",
            "Epoch: 2/100, Loss: 0.720012\n",
            "Epoch: 3/100, Loss: 0.645783\n",
            "Epoch: 4/100, Loss: 0.610090\n",
            "Epoch: 5/100, Loss: 0.587439\n",
            "Epoch: 6/100, Loss: 0.570066\n",
            "Epoch: 7/100, Loss: 0.555375\n",
            "Epoch: 8/100, Loss: 0.542423\n",
            "Epoch: 9/100, Loss: 0.530695\n",
            "Epoch: 10/100, Loss: 0.519847\n",
            "Epoch: 11/100, Loss: 0.509687\n",
            "Epoch: 12/100, Loss: 0.500063\n",
            "Epoch: 13/100, Loss: 0.490880\n",
            "Epoch: 14/100, Loss: 0.482059\n",
            "Epoch: 15/100, Loss: 0.473546\n",
            "Epoch: 16/100, Loss: 0.465304\n",
            "Epoch: 17/100, Loss: 0.457302\n",
            "Epoch: 18/100, Loss: 0.449513\n",
            "Epoch: 19/100, Loss: 0.441924\n",
            "Epoch: 20/100, Loss: 0.434521\n",
            "Epoch: 21/100, Loss: 0.427281\n",
            "Epoch: 22/100, Loss: 0.420199\n",
            "Epoch: 23/100, Loss: 0.413270\n",
            "Epoch: 24/100, Loss: 0.406474\n",
            "Epoch: 25/100, Loss: 0.399809\n",
            "Epoch: 26/100, Loss: 0.393271\n",
            "Epoch: 27/100, Loss: 0.386846\n",
            "Epoch: 28/100, Loss: 0.380532\n",
            "Epoch: 29/100, Loss: 0.374325\n",
            "Epoch: 30/100, Loss: 0.368221\n",
            "Epoch: 31/100, Loss: 0.362216\n",
            "Epoch: 32/100, Loss: 0.356306\n",
            "Epoch: 33/100, Loss: 0.350487\n",
            "Epoch: 34/100, Loss: 0.344761\n",
            "Epoch: 35/100, Loss: 0.339115\n",
            "Epoch: 36/100, Loss: 0.333550\n",
            "Epoch: 37/100, Loss: 0.328061\n",
            "Epoch: 38/100, Loss: 0.322653\n",
            "Epoch: 39/100, Loss: 0.317319\n",
            "Epoch: 40/100, Loss: 0.312055\n",
            "Epoch: 41/100, Loss: 0.306859\n",
            "Epoch: 42/100, Loss: 0.301729\n",
            "Epoch: 43/100, Loss: 0.296675\n",
            "Epoch: 44/100, Loss: 0.291679\n",
            "Epoch: 45/100, Loss: 0.286751\n",
            "Epoch: 46/100, Loss: 0.281886\n",
            "Epoch: 47/100, Loss: 0.277082\n",
            "Epoch: 48/100, Loss: 0.272335\n",
            "Epoch: 49/100, Loss: 0.267647\n",
            "Epoch: 50/100, Loss: 0.263017\n",
            "Epoch: 51/100, Loss: 0.258444\n",
            "Epoch: 52/100, Loss: 0.253927\n",
            "Epoch: 53/100, Loss: 0.249469\n",
            "Epoch: 54/100, Loss: 0.245067\n",
            "Epoch: 55/100, Loss: 0.240719\n",
            "Epoch: 56/100, Loss: 0.236428\n",
            "Epoch: 57/100, Loss: 0.232187\n",
            "Epoch: 58/100, Loss: 0.228005\n",
            "Epoch: 59/100, Loss: 0.223867\n",
            "Epoch: 60/100, Loss: 0.219784\n",
            "Epoch: 61/100, Loss: 0.215754\n",
            "Epoch: 62/100, Loss: 0.211771\n",
            "Epoch: 63/100, Loss: 0.207842\n",
            "Epoch: 64/100, Loss: 0.203960\n",
            "Epoch: 65/100, Loss: 0.200129\n",
            "Epoch: 66/100, Loss: 0.196351\n",
            "Epoch: 67/100, Loss: 0.192620\n",
            "Epoch: 68/100, Loss: 0.188939\n",
            "Epoch: 69/100, Loss: 0.185307\n",
            "Epoch: 70/100, Loss: 0.181719\n",
            "Epoch: 71/100, Loss: 0.178181\n",
            "Epoch: 72/100, Loss: 0.174693\n",
            "Epoch: 73/100, Loss: 0.171249\n",
            "Epoch: 74/100, Loss: 0.167857\n",
            "Epoch: 75/100, Loss: 0.164509\n",
            "Epoch: 76/100, Loss: 0.161210\n",
            "Epoch: 77/100, Loss: 0.157961\n",
            "Epoch: 78/100, Loss: 0.154756\n",
            "Epoch: 79/100, Loss: 0.151602\n",
            "Epoch: 80/100, Loss: 0.148490\n",
            "Epoch: 81/100, Loss: 0.145429\n",
            "Epoch: 82/100, Loss: 0.142417\n",
            "Epoch: 83/100, Loss: 0.139469\n",
            "Epoch: 84/100, Loss: 0.136561\n",
            "Epoch: 85/100, Loss: 0.133672\n",
            "Epoch: 86/100, Loss: 0.130797\n",
            "Epoch: 87/100, Loss: 0.127966\n",
            "Epoch: 88/100, Loss: 0.125184\n",
            "Epoch: 89/100, Loss: 0.122464\n",
            "Epoch: 90/100, Loss: 0.119803\n",
            "Epoch: 91/100, Loss: 0.117182\n",
            "Epoch: 92/100, Loss: 0.114620\n",
            "Epoch: 93/100, Loss: 0.112123\n",
            "Epoch: 94/100, Loss: 0.109687\n",
            "Epoch: 95/100, Loss: 0.107335\n",
            "Epoch: 96/100, Loss: 0.105023\n",
            "Epoch: 97/100, Loss: 0.102701\n",
            "Epoch: 98/100, Loss: 0.100291\n",
            "Epoch: 99/100, Loss: 0.097883\n",
            "Epoch: 100/100, Loss: 0.095524\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6488\n",
            "Normalised mutual info score on k-means on latent space: 0.5940051380608841\n",
            "ARI score on k-means on latent space: 0.4920873461717409\n",
            "K-means cluster error on latent space: 44961.06640625\n",
            "K-means silhouette score on latent space: 0.18462837 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6469\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6476624776908987 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5078399897152908 \n",
            "\n",
            "ROUND NUMBER  10 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.939431\n",
            "Epoch: 2/100, Loss: 0.750768\n",
            "Epoch: 3/100, Loss: 0.666030\n",
            "Epoch: 4/100, Loss: 0.619747\n",
            "Epoch: 5/100, Loss: 0.591577\n",
            "Epoch: 6/100, Loss: 0.571831\n",
            "Epoch: 7/100, Loss: 0.556371\n",
            "Epoch: 8/100, Loss: 0.543094\n",
            "Epoch: 9/100, Loss: 0.531213\n",
            "Epoch: 10/100, Loss: 0.520297\n",
            "Epoch: 11/100, Loss: 0.510088\n",
            "Epoch: 12/100, Loss: 0.500424\n",
            "Epoch: 13/100, Loss: 0.491206\n",
            "Epoch: 14/100, Loss: 0.482360\n",
            "Epoch: 15/100, Loss: 0.473841\n",
            "Epoch: 16/100, Loss: 0.465598\n",
            "Epoch: 17/100, Loss: 0.457592\n",
            "Epoch: 18/100, Loss: 0.449812\n",
            "Epoch: 19/100, Loss: 0.442239\n",
            "Epoch: 20/100, Loss: 0.434848\n",
            "Epoch: 21/100, Loss: 0.427631\n",
            "Epoch: 22/100, Loss: 0.420572\n",
            "Epoch: 23/100, Loss: 0.413660\n",
            "Epoch: 24/100, Loss: 0.406886\n",
            "Epoch: 25/100, Loss: 0.400239\n",
            "Epoch: 26/100, Loss: 0.393715\n",
            "Epoch: 27/100, Loss: 0.387309\n",
            "Epoch: 28/100, Loss: 0.381016\n",
            "Epoch: 29/100, Loss: 0.374826\n",
            "Epoch: 30/100, Loss: 0.368743\n",
            "Epoch: 31/100, Loss: 0.362753\n",
            "Epoch: 32/100, Loss: 0.356860\n",
            "Epoch: 33/100, Loss: 0.351055\n",
            "Epoch: 34/100, Loss: 0.345340\n",
            "Epoch: 35/100, Loss: 0.339711\n",
            "Epoch: 36/100, Loss: 0.334162\n",
            "Epoch: 37/100, Loss: 0.328691\n",
            "Epoch: 38/100, Loss: 0.323298\n",
            "Epoch: 39/100, Loss: 0.317976\n",
            "Epoch: 40/100, Loss: 0.312722\n",
            "Epoch: 41/100, Loss: 0.307540\n",
            "Epoch: 42/100, Loss: 0.302426\n",
            "Epoch: 43/100, Loss: 0.297376\n",
            "Epoch: 44/100, Loss: 0.292389\n",
            "Epoch: 45/100, Loss: 0.287468\n",
            "Epoch: 46/100, Loss: 0.282609\n",
            "Epoch: 47/100, Loss: 0.277808\n",
            "Epoch: 48/100, Loss: 0.273071\n",
            "Epoch: 49/100, Loss: 0.268390\n",
            "Epoch: 50/100, Loss: 0.263770\n",
            "Epoch: 51/100, Loss: 0.259206\n",
            "Epoch: 52/100, Loss: 0.254696\n",
            "Epoch: 53/100, Loss: 0.250243\n",
            "Epoch: 54/100, Loss: 0.245843\n",
            "Epoch: 55/100, Loss: 0.241498\n",
            "Epoch: 56/100, Loss: 0.237208\n",
            "Epoch: 57/100, Loss: 0.232969\n",
            "Epoch: 58/100, Loss: 0.228782\n",
            "Epoch: 59/100, Loss: 0.224647\n",
            "Epoch: 60/100, Loss: 0.220566\n",
            "Epoch: 61/100, Loss: 0.216534\n",
            "Epoch: 62/100, Loss: 0.212554\n",
            "Epoch: 63/100, Loss: 0.208626\n",
            "Epoch: 64/100, Loss: 0.204746\n",
            "Epoch: 65/100, Loss: 0.200917\n",
            "Epoch: 66/100, Loss: 0.197134\n",
            "Epoch: 67/100, Loss: 0.193403\n",
            "Epoch: 68/100, Loss: 0.189719\n",
            "Epoch: 69/100, Loss: 0.186084\n",
            "Epoch: 70/100, Loss: 0.182498\n",
            "Epoch: 71/100, Loss: 0.178959\n",
            "Epoch: 72/100, Loss: 0.175467\n",
            "Epoch: 73/100, Loss: 0.172024\n",
            "Epoch: 74/100, Loss: 0.168627\n",
            "Epoch: 75/100, Loss: 0.165273\n",
            "Epoch: 76/100, Loss: 0.161964\n",
            "Epoch: 77/100, Loss: 0.158700\n",
            "Epoch: 78/100, Loss: 0.155482\n",
            "Epoch: 79/100, Loss: 0.152314\n",
            "Epoch: 80/100, Loss: 0.149190\n",
            "Epoch: 81/100, Loss: 0.146115\n",
            "Epoch: 82/100, Loss: 0.143085\n",
            "Epoch: 83/100, Loss: 0.140101\n",
            "Epoch: 84/100, Loss: 0.137159\n",
            "Epoch: 85/100, Loss: 0.134272\n",
            "Epoch: 86/100, Loss: 0.131434\n",
            "Epoch: 87/100, Loss: 0.128656\n",
            "Epoch: 88/100, Loss: 0.125924\n",
            "Epoch: 89/100, Loss: 0.123224\n",
            "Epoch: 90/100, Loss: 0.120568\n",
            "Epoch: 91/100, Loss: 0.117973\n",
            "Epoch: 92/100, Loss: 0.115421\n",
            "Epoch: 93/100, Loss: 0.112875\n",
            "Epoch: 94/100, Loss: 0.110343\n",
            "Epoch: 95/100, Loss: 0.107858\n",
            "Epoch: 96/100, Loss: 0.105437\n",
            "Epoch: 97/100, Loss: 0.103074\n",
            "Epoch: 98/100, Loss: 0.100781\n",
            "Epoch: 99/100, Loss: 0.098523\n",
            "Epoch: 100/100, Loss: 0.096292\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6489\n",
            "Normalised mutual info score on k-means on latent space: 0.5873778092582784\n",
            "ARI score on k-means on latent space: 0.47194211094746596\n",
            "K-means cluster error on latent space: 42217.2421875\n",
            "K-means silhouette score on latent space: 0.18873343 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6241\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6088451300648258 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.45800807977785807 \n",
            "\n",
            "Average k-means accuracy score at latent space: 0.6629499999999999 \n",
            "\n",
            "Average k-means NMI score at latent space: 0.5958266546546712 \n",
            "\n",
            "Average k-means ARI score at latent space: 0.4918336479244963 \n",
            "\n",
            "Average agglomerative clustering accuracy score at latent space: 0.68517 \n",
            "\n",
            "Average agglomerative clustering NMI score at latent space: 0.6717045049575894 \n",
            "\n",
            "Average agglomerative clustering ARI score at latent space: 0.547135477447528 \n",
            "\n",
            "Average k-means silhouette score on latent space: 0.18736273050308228 \n",
            "\n",
            "Average k-means cluster error on latent space: 43299.7953125 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_for_k_80 = run_experiment(80, dataloader, 784, datapoints = images, datapoints_reshaped = images_reshaped, labels = labels, epochs=100, hl1_neurons=32, hl2_neurons=64, hl3_neurons=128, latent_dimension=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlTlH1et7ymn",
        "outputId": "40221648-8077-401f-e33a-fd249fdb69d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment results for k-means with k = 80 clusters:\n",
            "\n",
            "Running k-means algorithm in order to get our pseudolabels: \n",
            "\n",
            "K_means greedy accuracy score (initial space): 0.8435\n",
            "Normalised mutual info score (initial space): 0.73265413982975\n",
            "ARI (initial space): 0.7006664614870293 \n",
            "\n",
            "Using the autoencoder model on our data: \n",
            "\n",
            "ROUND NUMBER  1 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.943787\n",
            "Epoch: 2/100, Loss: 0.743224\n",
            "Epoch: 3/100, Loss: 0.661344\n",
            "Epoch: 4/100, Loss: 0.620281\n",
            "Epoch: 5/100, Loss: 0.593980\n",
            "Epoch: 6/100, Loss: 0.574368\n",
            "Epoch: 7/100, Loss: 0.558605\n",
            "Epoch: 8/100, Loss: 0.545226\n",
            "Epoch: 9/100, Loss: 0.533332\n",
            "Epoch: 10/100, Loss: 0.522437\n",
            "Epoch: 11/100, Loss: 0.512255\n",
            "Epoch: 12/100, Loss: 0.502609\n",
            "Epoch: 13/100, Loss: 0.493387\n",
            "Epoch: 14/100, Loss: 0.484529\n",
            "Epoch: 15/100, Loss: 0.475978\n",
            "Epoch: 16/100, Loss: 0.467694\n",
            "Epoch: 17/100, Loss: 0.459651\n",
            "Epoch: 18/100, Loss: 0.451827\n",
            "Epoch: 19/100, Loss: 0.444205\n",
            "Epoch: 20/100, Loss: 0.436762\n",
            "Epoch: 21/100, Loss: 0.429490\n",
            "Epoch: 22/100, Loss: 0.422370\n",
            "Epoch: 23/100, Loss: 0.415399\n",
            "Epoch: 24/100, Loss: 0.408567\n",
            "Epoch: 25/100, Loss: 0.401863\n",
            "Epoch: 26/100, Loss: 0.395279\n",
            "Epoch: 27/100, Loss: 0.388814\n",
            "Epoch: 28/100, Loss: 0.382458\n",
            "Epoch: 29/100, Loss: 0.376206\n",
            "Epoch: 30/100, Loss: 0.370058\n",
            "Epoch: 31/100, Loss: 0.364013\n",
            "Epoch: 32/100, Loss: 0.358063\n",
            "Epoch: 33/100, Loss: 0.352206\n",
            "Epoch: 34/100, Loss: 0.346435\n",
            "Epoch: 35/100, Loss: 0.340744\n",
            "Epoch: 36/100, Loss: 0.335140\n",
            "Epoch: 37/100, Loss: 0.329615\n",
            "Epoch: 38/100, Loss: 0.324167\n",
            "Epoch: 39/100, Loss: 0.318794\n",
            "Epoch: 40/100, Loss: 0.313489\n",
            "Epoch: 41/100, Loss: 0.308258\n",
            "Epoch: 42/100, Loss: 0.303099\n",
            "Epoch: 43/100, Loss: 0.298009\n",
            "Epoch: 44/100, Loss: 0.292983\n",
            "Epoch: 45/100, Loss: 0.288018\n",
            "Epoch: 46/100, Loss: 0.283122\n",
            "Epoch: 47/100, Loss: 0.278285\n",
            "Epoch: 48/100, Loss: 0.273509\n",
            "Epoch: 49/100, Loss: 0.268796\n",
            "Epoch: 50/100, Loss: 0.264141\n",
            "Epoch: 51/100, Loss: 0.259542\n",
            "Epoch: 52/100, Loss: 0.255003\n",
            "Epoch: 53/100, Loss: 0.250519\n",
            "Epoch: 54/100, Loss: 0.246094\n",
            "Epoch: 55/100, Loss: 0.241724\n",
            "Epoch: 56/100, Loss: 0.237407\n",
            "Epoch: 57/100, Loss: 0.233145\n",
            "Epoch: 58/100, Loss: 0.228934\n",
            "Epoch: 59/100, Loss: 0.224776\n",
            "Epoch: 60/100, Loss: 0.220669\n",
            "Epoch: 61/100, Loss: 0.216620\n",
            "Epoch: 62/100, Loss: 0.212617\n",
            "Epoch: 63/100, Loss: 0.208666\n",
            "Epoch: 64/100, Loss: 0.204765\n",
            "Epoch: 65/100, Loss: 0.200917\n",
            "Epoch: 66/100, Loss: 0.197117\n",
            "Epoch: 67/100, Loss: 0.193368\n",
            "Epoch: 68/100, Loss: 0.189668\n",
            "Epoch: 69/100, Loss: 0.186020\n",
            "Epoch: 70/100, Loss: 0.182419\n",
            "Epoch: 71/100, Loss: 0.178866\n",
            "Epoch: 72/100, Loss: 0.175359\n",
            "Epoch: 73/100, Loss: 0.171898\n",
            "Epoch: 74/100, Loss: 0.168488\n",
            "Epoch: 75/100, Loss: 0.165123\n",
            "Epoch: 76/100, Loss: 0.161815\n",
            "Epoch: 77/100, Loss: 0.158552\n",
            "Epoch: 78/100, Loss: 0.155341\n",
            "Epoch: 79/100, Loss: 0.152187\n",
            "Epoch: 80/100, Loss: 0.149094\n",
            "Epoch: 81/100, Loss: 0.146072\n",
            "Epoch: 82/100, Loss: 0.143067\n",
            "Epoch: 83/100, Loss: 0.140062\n",
            "Epoch: 84/100, Loss: 0.137087\n",
            "Epoch: 85/100, Loss: 0.134116\n",
            "Epoch: 86/100, Loss: 0.131200\n",
            "Epoch: 87/100, Loss: 0.128363\n",
            "Epoch: 88/100, Loss: 0.125583\n",
            "Epoch: 89/100, Loss: 0.122850\n",
            "Epoch: 90/100, Loss: 0.120161\n",
            "Epoch: 91/100, Loss: 0.117523\n",
            "Epoch: 92/100, Loss: 0.114935\n",
            "Epoch: 93/100, Loss: 0.112388\n",
            "Epoch: 94/100, Loss: 0.109884\n",
            "Epoch: 95/100, Loss: 0.107445\n",
            "Epoch: 96/100, Loss: 0.105055\n",
            "Epoch: 97/100, Loss: 0.102676\n",
            "Epoch: 98/100, Loss: 0.100312\n",
            "Epoch: 99/100, Loss: 0.097975\n",
            "Epoch: 100/100, Loss: 0.095685\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6975\n",
            "Normalised mutual info score on k-means on latent space: 0.5985157474854509\n",
            "ARI score on k-means on latent space: 0.5160937096498094\n",
            "K-means cluster error on latent space: 44214.27734375\n",
            "K-means silhouette score on latent space: 0.18594246 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6694\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6523280641233917 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.4995560081625095 \n",
            "\n",
            "ROUND NUMBER  2 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.939429\n",
            "Epoch: 2/100, Loss: 0.735830\n",
            "Epoch: 3/100, Loss: 0.657320\n",
            "Epoch: 4/100, Loss: 0.619397\n",
            "Epoch: 5/100, Loss: 0.594785\n",
            "Epoch: 6/100, Loss: 0.575953\n",
            "Epoch: 7/100, Loss: 0.560262\n",
            "Epoch: 8/100, Loss: 0.546596\n",
            "Epoch: 9/100, Loss: 0.534328\n",
            "Epoch: 10/100, Loss: 0.523051\n",
            "Epoch: 11/100, Loss: 0.512536\n",
            "Epoch: 12/100, Loss: 0.502614\n",
            "Epoch: 13/100, Loss: 0.493182\n",
            "Epoch: 14/100, Loss: 0.484163\n",
            "Epoch: 15/100, Loss: 0.475487\n",
            "Epoch: 16/100, Loss: 0.467104\n",
            "Epoch: 17/100, Loss: 0.458986\n",
            "Epoch: 18/100, Loss: 0.451107\n",
            "Epoch: 19/100, Loss: 0.443432\n",
            "Epoch: 20/100, Loss: 0.435952\n",
            "Epoch: 21/100, Loss: 0.428651\n",
            "Epoch: 22/100, Loss: 0.421505\n",
            "Epoch: 23/100, Loss: 0.414512\n",
            "Epoch: 24/100, Loss: 0.407665\n",
            "Epoch: 25/100, Loss: 0.400950\n",
            "Epoch: 26/100, Loss: 0.394361\n",
            "Epoch: 27/100, Loss: 0.387891\n",
            "Epoch: 28/100, Loss: 0.381540\n",
            "Epoch: 29/100, Loss: 0.375297\n",
            "Epoch: 30/100, Loss: 0.369157\n",
            "Epoch: 31/100, Loss: 0.363119\n",
            "Epoch: 32/100, Loss: 0.357177\n",
            "Epoch: 33/100, Loss: 0.351322\n",
            "Epoch: 34/100, Loss: 0.345560\n",
            "Epoch: 35/100, Loss: 0.339886\n",
            "Epoch: 36/100, Loss: 0.334293\n",
            "Epoch: 37/100, Loss: 0.328775\n",
            "Epoch: 38/100, Loss: 0.323331\n",
            "Epoch: 39/100, Loss: 0.317963\n",
            "Epoch: 40/100, Loss: 0.312671\n",
            "Epoch: 41/100, Loss: 0.307451\n",
            "Epoch: 42/100, Loss: 0.302301\n",
            "Epoch: 43/100, Loss: 0.297214\n",
            "Epoch: 44/100, Loss: 0.292191\n",
            "Epoch: 45/100, Loss: 0.287234\n",
            "Epoch: 46/100, Loss: 0.282342\n",
            "Epoch: 47/100, Loss: 0.277508\n",
            "Epoch: 48/100, Loss: 0.272739\n",
            "Epoch: 49/100, Loss: 0.268031\n",
            "Epoch: 50/100, Loss: 0.263378\n",
            "Epoch: 51/100, Loss: 0.258784\n",
            "Epoch: 52/100, Loss: 0.254246\n",
            "Epoch: 53/100, Loss: 0.249767\n",
            "Epoch: 54/100, Loss: 0.245346\n",
            "Epoch: 55/100, Loss: 0.240980\n",
            "Epoch: 56/100, Loss: 0.236666\n",
            "Epoch: 57/100, Loss: 0.232406\n",
            "Epoch: 58/100, Loss: 0.228200\n",
            "Epoch: 59/100, Loss: 0.224042\n",
            "Epoch: 60/100, Loss: 0.219940\n",
            "Epoch: 61/100, Loss: 0.215893\n",
            "Epoch: 62/100, Loss: 0.211895\n",
            "Epoch: 63/100, Loss: 0.207943\n",
            "Epoch: 64/100, Loss: 0.204049\n",
            "Epoch: 65/100, Loss: 0.200206\n",
            "Epoch: 66/100, Loss: 0.196410\n",
            "Epoch: 67/100, Loss: 0.192668\n",
            "Epoch: 68/100, Loss: 0.188970\n",
            "Epoch: 69/100, Loss: 0.185326\n",
            "Epoch: 70/100, Loss: 0.181731\n",
            "Epoch: 71/100, Loss: 0.178188\n",
            "Epoch: 72/100, Loss: 0.174695\n",
            "Epoch: 73/100, Loss: 0.171261\n",
            "Epoch: 74/100, Loss: 0.167878\n",
            "Epoch: 75/100, Loss: 0.164561\n",
            "Epoch: 76/100, Loss: 0.161293\n",
            "Epoch: 77/100, Loss: 0.158055\n",
            "Epoch: 78/100, Loss: 0.154862\n",
            "Epoch: 79/100, Loss: 0.151670\n",
            "Epoch: 80/100, Loss: 0.148518\n",
            "Epoch: 81/100, Loss: 0.145420\n",
            "Epoch: 82/100, Loss: 0.142347\n",
            "Epoch: 83/100, Loss: 0.139305\n",
            "Epoch: 84/100, Loss: 0.136330\n",
            "Epoch: 85/100, Loss: 0.133431\n",
            "Epoch: 86/100, Loss: 0.130581\n",
            "Epoch: 87/100, Loss: 0.127771\n",
            "Epoch: 88/100, Loss: 0.125012\n",
            "Epoch: 89/100, Loss: 0.122308\n",
            "Epoch: 90/100, Loss: 0.119644\n",
            "Epoch: 91/100, Loss: 0.117030\n",
            "Epoch: 92/100, Loss: 0.114488\n",
            "Epoch: 93/100, Loss: 0.111981\n",
            "Epoch: 94/100, Loss: 0.109482\n",
            "Epoch: 95/100, Loss: 0.107014\n",
            "Epoch: 96/100, Loss: 0.104554\n",
            "Epoch: 97/100, Loss: 0.102095\n",
            "Epoch: 98/100, Loss: 0.099653\n",
            "Epoch: 99/100, Loss: 0.097246\n",
            "Epoch: 100/100, Loss: 0.094910\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.702\n",
            "Normalised mutual info score on k-means on latent space: 0.6105404390976558\n",
            "ARI score on k-means on latent space: 0.5255374522148327\n",
            "K-means cluster error on latent space: 43242.046875\n",
            "K-means silhouette score on latent space: 0.19280598 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7028\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.661986196896676 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5322775027478116 \n",
            "\n",
            "ROUND NUMBER  3 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.910219\n",
            "Epoch: 2/100, Loss: 0.720246\n",
            "Epoch: 3/100, Loss: 0.647734\n",
            "Epoch: 4/100, Loss: 0.610774\n",
            "Epoch: 5/100, Loss: 0.586551\n",
            "Epoch: 6/100, Loss: 0.568616\n",
            "Epoch: 7/100, Loss: 0.554013\n",
            "Epoch: 8/100, Loss: 0.541316\n",
            "Epoch: 9/100, Loss: 0.529872\n",
            "Epoch: 10/100, Loss: 0.519301\n",
            "Epoch: 11/100, Loss: 0.509360\n",
            "Epoch: 12/100, Loss: 0.499910\n",
            "Epoch: 13/100, Loss: 0.490863\n",
            "Epoch: 14/100, Loss: 0.482154\n",
            "Epoch: 15/100, Loss: 0.473731\n",
            "Epoch: 16/100, Loss: 0.465563\n",
            "Epoch: 17/100, Loss: 0.457626\n",
            "Epoch: 18/100, Loss: 0.449885\n",
            "Epoch: 19/100, Loss: 0.442329\n",
            "Epoch: 20/100, Loss: 0.434949\n",
            "Epoch: 21/100, Loss: 0.427730\n",
            "Epoch: 22/100, Loss: 0.420665\n",
            "Epoch: 25/100, Loss: 0.400301\n",
            "Epoch: 26/100, Loss: 0.393770\n",
            "Epoch: 27/100, Loss: 0.387354\n",
            "Epoch: 28/100, Loss: 0.381046\n",
            "Epoch: 29/100, Loss: 0.374846\n",
            "Epoch: 30/100, Loss: 0.368745\n",
            "Epoch: 31/100, Loss: 0.362739\n",
            "Epoch: 32/100, Loss: 0.356829\n",
            "Epoch: 33/100, Loss: 0.351005\n",
            "Epoch: 34/100, Loss: 0.345271\n",
            "Epoch: 35/100, Loss: 0.339621\n",
            "Epoch: 36/100, Loss: 0.334058\n",
            "Epoch: 37/100, Loss: 0.328563\n",
            "Epoch: 38/100, Loss: 0.323149\n",
            "Epoch: 39/100, Loss: 0.317809\n",
            "Epoch: 40/100, Loss: 0.312538\n",
            "Epoch: 41/100, Loss: 0.307339\n",
            "Epoch: 42/100, Loss: 0.302207\n",
            "Epoch: 43/100, Loss: 0.297140\n",
            "Epoch: 44/100, Loss: 0.292142\n",
            "Epoch: 45/100, Loss: 0.287208\n",
            "Epoch: 46/100, Loss: 0.282335\n",
            "Epoch: 47/100, Loss: 0.277525\n",
            "Epoch: 48/100, Loss: 0.272776\n",
            "Epoch: 49/100, Loss: 0.268084\n",
            "Epoch: 50/100, Loss: 0.263453\n",
            "Epoch: 51/100, Loss: 0.258880\n",
            "Epoch: 52/100, Loss: 0.254365\n",
            "Epoch: 53/100, Loss: 0.249908\n",
            "Epoch: 54/100, Loss: 0.245505\n",
            "Epoch: 55/100, Loss: 0.241157\n",
            "Epoch: 56/100, Loss: 0.236861\n",
            "Epoch: 57/100, Loss: 0.232619\n",
            "Epoch: 58/100, Loss: 0.228427\n",
            "Epoch: 59/100, Loss: 0.224293\n",
            "Epoch: 60/100, Loss: 0.220208\n",
            "Epoch: 61/100, Loss: 0.216174\n",
            "Epoch: 62/100, Loss: 0.212194\n",
            "Epoch: 63/100, Loss: 0.208262\n",
            "Epoch: 64/100, Loss: 0.204382\n",
            "Epoch: 65/100, Loss: 0.200547\n",
            "Epoch: 66/100, Loss: 0.196766\n",
            "Epoch: 67/100, Loss: 0.193031\n",
            "Epoch: 68/100, Loss: 0.189346\n",
            "Epoch: 69/100, Loss: 0.185710\n",
            "Epoch: 70/100, Loss: 0.182119\n",
            "Epoch: 71/100, Loss: 0.178575\n",
            "Epoch: 72/100, Loss: 0.175079\n",
            "Epoch: 73/100, Loss: 0.171629\n",
            "Epoch: 74/100, Loss: 0.168229\n",
            "Epoch: 75/100, Loss: 0.164872\n",
            "Epoch: 76/100, Loss: 0.161563\n",
            "Epoch: 77/100, Loss: 0.158300\n",
            "Epoch: 78/100, Loss: 0.155081\n",
            "Epoch: 79/100, Loss: 0.151910\n",
            "Epoch: 80/100, Loss: 0.148786\n",
            "Epoch: 81/100, Loss: 0.145704\n",
            "Epoch: 82/100, Loss: 0.142670\n",
            "Epoch: 83/100, Loss: 0.139682\n",
            "Epoch: 84/100, Loss: 0.136737\n",
            "Epoch: 85/100, Loss: 0.133839\n",
            "Epoch: 86/100, Loss: 0.130980\n",
            "Epoch: 87/100, Loss: 0.128177\n",
            "Epoch: 88/100, Loss: 0.125425\n",
            "Epoch: 89/100, Loss: 0.122717\n",
            "Epoch: 90/100, Loss: 0.120045\n",
            "Epoch: 91/100, Loss: 0.117420\n",
            "Epoch: 92/100, Loss: 0.114843\n",
            "Epoch: 93/100, Loss: 0.112293\n",
            "Epoch: 94/100, Loss: 0.109776\n",
            "Epoch: 95/100, Loss: 0.107314\n",
            "Epoch: 96/100, Loss: 0.104904\n",
            "Epoch: 97/100, Loss: 0.102562\n",
            "Epoch: 98/100, Loss: 0.100274\n",
            "Epoch: 99/100, Loss: 0.097986\n",
            "Epoch: 100/100, Loss: 0.095704\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6958\n",
            "Normalised mutual info score on k-means on latent space: 0.6189956029543803\n",
            "ARI score on k-means on latent space: 0.5239676351969192\n",
            "K-means cluster error on latent space: 44987.83203125\n",
            "K-means silhouette score on latent space: 0.1794874 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6788\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6571936304781054 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.528392911003223 \n",
            "\n",
            "ROUND NUMBER  4 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.956206\n",
            "Epoch: 2/100, Loss: 0.753751\n",
            "Epoch: 3/100, Loss: 0.670172\n",
            "Epoch: 4/100, Loss: 0.629351\n",
            "Epoch: 5/100, Loss: 0.603199\n",
            "Epoch: 6/100, Loss: 0.582755\n",
            "Epoch: 7/100, Loss: 0.565660\n",
            "Epoch: 8/100, Loss: 0.550796\n",
            "Epoch: 9/100, Loss: 0.537561\n",
            "Epoch: 10/100, Loss: 0.525573\n",
            "Epoch: 11/100, Loss: 0.514491\n",
            "Epoch: 12/100, Loss: 0.504131\n",
            "Epoch: 13/100, Loss: 0.494360\n",
            "Epoch: 14/100, Loss: 0.485080\n",
            "Epoch: 15/100, Loss: 0.476203\n",
            "Epoch: 16/100, Loss: 0.467659\n",
            "Epoch: 17/100, Loss: 0.459415\n",
            "Epoch: 18/100, Loss: 0.451437\n",
            "Epoch: 19/100, Loss: 0.443697\n",
            "Epoch: 20/100, Loss: 0.436169\n",
            "Epoch: 21/100, Loss: 0.428835\n",
            "Epoch: 22/100, Loss: 0.421676\n",
            "Epoch: 23/100, Loss: 0.414677\n",
            "Epoch: 24/100, Loss: 0.407832\n",
            "Epoch: 25/100, Loss: 0.401118\n",
            "Epoch: 26/100, Loss: 0.394534\n",
            "Epoch: 27/100, Loss: 0.388074\n",
            "Epoch: 28/100, Loss: 0.381729\n",
            "Epoch: 29/100, Loss: 0.375497\n",
            "Epoch: 30/100, Loss: 0.369366\n",
            "Epoch: 31/100, Loss: 0.363335\n",
            "Epoch: 32/100, Loss: 0.357396\n",
            "Epoch: 33/100, Loss: 0.351550\n",
            "Epoch: 34/100, Loss: 0.345796\n",
            "Epoch: 35/100, Loss: 0.340129\n",
            "Epoch: 36/100, Loss: 0.334542\n",
            "Epoch: 37/100, Loss: 0.329037\n",
            "Epoch: 38/100, Loss: 0.323607\n",
            "Epoch: 39/100, Loss: 0.318251\n",
            "Epoch: 40/100, Loss: 0.312971\n",
            "Epoch: 41/100, Loss: 0.307760\n",
            "Epoch: 42/100, Loss: 0.302617\n",
            "Epoch: 43/100, Loss: 0.297540\n",
            "Epoch: 44/100, Loss: 0.292530\n",
            "Epoch: 45/100, Loss: 0.287583\n",
            "Epoch: 46/100, Loss: 0.282701\n",
            "Epoch: 47/100, Loss: 0.277878\n",
            "Epoch: 48/100, Loss: 0.273116\n",
            "Epoch: 49/100, Loss: 0.268415\n",
            "Epoch: 50/100, Loss: 0.263773\n",
            "Epoch: 51/100, Loss: 0.259191\n",
            "Epoch: 52/100, Loss: 0.254662\n",
            "Epoch: 53/100, Loss: 0.250194\n",
            "Epoch: 54/100, Loss: 0.245781\n",
            "Epoch: 55/100, Loss: 0.241418\n",
            "Epoch: 56/100, Loss: 0.237111\n",
            "Epoch: 57/100, Loss: 0.232856\n",
            "Epoch: 58/100, Loss: 0.228656\n",
            "Epoch: 59/100, Loss: 0.224509\n",
            "Epoch: 60/100, Loss: 0.220412\n",
            "Epoch: 61/100, Loss: 0.216368\n",
            "Epoch: 62/100, Loss: 0.212375\n",
            "Epoch: 63/100, Loss: 0.208430\n",
            "Epoch: 64/100, Loss: 0.204537\n",
            "Epoch: 65/100, Loss: 0.200695\n",
            "Epoch: 66/100, Loss: 0.196900\n",
            "Epoch: 67/100, Loss: 0.193156\n",
            "Epoch: 68/100, Loss: 0.189459\n",
            "Epoch: 69/100, Loss: 0.185814\n",
            "Epoch: 70/100, Loss: 0.182216\n",
            "Epoch: 71/100, Loss: 0.178664\n",
            "Epoch: 72/100, Loss: 0.175155\n",
            "Epoch: 73/100, Loss: 0.171694\n",
            "Epoch: 74/100, Loss: 0.168283\n",
            "Epoch: 75/100, Loss: 0.164918\n",
            "Epoch: 76/100, Loss: 0.161596\n",
            "Epoch: 77/100, Loss: 0.158325\n",
            "Epoch: 78/100, Loss: 0.155098\n",
            "Epoch: 79/100, Loss: 0.151915\n",
            "Epoch: 80/100, Loss: 0.148777\n",
            "Epoch: 81/100, Loss: 0.145684\n",
            "Epoch: 82/100, Loss: 0.142639\n",
            "Epoch: 83/100, Loss: 0.139641\n",
            "Epoch: 84/100, Loss: 0.136689\n",
            "Epoch: 85/100, Loss: 0.133786\n",
            "Epoch: 86/100, Loss: 0.130933\n",
            "Epoch: 87/100, Loss: 0.128131\n",
            "Epoch: 88/100, Loss: 0.125372\n",
            "Epoch: 89/100, Loss: 0.122650\n",
            "Epoch: 90/100, Loss: 0.119969\n",
            "Epoch: 91/100, Loss: 0.117335\n",
            "Epoch: 92/100, Loss: 0.114728\n",
            "Epoch: 93/100, Loss: 0.112160\n",
            "Epoch: 94/100, Loss: 0.109653\n",
            "Epoch: 95/100, Loss: 0.107185\n",
            "Epoch: 96/100, Loss: 0.104744\n",
            "Epoch: 97/100, Loss: 0.102337\n",
            "Epoch: 98/100, Loss: 0.099980\n",
            "Epoch: 99/100, Loss: 0.097657\n",
            "Epoch: 100/100, Loss: 0.095360\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6704\n",
            "Normalised mutual info score on k-means on latent space: 0.5942324218473327\n",
            "ARI score on k-means on latent space: 0.49081091636451024\n",
            "K-means cluster error on latent space: 45480.98828125\n",
            "K-means silhouette score on latent space: 0.18013042 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7066\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.7206262912017521 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5994196865843054 \n",
            "\n",
            "ROUND NUMBER  5 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.962942\n",
            "Epoch: 2/100, Loss: 0.771417\n",
            "Epoch: 3/100, Loss: 0.677491\n",
            "Epoch: 4/100, Loss: 0.626270\n",
            "Epoch: 5/100, Loss: 0.595920\n",
            "Epoch: 6/100, Loss: 0.575513\n",
            "Epoch: 7/100, Loss: 0.559548\n",
            "Epoch: 8/100, Loss: 0.545872\n",
            "Epoch: 9/100, Loss: 0.533619\n",
            "Epoch: 10/100, Loss: 0.522395\n",
            "Epoch: 11/100, Loss: 0.511949\n",
            "Epoch: 12/100, Loss: 0.502109\n",
            "Epoch: 13/100, Loss: 0.492749\n",
            "Epoch: 14/100, Loss: 0.483790\n",
            "Epoch: 15/100, Loss: 0.475158\n",
            "Epoch: 16/100, Loss: 0.466820\n",
            "Epoch: 17/100, Loss: 0.458732\n",
            "Epoch: 18/100, Loss: 0.450874\n",
            "Epoch: 19/100, Loss: 0.443220\n",
            "Epoch: 20/100, Loss: 0.435757\n",
            "Epoch: 21/100, Loss: 0.428472\n",
            "Epoch: 22/100, Loss: 0.421349\n",
            "Epoch: 23/100, Loss: 0.414369\n",
            "Epoch: 24/100, Loss: 0.407530\n",
            "Epoch: 25/100, Loss: 0.400827\n",
            "Epoch: 26/100, Loss: 0.394250\n",
            "Epoch: 27/100, Loss: 0.387794\n",
            "Epoch: 28/100, Loss: 0.381456\n",
            "Epoch: 29/100, Loss: 0.375225\n",
            "Epoch: 30/100, Loss: 0.369100\n",
            "Epoch: 31/100, Loss: 0.363072\n",
            "Epoch: 32/100, Loss: 0.357141\n",
            "Epoch: 33/100, Loss: 0.351302\n",
            "Epoch: 34/100, Loss: 0.345546\n",
            "Epoch: 35/100, Loss: 0.339874\n",
            "Epoch: 36/100, Loss: 0.334288\n",
            "Epoch: 37/100, Loss: 0.328786\n",
            "Epoch: 38/100, Loss: 0.323361\n",
            "Epoch: 39/100, Loss: 0.318010\n",
            "Epoch: 40/100, Loss: 0.312728\n",
            "Epoch: 41/100, Loss: 0.307521\n",
            "Epoch: 42/100, Loss: 0.302386\n",
            "Epoch: 43/100, Loss: 0.297315\n",
            "Epoch: 44/100, Loss: 0.292307\n",
            "Epoch: 45/100, Loss: 0.287367\n",
            "Epoch: 46/100, Loss: 0.282489\n",
            "Epoch: 47/100, Loss: 0.277669\n",
            "Epoch: 48/100, Loss: 0.272912\n",
            "Epoch: 49/100, Loss: 0.268218\n",
            "Epoch: 50/100, Loss: 0.263584\n",
            "Epoch: 51/100, Loss: 0.259008\n",
            "Epoch: 52/100, Loss: 0.254488\n",
            "Epoch: 53/100, Loss: 0.250017\n",
            "Epoch: 54/100, Loss: 0.245601\n",
            "Epoch: 55/100, Loss: 0.241248\n",
            "Epoch: 56/100, Loss: 0.236948\n",
            "Epoch: 57/100, Loss: 0.232701\n",
            "Epoch: 58/100, Loss: 0.228504\n",
            "Epoch: 59/100, Loss: 0.224359\n",
            "Epoch: 60/100, Loss: 0.220268\n",
            "Epoch: 61/100, Loss: 0.216229\n",
            "Epoch: 62/100, Loss: 0.212241\n",
            "Epoch: 63/100, Loss: 0.208305\n",
            "Epoch: 64/100, Loss: 0.204415\n",
            "Epoch: 65/100, Loss: 0.200576\n",
            "Epoch: 66/100, Loss: 0.196786\n",
            "Epoch: 67/100, Loss: 0.193048\n",
            "Epoch: 68/100, Loss: 0.189360\n",
            "Epoch: 69/100, Loss: 0.185721\n",
            "Epoch: 70/100, Loss: 0.182125\n",
            "Epoch: 71/100, Loss: 0.178580\n",
            "Epoch: 72/100, Loss: 0.175078\n",
            "Epoch: 73/100, Loss: 0.171625\n",
            "Epoch: 74/100, Loss: 0.168219\n",
            "Epoch: 75/100, Loss: 0.164863\n",
            "Epoch: 76/100, Loss: 0.161547\n",
            "Epoch: 77/100, Loss: 0.158281\n",
            "Epoch: 78/100, Loss: 0.155060\n",
            "Epoch: 79/100, Loss: 0.151890\n",
            "Epoch: 80/100, Loss: 0.148764\n",
            "Epoch: 81/100, Loss: 0.145685\n",
            "Epoch: 82/100, Loss: 0.142645\n",
            "Epoch: 83/100, Loss: 0.139650\n",
            "Epoch: 84/100, Loss: 0.136703\n",
            "Epoch: 85/100, Loss: 0.133794\n",
            "Epoch: 86/100, Loss: 0.130929\n",
            "Epoch: 87/100, Loss: 0.128112\n",
            "Epoch: 88/100, Loss: 0.125340\n",
            "Epoch: 89/100, Loss: 0.122623\n",
            "Epoch: 90/100, Loss: 0.119950\n",
            "Epoch: 91/100, Loss: 0.117326\n",
            "Epoch: 92/100, Loss: 0.114746\n",
            "Epoch: 93/100, Loss: 0.112195\n",
            "Epoch: 94/100, Loss: 0.109669\n",
            "Epoch: 95/100, Loss: 0.107179\n",
            "Epoch: 96/100, Loss: 0.104756\n",
            "Epoch: 97/100, Loss: 0.102398\n",
            "Epoch: 98/100, Loss: 0.100091\n",
            "Epoch: 99/100, Loss: 0.097812\n",
            "Epoch: 100/100, Loss: 0.095558\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.7141\n",
            "Normalised mutual info score on k-means on latent space: 0.638678451982731\n",
            "ARI score on k-means on latent space: 0.5542746742245371\n",
            "K-means cluster error on latent space: 42235.21875\n",
            "K-means silhouette score on latent space: 0.2001751 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7733\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.7376343784007698 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.647011781593313 \n",
            "\n",
            "ROUND NUMBER  6 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.928306\n",
            "Epoch: 2/100, Loss: 0.727655\n",
            "Epoch: 3/100, Loss: 0.653887\n",
            "Epoch: 4/100, Loss: 0.616102\n",
            "Epoch: 5/100, Loss: 0.590795\n",
            "Epoch: 6/100, Loss: 0.571747\n",
            "Epoch: 7/100, Loss: 0.556213\n",
            "Epoch: 8/100, Loss: 0.542783\n",
            "Epoch: 9/100, Loss: 0.530736\n",
            "Epoch: 10/100, Loss: 0.519675\n",
            "Epoch: 11/100, Loss: 0.509345\n",
            "Epoch: 12/100, Loss: 0.499578\n",
            "Epoch: 13/100, Loss: 0.490276\n",
            "Epoch: 14/100, Loss: 0.481359\n",
            "Epoch: 15/100, Loss: 0.472772\n",
            "Epoch: 16/100, Loss: 0.464473\n",
            "Epoch: 17/100, Loss: 0.456425\n",
            "Epoch: 18/100, Loss: 0.448601\n",
            "Epoch: 19/100, Loss: 0.440988\n",
            "Epoch: 20/100, Loss: 0.433572\n",
            "Epoch: 21/100, Loss: 0.426334\n",
            "Epoch: 22/100, Loss: 0.419251\n",
            "Epoch: 23/100, Loss: 0.412326\n",
            "Epoch: 24/100, Loss: 0.405546\n",
            "Epoch: 25/100, Loss: 0.398893\n",
            "Epoch: 26/100, Loss: 0.392365\n",
            "Epoch: 27/100, Loss: 0.385961\n",
            "Epoch: 28/100, Loss: 0.379667\n",
            "Epoch: 29/100, Loss: 0.373480\n",
            "Epoch: 30/100, Loss: 0.367393\n",
            "Epoch: 31/100, Loss: 0.361409\n",
            "Epoch: 32/100, Loss: 0.355513\n",
            "Epoch: 33/100, Loss: 0.349709\n",
            "Epoch: 34/100, Loss: 0.343993\n",
            "Epoch: 35/100, Loss: 0.338360\n",
            "Epoch: 36/100, Loss: 0.332806\n",
            "Epoch: 37/100, Loss: 0.327331\n",
            "Epoch: 38/100, Loss: 0.321934\n",
            "Epoch: 39/100, Loss: 0.316610\n",
            "Epoch: 40/100, Loss: 0.311355\n",
            "Epoch: 41/100, Loss: 0.306175\n",
            "Epoch: 42/100, Loss: 0.301063\n",
            "Epoch: 43/100, Loss: 0.296016\n",
            "Epoch: 44/100, Loss: 0.291032\n",
            "Epoch: 45/100, Loss: 0.286111\n",
            "Epoch: 46/100, Loss: 0.281252\n",
            "Epoch: 47/100, Loss: 0.276454\n",
            "Epoch: 48/100, Loss: 0.271718\n",
            "Epoch: 49/100, Loss: 0.267041\n",
            "Epoch: 50/100, Loss: 0.262419\n",
            "Epoch: 51/100, Loss: 0.257856\n",
            "Epoch: 52/100, Loss: 0.253352\n",
            "Epoch: 53/100, Loss: 0.248902\n",
            "Epoch: 54/100, Loss: 0.244509\n",
            "Epoch: 55/100, Loss: 0.240169\n",
            "Epoch: 56/100, Loss: 0.235882\n",
            "Epoch: 57/100, Loss: 0.231651\n",
            "Epoch: 58/100, Loss: 0.227472\n",
            "Epoch: 59/100, Loss: 0.223346\n",
            "Epoch: 60/100, Loss: 0.219272\n",
            "Epoch: 61/100, Loss: 0.215250\n",
            "Epoch: 62/100, Loss: 0.211278\n",
            "Epoch: 63/100, Loss: 0.207355\n",
            "Epoch: 64/100, Loss: 0.203483\n",
            "Epoch: 65/100, Loss: 0.199662\n",
            "Epoch: 66/100, Loss: 0.195891\n",
            "Epoch: 67/100, Loss: 0.192171\n",
            "Epoch: 68/100, Loss: 0.188499\n",
            "Epoch: 69/100, Loss: 0.184879\n",
            "Epoch: 70/100, Loss: 0.181300\n",
            "Epoch: 71/100, Loss: 0.177770\n",
            "Epoch: 72/100, Loss: 0.174288\n",
            "Epoch: 73/100, Loss: 0.170855\n",
            "Epoch: 74/100, Loss: 0.167467\n",
            "Epoch: 75/100, Loss: 0.164123\n",
            "Epoch: 76/100, Loss: 0.160830\n",
            "Epoch: 77/100, Loss: 0.157584\n",
            "Epoch: 78/100, Loss: 0.154391\n",
            "Epoch: 79/100, Loss: 0.151252\n",
            "Epoch: 80/100, Loss: 0.148165\n",
            "Epoch: 81/100, Loss: 0.145135\n",
            "Epoch: 82/100, Loss: 0.142167\n",
            "Epoch: 83/100, Loss: 0.139273\n",
            "Epoch: 84/100, Loss: 0.136394\n",
            "Epoch: 85/100, Loss: 0.133496\n",
            "Epoch: 86/100, Loss: 0.130593\n",
            "Epoch: 87/100, Loss: 0.127766\n",
            "Epoch: 88/100, Loss: 0.124972\n",
            "Epoch: 89/100, Loss: 0.122206\n",
            "Epoch: 90/100, Loss: 0.119506\n",
            "Epoch: 91/100, Loss: 0.116876\n",
            "Epoch: 92/100, Loss: 0.114292\n",
            "Epoch: 93/100, Loss: 0.111747\n",
            "Epoch: 94/100, Loss: 0.109246\n",
            "Epoch: 95/100, Loss: 0.106770\n",
            "Epoch: 96/100, Loss: 0.104313\n",
            "Epoch: 97/100, Loss: 0.101886\n",
            "Epoch: 98/100, Loss: 0.099505\n",
            "Epoch: 99/100, Loss: 0.097171\n",
            "Epoch: 100/100, Loss: 0.094877\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6853\n",
            "Normalised mutual info score on k-means on latent space: 0.6170401915750461\n",
            "ARI score on k-means on latent space: 0.5163422888301945\n",
            "K-means cluster error on latent space: 44881.6171875\n",
            "K-means silhouette score on latent space: 0.1915731 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7049\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.7070743531337057 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5757487868219926 \n",
            "\n",
            "ROUND NUMBER  7 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.929826\n",
            "Epoch: 2/100, Loss: 0.747659\n",
            "Epoch: 3/100, Loss: 0.662049\n",
            "Epoch: 4/100, Loss: 0.617079\n",
            "Epoch: 5/100, Loss: 0.589928\n",
            "Epoch: 6/100, Loss: 0.570654\n",
            "Epoch: 7/100, Loss: 0.555404\n",
            "Epoch: 8/100, Loss: 0.542280\n",
            "Epoch: 9/100, Loss: 0.530465\n",
            "Epoch: 10/100, Loss: 0.519583\n",
            "Epoch: 11/100, Loss: 0.509427\n",
            "Epoch: 12/100, Loss: 0.499830\n",
            "Epoch: 13/100, Loss: 0.490675\n",
            "Epoch: 14/100, Loss: 0.481867\n",
            "Epoch: 15/100, Loss: 0.473373\n",
            "Epoch: 16/100, Loss: 0.465148\n",
            "Epoch: 17/100, Loss: 0.457156\n",
            "Epoch: 18/100, Loss: 0.449383\n",
            "Epoch: 19/100, Loss: 0.441798\n",
            "Epoch: 20/100, Loss: 0.434389\n",
            "Epoch: 21/100, Loss: 0.427145\n",
            "Epoch: 22/100, Loss: 0.420061\n",
            "Epoch: 23/100, Loss: 0.413117\n",
            "Epoch: 24/100, Loss: 0.406315\n",
            "Epoch: 25/100, Loss: 0.399646\n",
            "Epoch: 26/100, Loss: 0.393098\n",
            "Epoch: 27/100, Loss: 0.386667\n",
            "Epoch: 28/100, Loss: 0.380349\n",
            "Epoch: 29/100, Loss: 0.374144\n",
            "Epoch: 30/100, Loss: 0.368038\n",
            "Epoch: 31/100, Loss: 0.362034\n",
            "Epoch: 32/100, Loss: 0.356123\n",
            "Epoch: 33/100, Loss: 0.350300\n",
            "Epoch: 34/100, Loss: 0.344567\n",
            "Epoch: 35/100, Loss: 0.338920\n",
            "Epoch: 36/100, Loss: 0.333357\n",
            "Epoch: 37/100, Loss: 0.327873\n",
            "Epoch: 38/100, Loss: 0.322463\n",
            "Epoch: 39/100, Loss: 0.317125\n",
            "Epoch: 40/100, Loss: 0.311862\n",
            "Epoch: 41/100, Loss: 0.306666\n",
            "Epoch: 42/100, Loss: 0.301539\n",
            "Epoch: 43/100, Loss: 0.296481\n",
            "Epoch: 44/100, Loss: 0.291486\n",
            "Epoch: 45/100, Loss: 0.286559\n",
            "Epoch: 46/100, Loss: 0.281692\n",
            "Epoch: 47/100, Loss: 0.276889\n",
            "Epoch: 48/100, Loss: 0.272144\n",
            "Epoch: 49/100, Loss: 0.267461\n",
            "Epoch: 50/100, Loss: 0.262835\n",
            "Epoch: 51/100, Loss: 0.258269\n",
            "Epoch: 52/100, Loss: 0.253760\n",
            "Epoch: 53/100, Loss: 0.249306\n",
            "Epoch: 54/100, Loss: 0.244908\n",
            "Epoch: 55/100, Loss: 0.240563\n",
            "Epoch: 56/100, Loss: 0.236276\n",
            "Epoch: 57/100, Loss: 0.232041\n",
            "Epoch: 58/100, Loss: 0.227860\n",
            "Epoch: 59/100, Loss: 0.223731\n",
            "Epoch: 60/100, Loss: 0.219654\n",
            "Epoch: 61/100, Loss: 0.215628\n",
            "Epoch: 62/100, Loss: 0.211653\n",
            "Epoch: 63/100, Loss: 0.207728\n",
            "Epoch: 64/100, Loss: 0.203854\n",
            "Epoch: 65/100, Loss: 0.200034\n",
            "Epoch: 66/100, Loss: 0.196259\n",
            "Epoch: 67/100, Loss: 0.192531\n",
            "Epoch: 68/100, Loss: 0.188853\n",
            "Epoch: 69/100, Loss: 0.185223\n",
            "Epoch: 70/100, Loss: 0.181641\n",
            "Epoch: 71/100, Loss: 0.178107\n",
            "Epoch: 72/100, Loss: 0.174619\n",
            "Epoch: 73/100, Loss: 0.171178\n",
            "Epoch: 74/100, Loss: 0.167782\n",
            "Epoch: 75/100, Loss: 0.164433\n",
            "Epoch: 76/100, Loss: 0.161129\n",
            "Epoch: 77/100, Loss: 0.157872\n",
            "Epoch: 78/100, Loss: 0.154660\n",
            "Epoch: 79/100, Loss: 0.151492\n",
            "Epoch: 80/100, Loss: 0.148368\n",
            "Epoch: 81/100, Loss: 0.145290\n",
            "Epoch: 82/100, Loss: 0.142259\n",
            "Epoch: 83/100, Loss: 0.139268\n",
            "Epoch: 84/100, Loss: 0.136321\n",
            "Epoch: 85/100, Loss: 0.133419\n",
            "Epoch: 86/100, Loss: 0.130565\n",
            "Epoch: 87/100, Loss: 0.127753\n",
            "Epoch: 88/100, Loss: 0.124989\n",
            "Epoch: 89/100, Loss: 0.122273\n",
            "Epoch: 90/100, Loss: 0.119605\n",
            "Epoch: 91/100, Loss: 0.116995\n",
            "Epoch: 92/100, Loss: 0.114436\n",
            "Epoch: 93/100, Loss: 0.111935\n",
            "Epoch: 94/100, Loss: 0.109498\n",
            "Epoch: 95/100, Loss: 0.107098\n",
            "Epoch: 96/100, Loss: 0.104701\n",
            "Epoch: 97/100, Loss: 0.102281\n",
            "Epoch: 98/100, Loss: 0.099829\n",
            "Epoch: 99/100, Loss: 0.097420\n",
            "Epoch: 100/100, Loss: 0.095083\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.7127\n",
            "Normalised mutual info score on k-means on latent space: 0.6264370497828098\n",
            "ARI score on k-means on latent space: 0.5465397038890217\n",
            "K-means cluster error on latent space: 42007.04296875\n",
            "K-means silhouette score on latent space: 0.18629932 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7514\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.7080753950210724 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.6111948885331853 \n",
            "\n",
            "ROUND NUMBER  8 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.944576\n",
            "Epoch: 2/100, Loss: 0.732852\n",
            "Epoch: 3/100, Loss: 0.647459\n",
            "Epoch: 4/100, Loss: 0.609177\n",
            "Epoch: 5/100, Loss: 0.585937\n",
            "Epoch: 6/100, Loss: 0.568531\n",
            "Epoch: 7/100, Loss: 0.554007\n",
            "Epoch: 8/100, Loss: 0.541250\n",
            "Epoch: 9/100, Loss: 0.529689\n",
            "Epoch: 10/100, Loss: 0.518962\n",
            "Epoch: 11/100, Loss: 0.508874\n",
            "Epoch: 12/100, Loss: 0.499292\n",
            "Epoch: 13/100, Loss: 0.490129\n",
            "Epoch: 14/100, Loss: 0.481326\n",
            "Epoch: 15/100, Loss: 0.472830\n",
            "Epoch: 16/100, Loss: 0.464601\n",
            "Epoch: 17/100, Loss: 0.456612\n",
            "Epoch: 18/100, Loss: 0.448839\n",
            "Epoch: 19/100, Loss: 0.441259\n",
            "Epoch: 20/100, Loss: 0.433864\n",
            "Epoch: 21/100, Loss: 0.426646\n",
            "Epoch: 22/100, Loss: 0.419584\n",
            "Epoch: 23/100, Loss: 0.412670\n",
            "Epoch: 24/100, Loss: 0.405898\n",
            "Epoch: 25/100, Loss: 0.399251\n",
            "Epoch: 26/100, Loss: 0.392725\n",
            "Epoch: 27/100, Loss: 0.386318\n",
            "Epoch: 28/100, Loss: 0.380025\n",
            "Epoch: 29/100, Loss: 0.373840\n",
            "Epoch: 30/100, Loss: 0.367757\n",
            "Epoch: 31/100, Loss: 0.361765\n",
            "Epoch: 32/100, Loss: 0.355874\n",
            "Epoch: 33/100, Loss: 0.350070\n",
            "Epoch: 34/100, Loss: 0.344351\n",
            "Epoch: 35/100, Loss: 0.338717\n",
            "Epoch: 36/100, Loss: 0.333162\n",
            "Epoch: 37/100, Loss: 0.327689\n",
            "Epoch: 38/100, Loss: 0.322290\n",
            "Epoch: 39/100, Loss: 0.316969\n",
            "Epoch: 40/100, Loss: 0.311718\n",
            "Epoch: 41/100, Loss: 0.306535\n",
            "Epoch: 42/100, Loss: 0.301422\n",
            "Epoch: 43/100, Loss: 0.296370\n",
            "Epoch: 44/100, Loss: 0.291385\n",
            "Epoch: 45/100, Loss: 0.286460\n",
            "Epoch: 46/100, Loss: 0.281600\n",
            "Epoch: 47/100, Loss: 0.276802\n",
            "Epoch: 48/100, Loss: 0.272064\n",
            "Epoch: 49/100, Loss: 0.267383\n",
            "Epoch: 50/100, Loss: 0.262762\n",
            "Epoch: 51/100, Loss: 0.258196\n",
            "Epoch: 52/100, Loss: 0.253688\n",
            "Epoch: 53/100, Loss: 0.249235\n",
            "Epoch: 54/100, Loss: 0.244839\n",
            "Epoch: 55/100, Loss: 0.240495\n",
            "Epoch: 56/100, Loss: 0.236206\n",
            "Epoch: 57/100, Loss: 0.231969\n",
            "Epoch: 58/100, Loss: 0.227781\n",
            "Epoch: 59/100, Loss: 0.223649\n",
            "Epoch: 60/100, Loss: 0.219568\n",
            "Epoch: 61/100, Loss: 0.215541\n",
            "Epoch: 62/100, Loss: 0.211562\n",
            "Epoch: 63/100, Loss: 0.207631\n",
            "Epoch: 64/100, Loss: 0.203750\n",
            "Epoch: 65/100, Loss: 0.199918\n",
            "Epoch: 66/100, Loss: 0.196136\n",
            "Epoch: 67/100, Loss: 0.192404\n",
            "Epoch: 68/100, Loss: 0.188720\n",
            "Epoch: 69/100, Loss: 0.185086\n",
            "Epoch: 70/100, Loss: 0.181496\n",
            "Epoch: 71/100, Loss: 0.177953\n",
            "Epoch: 72/100, Loss: 0.174456\n",
            "Epoch: 73/100, Loss: 0.171007\n",
            "Epoch: 74/100, Loss: 0.167604\n",
            "Epoch: 75/100, Loss: 0.164250\n",
            "Epoch: 76/100, Loss: 0.160942\n",
            "Epoch: 77/100, Loss: 0.157678\n",
            "Epoch: 78/100, Loss: 0.154461\n",
            "Epoch: 79/100, Loss: 0.151285\n",
            "Epoch: 80/100, Loss: 0.148151\n",
            "Epoch: 81/100, Loss: 0.145067\n",
            "Epoch: 82/100, Loss: 0.142032\n",
            "Epoch: 83/100, Loss: 0.139042\n",
            "Epoch: 84/100, Loss: 0.136103\n",
            "Epoch: 85/100, Loss: 0.133213\n",
            "Epoch: 86/100, Loss: 0.130376\n",
            "Epoch: 87/100, Loss: 0.127593\n",
            "Epoch: 88/100, Loss: 0.124858\n",
            "Epoch: 89/100, Loss: 0.122175\n",
            "Epoch: 90/100, Loss: 0.119526\n",
            "Epoch: 91/100, Loss: 0.116865\n",
            "Epoch: 92/100, Loss: 0.114215\n",
            "Epoch: 93/100, Loss: 0.111636\n",
            "Epoch: 94/100, Loss: 0.109128\n",
            "Epoch: 95/100, Loss: 0.106708\n",
            "Epoch: 96/100, Loss: 0.104313\n",
            "Epoch: 97/100, Loss: 0.101906\n",
            "Epoch: 98/100, Loss: 0.099509\n",
            "Epoch: 99/100, Loss: 0.097135\n",
            "Epoch: 100/100, Loss: 0.094804\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.7017\n",
            "Normalised mutual info score on k-means on latent space: 0.6193969062583002\n",
            "ARI score on k-means on latent space: 0.5349819211018443\n",
            "K-means cluster error on latent space: 44962.34375\n",
            "K-means silhouette score on latent space: 0.17630334 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7774\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.728329374707757 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.6432186031053774 \n",
            "\n",
            "ROUND NUMBER  9 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.936761\n",
            "Epoch: 2/100, Loss: 0.737555\n",
            "Epoch: 3/100, Loss: 0.654090\n",
            "Epoch: 4/100, Loss: 0.613427\n",
            "Epoch: 5/100, Loss: 0.588818\n",
            "Epoch: 6/100, Loss: 0.570832\n",
            "Epoch: 7/100, Loss: 0.556067\n",
            "Epoch: 8/100, Loss: 0.543160\n",
            "Epoch: 9/100, Loss: 0.531490\n",
            "Epoch: 10/100, Loss: 0.520697\n",
            "Epoch: 11/100, Loss: 0.510568\n",
            "Epoch: 12/100, Loss: 0.500969\n",
            "Epoch: 13/100, Loss: 0.491799\n",
            "Epoch: 14/100, Loss: 0.482988\n",
            "Epoch: 15/100, Loss: 0.474487\n",
            "Epoch: 16/100, Loss: 0.466253\n",
            "Epoch: 17/100, Loss: 0.458260\n",
            "Epoch: 18/100, Loss: 0.450480\n",
            "Epoch: 19/100, Loss: 0.442890\n",
            "Epoch: 20/100, Loss: 0.435479\n",
            "Epoch: 21/100, Loss: 0.428234\n",
            "Epoch: 22/100, Loss: 0.421144\n",
            "Epoch: 23/100, Loss: 0.414198\n",
            "Epoch: 24/100, Loss: 0.407389\n",
            "Epoch: 25/100, Loss: 0.400713\n",
            "Epoch: 26/100, Loss: 0.394161\n",
            "Epoch: 27/100, Loss: 0.387729\n",
            "Epoch: 28/100, Loss: 0.381408\n",
            "Epoch: 29/100, Loss: 0.375191\n",
            "Epoch: 30/100, Loss: 0.369075\n",
            "Epoch: 31/100, Loss: 0.363058\n",
            "Epoch: 32/100, Loss: 0.357131\n",
            "Epoch: 33/100, Loss: 0.351294\n",
            "Epoch: 34/100, Loss: 0.345547\n",
            "Epoch: 35/100, Loss: 0.339882\n",
            "Epoch: 36/100, Loss: 0.334294\n",
            "Epoch: 37/100, Loss: 0.328787\n",
            "Epoch: 38/100, Loss: 0.323356\n",
            "Epoch: 39/100, Loss: 0.318001\n",
            "Epoch: 40/100, Loss: 0.312718\n",
            "Epoch: 41/100, Loss: 0.307508\n",
            "Epoch: 42/100, Loss: 0.302362\n",
            "Epoch: 43/100, Loss: 0.297284\n",
            "Epoch: 44/100, Loss: 0.292269\n",
            "Epoch: 45/100, Loss: 0.287316\n",
            "Epoch: 46/100, Loss: 0.282426\n",
            "Epoch: 47/100, Loss: 0.277597\n",
            "Epoch: 48/100, Loss: 0.272830\n",
            "Epoch: 49/100, Loss: 0.268128\n",
            "Epoch: 50/100, Loss: 0.263479\n",
            "Epoch: 51/100, Loss: 0.258888\n",
            "Epoch: 52/100, Loss: 0.254355\n",
            "Epoch: 53/100, Loss: 0.249876\n",
            "Epoch: 54/100, Loss: 0.245453\n",
            "Epoch: 55/100, Loss: 0.241085\n",
            "Epoch: 56/100, Loss: 0.236771\n",
            "Epoch: 57/100, Loss: 0.232511\n",
            "Epoch: 58/100, Loss: 0.228305\n",
            "Epoch: 59/100, Loss: 0.224150\n",
            "Epoch: 60/100, Loss: 0.220048\n",
            "Epoch: 61/100, Loss: 0.215998\n",
            "Epoch: 62/100, Loss: 0.211994\n",
            "Epoch: 63/100, Loss: 0.208047\n",
            "Epoch: 64/100, Loss: 0.204149\n",
            "Epoch: 65/100, Loss: 0.200304\n",
            "Epoch: 66/100, Loss: 0.196507\n",
            "Epoch: 67/100, Loss: 0.192760\n",
            "Epoch: 68/100, Loss: 0.189063\n",
            "Epoch: 69/100, Loss: 0.185412\n",
            "Epoch: 70/100, Loss: 0.181813\n",
            "Epoch: 71/100, Loss: 0.178258\n",
            "Epoch: 72/100, Loss: 0.174753\n",
            "Epoch: 73/100, Loss: 0.171297\n",
            "Epoch: 74/100, Loss: 0.167887\n",
            "Epoch: 75/100, Loss: 0.164524\n",
            "Epoch: 76/100, Loss: 0.161210\n",
            "Epoch: 77/100, Loss: 0.157946\n",
            "Epoch: 78/100, Loss: 0.154742\n",
            "Epoch: 79/100, Loss: 0.151587\n",
            "Epoch: 80/100, Loss: 0.148478\n",
            "Epoch: 81/100, Loss: 0.145427\n",
            "Epoch: 82/100, Loss: 0.142445\n",
            "Epoch: 83/100, Loss: 0.139503\n",
            "Epoch: 84/100, Loss: 0.136590\n",
            "Epoch: 85/100, Loss: 0.133657\n",
            "Epoch: 86/100, Loss: 0.130710\n",
            "Epoch: 87/100, Loss: 0.127819\n",
            "Epoch: 88/100, Loss: 0.125000\n",
            "Epoch: 89/100, Loss: 0.122249\n",
            "Epoch: 90/100, Loss: 0.119563\n",
            "Epoch: 91/100, Loss: 0.116933\n",
            "Epoch: 92/100, Loss: 0.114343\n",
            "Epoch: 93/100, Loss: 0.111789\n",
            "Epoch: 94/100, Loss: 0.109284\n",
            "Epoch: 95/100, Loss: 0.106812\n",
            "Epoch: 96/100, Loss: 0.104377\n",
            "Epoch: 97/100, Loss: 0.101992\n",
            "Epoch: 98/100, Loss: 0.099635\n",
            "Epoch: 99/100, Loss: 0.097325\n",
            "Epoch: 100/100, Loss: 0.095045\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6975\n",
            "Normalised mutual info score on k-means on latent space: 0.6264710184809396\n",
            "ARI score on k-means on latent space: 0.538513877632981\n",
            "K-means cluster error on latent space: 43700.07421875\n",
            "K-means silhouette score on latent space: 0.178853 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7012\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6956999094793368 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5464763923039523 \n",
            "\n",
            "ROUND NUMBER  10 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.932805\n",
            "Epoch: 2/100, Loss: 0.738476\n",
            "Epoch: 3/100, Loss: 0.657189\n",
            "Epoch: 4/100, Loss: 0.616271\n",
            "Epoch: 5/100, Loss: 0.590400\n",
            "Epoch: 6/100, Loss: 0.571290\n",
            "Epoch: 7/100, Loss: 0.555869\n",
            "Epoch: 8/100, Loss: 0.542649\n",
            "Epoch: 9/100, Loss: 0.530840\n",
            "Epoch: 10/100, Loss: 0.519980\n",
            "Epoch: 11/100, Loss: 0.509812\n",
            "Epoch: 12/100, Loss: 0.500172\n",
            "Epoch: 13/100, Loss: 0.490953\n",
            "Epoch: 14/100, Loss: 0.482098\n",
            "Epoch: 15/100, Loss: 0.473554\n",
            "Epoch: 16/100, Loss: 0.465296\n",
            "Epoch: 17/100, Loss: 0.457284\n",
            "Epoch: 18/100, Loss: 0.449497\n",
            "Epoch: 19/100, Loss: 0.441911\n",
            "Epoch: 20/100, Loss: 0.434516\n",
            "Epoch: 21/100, Loss: 0.427285\n",
            "Epoch: 22/100, Loss: 0.420210\n",
            "Epoch: 23/100, Loss: 0.413284\n",
            "Epoch: 24/100, Loss: 0.406494\n",
            "Epoch: 25/100, Loss: 0.399839\n",
            "Epoch: 26/100, Loss: 0.393309\n",
            "Epoch: 27/100, Loss: 0.386893\n",
            "Epoch: 28/100, Loss: 0.380588\n",
            "Epoch: 29/100, Loss: 0.374389\n",
            "Epoch: 30/100, Loss: 0.368293\n",
            "Epoch: 31/100, Loss: 0.362296\n",
            "Epoch: 32/100, Loss: 0.356394\n",
            "Epoch: 33/100, Loss: 0.350582\n",
            "Epoch: 34/100, Loss: 0.344850\n",
            "Epoch: 35/100, Loss: 0.339205\n",
            "Epoch: 36/100, Loss: 0.333644\n",
            "Epoch: 37/100, Loss: 0.328157\n",
            "Epoch: 38/100, Loss: 0.322747\n",
            "Epoch: 39/100, Loss: 0.317414\n",
            "Epoch: 40/100, Loss: 0.312151\n",
            "Epoch: 41/100, Loss: 0.306962\n",
            "Epoch: 42/100, Loss: 0.301839\n",
            "Epoch: 43/100, Loss: 0.296784\n",
            "Epoch: 44/100, Loss: 0.291791\n",
            "Epoch: 45/100, Loss: 0.286863\n",
            "Epoch: 46/100, Loss: 0.281998\n",
            "Epoch: 47/100, Loss: 0.277192\n",
            "Epoch: 48/100, Loss: 0.272448\n",
            "Epoch: 49/100, Loss: 0.267761\n",
            "Epoch: 50/100, Loss: 0.263135\n",
            "Epoch: 51/100, Loss: 0.258569\n",
            "Epoch: 52/100, Loss: 0.254055\n",
            "Epoch: 53/100, Loss: 0.249597\n",
            "Epoch: 54/100, Loss: 0.245197\n",
            "Epoch: 55/100, Loss: 0.240849\n",
            "Epoch: 56/100, Loss: 0.236554\n",
            "Epoch: 57/100, Loss: 0.232314\n",
            "Epoch: 58/100, Loss: 0.228127\n",
            "Epoch: 59/100, Loss: 0.223993\n",
            "Epoch: 60/100, Loss: 0.219912\n",
            "Epoch: 61/100, Loss: 0.215881\n",
            "Epoch: 62/100, Loss: 0.211903\n",
            "Epoch: 63/100, Loss: 0.207974\n",
            "Epoch: 64/100, Loss: 0.204096\n",
            "Epoch: 65/100, Loss: 0.200267\n",
            "Epoch: 66/100, Loss: 0.196485\n",
            "Epoch: 67/100, Loss: 0.192748\n",
            "Epoch: 68/100, Loss: 0.189060\n",
            "Epoch: 69/100, Loss: 0.185422\n",
            "Epoch: 70/100, Loss: 0.181831\n",
            "Epoch: 71/100, Loss: 0.178288\n",
            "Epoch: 72/100, Loss: 0.174793\n",
            "Epoch: 73/100, Loss: 0.171344\n",
            "Epoch: 74/100, Loss: 0.167944\n",
            "Epoch: 75/100, Loss: 0.164587\n",
            "Epoch: 76/100, Loss: 0.161276\n",
            "Epoch: 77/100, Loss: 0.158012\n",
            "Epoch: 78/100, Loss: 0.154794\n",
            "Epoch: 79/100, Loss: 0.151623\n",
            "Epoch: 80/100, Loss: 0.148498\n",
            "Epoch: 81/100, Loss: 0.145420\n",
            "Epoch: 82/100, Loss: 0.142394\n",
            "Epoch: 83/100, Loss: 0.139417\n",
            "Epoch: 84/100, Loss: 0.136490\n",
            "Epoch: 85/100, Loss: 0.133614\n",
            "Epoch: 86/100, Loss: 0.130787\n",
            "Epoch: 87/100, Loss: 0.128003\n",
            "Epoch: 88/100, Loss: 0.125231\n",
            "Epoch: 89/100, Loss: 0.122502\n",
            "Epoch: 90/100, Loss: 0.119826\n",
            "Epoch: 91/100, Loss: 0.117189\n",
            "Epoch: 92/100, Loss: 0.114571\n",
            "Epoch: 93/100, Loss: 0.111961\n",
            "Epoch: 94/100, Loss: 0.109394\n",
            "Epoch: 95/100, Loss: 0.106883\n",
            "Epoch: 96/100, Loss: 0.104434\n",
            "Epoch: 97/100, Loss: 0.102054\n",
            "Epoch: 98/100, Loss: 0.099732\n",
            "Epoch: 99/100, Loss: 0.097473\n",
            "Epoch: 100/100, Loss: 0.095267\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.664\n",
            "Normalised mutual info score on k-means on latent space: 0.6018250915026528\n",
            "ARI score on k-means on latent space: 0.4966933336540073\n",
            "K-means cluster error on latent space: 43102.35546875\n",
            "K-means silhouette score on latent space: 0.19975345 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7163\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.7068101768733449 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5962681469547808 \n",
            "\n",
            "Average k-means accuracy score at latent space: 0.6940999999999999 \n",
            "\n",
            "Average k-means NMI score at latent space: 0.6152132920967299 \n",
            "\n",
            "Average k-means ARI score at latent space: 0.5243755512758658 \n",
            "\n",
            "Average agglomerative clustering accuracy score at latent space: 0.71821 \n",
            "\n",
            "Average agglomerative clustering NMI score at latent space: 0.6975757770315912 \n",
            "\n",
            "Average agglomerative clustering ARI score at latent space: 0.5779564707810451 \n",
            "\n",
            "Average k-means silhouette score on latent space: 0.1871323585510254 \n",
            "\n",
            "Average k-means cluster error on latent space: 43881.3796875 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_for_k_100 = run_experiment(100, dataloader, 784, datapoints = images, datapoints_reshaped = images_reshaped, labels = labels, epochs=100, hl1_neurons=32, hl2_neurons=64, hl3_neurons=128, latent_dimension=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_co3C7n7ypS",
        "outputId": "584eca95-f52e-44e4-964e-a615650a1445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment results for k-means with k = 100 clusters:\n",
            "\n",
            "Running k-means algorithm in order to get our pseudolabels: \n",
            "\n",
            "K_means greedy accuracy score (initial space): 0.8529\n",
            "Normalised mutual info score (initial space): 0.7435490582432477\n",
            "ARI (initial space): 0.7149857876358806 \n",
            "\n",
            "Using the autoencoder model on our data: \n",
            "\n",
            "ROUND NUMBER  1 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.938258\n",
            "Epoch: 2/100, Loss: 0.731635\n",
            "Epoch: 3/100, Loss: 0.658476\n",
            "Epoch: 4/100, Loss: 0.621519\n",
            "Epoch: 5/100, Loss: 0.598092\n",
            "Epoch: 6/100, Loss: 0.580176\n",
            "Epoch: 7/100, Loss: 0.564732\n",
            "Epoch: 8/100, Loss: 0.550781\n",
            "Epoch: 9/100, Loss: 0.538032\n",
            "Epoch: 10/100, Loss: 0.526317\n",
            "Epoch: 11/100, Loss: 0.515404\n",
            "Epoch: 12/100, Loss: 0.505118\n",
            "Epoch: 13/100, Loss: 0.495343\n",
            "Epoch: 14/100, Loss: 0.486013\n",
            "Epoch: 15/100, Loss: 0.477039\n",
            "Epoch: 16/100, Loss: 0.468405\n",
            "Epoch: 17/100, Loss: 0.460060\n",
            "Epoch: 18/100, Loss: 0.451978\n",
            "Epoch: 19/100, Loss: 0.444131\n",
            "Epoch: 20/100, Loss: 0.436505\n",
            "Epoch: 21/100, Loss: 0.429076\n",
            "Epoch: 22/100, Loss: 0.421822\n",
            "Epoch: 23/100, Loss: 0.414741\n",
            "Epoch: 24/100, Loss: 0.407822\n",
            "Epoch: 25/100, Loss: 0.401046\n",
            "Epoch: 26/100, Loss: 0.394406\n",
            "Epoch: 27/100, Loss: 0.387900\n",
            "Epoch: 28/100, Loss: 0.381509\n",
            "Epoch: 29/100, Loss: 0.375237\n",
            "Epoch: 30/100, Loss: 0.369075\n",
            "Epoch: 31/100, Loss: 0.363017\n",
            "Epoch: 32/100, Loss: 0.357056\n",
            "Epoch: 33/100, Loss: 0.351192\n",
            "Epoch: 34/100, Loss: 0.345416\n",
            "Epoch: 35/100, Loss: 0.339726\n",
            "Epoch: 36/100, Loss: 0.334122\n",
            "Epoch: 37/100, Loss: 0.328597\n",
            "Epoch: 38/100, Loss: 0.323152\n",
            "Epoch: 39/100, Loss: 0.317788\n",
            "Epoch: 40/100, Loss: 0.312501\n",
            "Epoch: 41/100, Loss: 0.307281\n",
            "Epoch: 42/100, Loss: 0.302131\n",
            "Epoch: 43/100, Loss: 0.297047\n",
            "Epoch: 44/100, Loss: 0.292033\n",
            "Epoch: 45/100, Loss: 0.287085\n",
            "Epoch: 46/100, Loss: 0.282201\n",
            "Epoch: 47/100, Loss: 0.277375\n",
            "Epoch: 48/100, Loss: 0.272616\n",
            "Epoch: 49/100, Loss: 0.267920\n",
            "Epoch: 50/100, Loss: 0.263279\n",
            "Epoch: 51/100, Loss: 0.258696\n",
            "Epoch: 52/100, Loss: 0.254170\n",
            "Epoch: 53/100, Loss: 0.249702\n",
            "Epoch: 54/100, Loss: 0.245288\n",
            "Epoch: 55/100, Loss: 0.240929\n",
            "Epoch: 56/100, Loss: 0.236626\n",
            "Epoch: 57/100, Loss: 0.232376\n",
            "Epoch: 58/100, Loss: 0.228178\n",
            "Epoch: 59/100, Loss: 0.224034\n",
            "Epoch: 60/100, Loss: 0.219941\n",
            "Epoch: 61/100, Loss: 0.215905\n",
            "Epoch: 62/100, Loss: 0.211919\n",
            "Epoch: 63/100, Loss: 0.207985\n",
            "Epoch: 64/100, Loss: 0.204100\n",
            "Epoch: 65/100, Loss: 0.200260\n",
            "Epoch: 66/100, Loss: 0.196474\n",
            "Epoch: 67/100, Loss: 0.192735\n",
            "Epoch: 68/100, Loss: 0.189044\n",
            "Epoch: 69/100, Loss: 0.185402\n",
            "Epoch: 70/100, Loss: 0.181809\n",
            "Epoch: 71/100, Loss: 0.178267\n",
            "Epoch: 72/100, Loss: 0.174773\n",
            "Epoch: 73/100, Loss: 0.171323\n",
            "Epoch: 74/100, Loss: 0.167918\n",
            "Epoch: 75/100, Loss: 0.164563\n",
            "Epoch: 76/100, Loss: 0.161252\n",
            "Epoch: 77/100, Loss: 0.157987\n",
            "Epoch: 78/100, Loss: 0.154768\n",
            "Epoch: 79/100, Loss: 0.151593\n",
            "Epoch: 80/100, Loss: 0.148465\n",
            "Epoch: 81/100, Loss: 0.145379\n",
            "Epoch: 82/100, Loss: 0.142336\n",
            "Epoch: 83/100, Loss: 0.139346\n",
            "Epoch: 84/100, Loss: 0.136411\n",
            "Epoch: 85/100, Loss: 0.133527\n",
            "Epoch: 86/100, Loss: 0.130702\n",
            "Epoch: 87/100, Loss: 0.127923\n",
            "Epoch: 88/100, Loss: 0.125161\n",
            "Epoch: 89/100, Loss: 0.122418\n",
            "Epoch: 90/100, Loss: 0.119702\n",
            "Epoch: 91/100, Loss: 0.117033\n",
            "Epoch: 92/100, Loss: 0.114411\n",
            "Epoch: 93/100, Loss: 0.111845\n",
            "Epoch: 94/100, Loss: 0.109335\n",
            "Epoch: 95/100, Loss: 0.106873\n",
            "Epoch: 96/100, Loss: 0.104462\n",
            "Epoch: 97/100, Loss: 0.102115\n",
            "Epoch: 98/100, Loss: 0.099820\n",
            "Epoch: 99/100, Loss: 0.097570\n",
            "Epoch: 100/100, Loss: 0.095385\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6973\n",
            "Normalised mutual info score on k-means on latent space: 0.6427167740588328\n",
            "ARI score on k-means on latent space: 0.5371254364786445\n",
            "K-means cluster error on latent space: 44116.07421875\n",
            "K-means silhouette score on latent space: 0.1786336 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6637\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6768189303335383 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5328031746230376 \n",
            "\n",
            "ROUND NUMBER  2 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.909432\n",
            "Epoch: 2/100, Loss: 0.710674\n",
            "Epoch: 3/100, Loss: 0.643577\n",
            "Epoch: 4/100, Loss: 0.609560\n",
            "Epoch: 5/100, Loss: 0.587295\n",
            "Epoch: 6/100, Loss: 0.570198\n",
            "Epoch: 7/100, Loss: 0.555785\n",
            "Epoch: 8/100, Loss: 0.543015\n",
            "Epoch: 9/100, Loss: 0.531378\n",
            "Epoch: 10/100, Loss: 0.520560\n",
            "Epoch: 11/100, Loss: 0.510392\n",
            "Epoch: 12/100, Loss: 0.500734\n",
            "Epoch: 13/100, Loss: 0.491495\n",
            "Epoch: 14/100, Loss: 0.482616\n",
            "Epoch: 15/100, Loss: 0.474047\n",
            "Epoch: 16/100, Loss: 0.465748\n",
            "Epoch: 17/100, Loss: 0.457681\n",
            "Epoch: 18/100, Loss: 0.449838\n",
            "Epoch: 19/100, Loss: 0.442200\n",
            "Epoch: 20/100, Loss: 0.434738\n",
            "Epoch: 21/100, Loss: 0.427451\n",
            "Epoch: 22/100, Loss: 0.420325\n",
            "Epoch: 23/100, Loss: 0.413348\n",
            "Epoch: 24/100, Loss: 0.406517\n",
            "Epoch: 25/100, Loss: 0.399819\n",
            "Epoch: 26/100, Loss: 0.393256\n",
            "Epoch: 27/100, Loss: 0.386810\n",
            "Epoch: 28/100, Loss: 0.380478\n",
            "Epoch: 29/100, Loss: 0.374255\n",
            "Epoch: 30/100, Loss: 0.368132\n",
            "Epoch: 31/100, Loss: 0.362114\n",
            "Epoch: 32/100, Loss: 0.356187\n",
            "Epoch: 33/100, Loss: 0.350355\n",
            "Epoch: 34/100, Loss: 0.344613\n",
            "Epoch: 35/100, Loss: 0.338954\n",
            "Epoch: 36/100, Loss: 0.333382\n",
            "Epoch: 37/100, Loss: 0.327887\n",
            "Epoch: 38/100, Loss: 0.322473\n",
            "Epoch: 39/100, Loss: 0.317131\n",
            "Epoch: 40/100, Loss: 0.311866\n",
            "Epoch: 41/100, Loss: 0.306667\n",
            "Epoch: 42/100, Loss: 0.301540\n",
            "Epoch: 43/100, Loss: 0.296481\n",
            "Epoch: 44/100, Loss: 0.291485\n",
            "Epoch: 45/100, Loss: 0.286552\n",
            "Epoch: 46/100, Loss: 0.281680\n",
            "Epoch: 47/100, Loss: 0.276871\n",
            "Epoch: 48/100, Loss: 0.272123\n",
            "Epoch: 49/100, Loss: 0.267437\n",
            "Epoch: 50/100, Loss: 0.262809\n",
            "Epoch: 51/100, Loss: 0.258238\n",
            "Epoch: 52/100, Loss: 0.253723\n",
            "Epoch: 53/100, Loss: 0.249264\n",
            "Epoch: 54/100, Loss: 0.244863\n",
            "Epoch: 55/100, Loss: 0.240515\n",
            "Epoch: 56/100, Loss: 0.236221\n",
            "Epoch: 57/100, Loss: 0.231983\n",
            "Epoch: 58/100, Loss: 0.227793\n",
            "Epoch: 59/100, Loss: 0.223660\n",
            "Epoch: 60/100, Loss: 0.219580\n",
            "Epoch: 61/100, Loss: 0.215548\n",
            "Epoch: 62/100, Loss: 0.211570\n",
            "Epoch: 63/100, Loss: 0.207639\n",
            "Epoch: 64/100, Loss: 0.203760\n",
            "Epoch: 65/100, Loss: 0.199930\n",
            "Epoch: 66/100, Loss: 0.196150\n",
            "Epoch: 67/100, Loss: 0.192416\n",
            "Epoch: 68/100, Loss: 0.188732\n",
            "Epoch: 69/100, Loss: 0.185097\n",
            "Epoch: 70/100, Loss: 0.181511\n",
            "Epoch: 71/100, Loss: 0.177972\n",
            "Epoch: 72/100, Loss: 0.174480\n",
            "Epoch: 73/100, Loss: 0.171031\n",
            "Epoch: 74/100, Loss: 0.167628\n",
            "Epoch: 75/100, Loss: 0.164275\n",
            "Epoch: 76/100, Loss: 0.160971\n",
            "Epoch: 77/100, Loss: 0.157713\n",
            "Epoch: 78/100, Loss: 0.154507\n",
            "Epoch: 79/100, Loss: 0.151350\n",
            "Epoch: 80/100, Loss: 0.148246\n",
            "Epoch: 81/100, Loss: 0.145183\n",
            "Epoch: 82/100, Loss: 0.142166\n",
            "Epoch: 83/100, Loss: 0.139198\n",
            "Epoch: 84/100, Loss: 0.136278\n",
            "Epoch: 85/100, Loss: 0.133409\n",
            "Epoch: 86/100, Loss: 0.130613\n",
            "Epoch: 87/100, Loss: 0.127891\n",
            "Epoch: 88/100, Loss: 0.125192\n",
            "Epoch: 89/100, Loss: 0.122506\n",
            "Epoch: 90/100, Loss: 0.119726\n",
            "Epoch: 91/100, Loss: 0.116944\n",
            "Epoch: 92/100, Loss: 0.114263\n",
            "Epoch: 93/100, Loss: 0.111661\n",
            "Epoch: 94/100, Loss: 0.109115\n",
            "Epoch: 95/100, Loss: 0.106629\n",
            "Epoch: 96/100, Loss: 0.104201\n",
            "Epoch: 97/100, Loss: 0.101827\n",
            "Epoch: 98/100, Loss: 0.099497\n",
            "Epoch: 99/100, Loss: 0.097200\n",
            "Epoch: 100/100, Loss: 0.094951\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.7053\n",
            "Normalised mutual info score on k-means on latent space: 0.6257064016686364\n",
            "ARI score on k-means on latent space: 0.5444179830278748\n",
            "K-means cluster error on latent space: 44637.6484375\n",
            "K-means silhouette score on latent space: 0.18292966 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6906\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6804845731527168 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5779845779999349 \n",
            "\n",
            "ROUND NUMBER  3 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.925717\n",
            "Epoch: 2/100, Loss: 0.725449\n",
            "Epoch: 3/100, Loss: 0.646426\n",
            "Epoch: 4/100, Loss: 0.610913\n",
            "Epoch: 5/100, Loss: 0.587963\n",
            "Epoch: 6/100, Loss: 0.570295\n",
            "Epoch: 7/100, Loss: 0.555570\n",
            "Epoch: 8/100, Loss: 0.542615\n",
            "Epoch: 9/100, Loss: 0.530859\n",
            "Epoch: 10/100, Loss: 0.520015\n",
            "Epoch: 11/100, Loss: 0.509832\n",
            "Epoch: 12/100, Loss: 0.500171\n",
            "Epoch: 13/100, Loss: 0.490955\n",
            "Epoch: 14/100, Loss: 0.482105\n",
            "Epoch: 15/100, Loss: 0.473571\n",
            "Epoch: 16/100, Loss: 0.465312\n",
            "Epoch: 17/100, Loss: 0.457298\n",
            "Epoch: 18/100, Loss: 0.449496\n",
            "Epoch: 19/100, Loss: 0.441890\n",
            "Epoch: 20/100, Loss: 0.434463\n",
            "Epoch: 21/100, Loss: 0.427209\n",
            "Epoch: 22/100, Loss: 0.420109\n",
            "Epoch: 23/100, Loss: 0.413160\n",
            "Epoch: 24/100, Loss: 0.406348\n",
            "Epoch: 25/100, Loss: 0.399673\n",
            "Epoch: 26/100, Loss: 0.393124\n",
            "Epoch: 27/100, Loss: 0.386689\n",
            "Epoch: 28/100, Loss: 0.380370\n",
            "Epoch: 29/100, Loss: 0.374163\n",
            "Epoch: 30/100, Loss: 0.368056\n",
            "Epoch: 31/100, Loss: 0.362050\n",
            "Epoch: 32/100, Loss: 0.356138\n",
            "Epoch: 33/100, Loss: 0.350321\n",
            "Epoch: 34/100, Loss: 0.344589\n",
            "Epoch: 35/100, Loss: 0.338938\n",
            "Epoch: 36/100, Loss: 0.333369\n",
            "Epoch: 37/100, Loss: 0.327877\n",
            "Epoch: 38/100, Loss: 0.322458\n",
            "Epoch: 39/100, Loss: 0.317120\n",
            "Epoch: 40/100, Loss: 0.311854\n",
            "Epoch: 41/100, Loss: 0.306659\n",
            "Epoch: 42/100, Loss: 0.301535\n",
            "Epoch: 43/100, Loss: 0.296475\n",
            "Epoch: 44/100, Loss: 0.291477\n",
            "Epoch: 45/100, Loss: 0.286544\n",
            "Epoch: 46/100, Loss: 0.281671\n",
            "Epoch: 47/100, Loss: 0.276863\n",
            "Epoch: 48/100, Loss: 0.272113\n",
            "Epoch: 49/100, Loss: 0.267421\n",
            "Epoch: 50/100, Loss: 0.262790\n",
            "Epoch: 51/100, Loss: 0.258217\n",
            "Epoch: 52/100, Loss: 0.253702\n",
            "Epoch: 53/100, Loss: 0.249244\n",
            "Epoch: 54/100, Loss: 0.244842\n",
            "Epoch: 55/100, Loss: 0.240494\n",
            "Epoch: 56/100, Loss: 0.236198\n",
            "Epoch: 57/100, Loss: 0.231958\n",
            "Epoch: 58/100, Loss: 0.227770\n",
            "Epoch: 59/100, Loss: 0.223634\n",
            "Epoch: 60/100, Loss: 0.219549\n",
            "Epoch: 61/100, Loss: 0.215517\n",
            "Epoch: 62/100, Loss: 0.211535\n",
            "Epoch: 63/100, Loss: 0.207602\n",
            "Epoch: 64/100, Loss: 0.203723\n",
            "Epoch: 65/100, Loss: 0.199892\n",
            "Epoch: 66/100, Loss: 0.196115\n",
            "Epoch: 67/100, Loss: 0.192387\n",
            "Epoch: 68/100, Loss: 0.188709\n",
            "Epoch: 69/100, Loss: 0.185080\n",
            "Epoch: 70/100, Loss: 0.181497\n",
            "Epoch: 71/100, Loss: 0.177959\n",
            "Epoch: 72/100, Loss: 0.174466\n",
            "Epoch: 73/100, Loss: 0.171020\n",
            "Epoch: 74/100, Loss: 0.167620\n",
            "Epoch: 75/100, Loss: 0.164269\n",
            "Epoch: 76/100, Loss: 0.160965\n",
            "Epoch: 77/100, Loss: 0.157711\n",
            "Epoch: 78/100, Loss: 0.154507\n",
            "Epoch: 79/100, Loss: 0.151356\n",
            "Epoch: 80/100, Loss: 0.148252\n",
            "Epoch: 81/100, Loss: 0.145196\n",
            "Epoch: 82/100, Loss: 0.142182\n",
            "Epoch: 83/100, Loss: 0.139227\n",
            "Epoch: 84/100, Loss: 0.136320\n",
            "Epoch: 85/100, Loss: 0.133465\n",
            "Epoch: 86/100, Loss: 0.130640\n",
            "Epoch: 87/100, Loss: 0.127819\n",
            "Epoch: 88/100, Loss: 0.125030\n",
            "Epoch: 89/100, Loss: 0.122285\n",
            "Epoch: 90/100, Loss: 0.119596\n",
            "Epoch: 91/100, Loss: 0.116967\n",
            "Epoch: 92/100, Loss: 0.114387\n",
            "Epoch: 93/100, Loss: 0.111855\n",
            "Epoch: 94/100, Loss: 0.109360\n",
            "Epoch: 95/100, Loss: 0.106906\n",
            "Epoch: 96/100, Loss: 0.104496\n",
            "Epoch: 97/100, Loss: 0.102132\n",
            "Epoch: 98/100, Loss: 0.099831\n",
            "Epoch: 99/100, Loss: 0.097524\n",
            "Epoch: 100/100, Loss: 0.095270\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6865\n",
            "Normalised mutual info score on k-means on latent space: 0.5930227847970669\n",
            "ARI score on k-means on latent space: 0.5085124672152066\n",
            "K-means cluster error on latent space: 43810.32421875\n",
            "K-means silhouette score on latent space: 0.18076597 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7441\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.724878096528535 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.6127281342882926 \n",
            "\n",
            "ROUND NUMBER  4 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.898666\n",
            "Epoch: 2/100, Loss: 0.722885\n",
            "Epoch: 3/100, Loss: 0.650560\n",
            "Epoch: 4/100, Loss: 0.610380\n",
            "Epoch: 5/100, Loss: 0.585887\n",
            "Epoch: 6/100, Loss: 0.568001\n",
            "Epoch: 7/100, Loss: 0.553429\n",
            "Epoch: 8/100, Loss: 0.540814\n",
            "Epoch: 9/100, Loss: 0.529402\n",
            "Epoch: 10/100, Loss: 0.518818\n",
            "Epoch: 11/100, Loss: 0.508852\n",
            "Epoch: 12/100, Loss: 0.499383\n",
            "Epoch: 13/100, Loss: 0.490295\n",
            "Epoch: 14/100, Loss: 0.481541\n",
            "Epoch: 15/100, Loss: 0.473081\n",
            "Epoch: 16/100, Loss: 0.464882\n",
            "Epoch: 17/100, Loss: 0.456922\n",
            "Epoch: 18/100, Loss: 0.449171\n",
            "Epoch: 19/100, Loss: 0.441614\n",
            "Epoch: 20/100, Loss: 0.434240\n",
            "Epoch: 21/100, Loss: 0.427030\n",
            "Epoch: 22/100, Loss: 0.419979\n",
            "Epoch: 23/100, Loss: 0.413076\n",
            "Epoch: 24/100, Loss: 0.406308\n",
            "Epoch: 25/100, Loss: 0.399669\n",
            "Epoch: 26/100, Loss: 0.393153\n",
            "Epoch: 27/100, Loss: 0.386749\n",
            "Epoch: 28/100, Loss: 0.380460\n",
            "Epoch: 29/100, Loss: 0.374272\n",
            "Epoch: 30/100, Loss: 0.368187\n",
            "Epoch: 31/100, Loss: 0.362196\n",
            "Epoch: 32/100, Loss: 0.356297\n",
            "Epoch: 33/100, Loss: 0.350490\n",
            "Epoch: 34/100, Loss: 0.344768\n",
            "Epoch: 35/100, Loss: 0.339127\n",
            "Epoch: 36/100, Loss: 0.333566\n",
            "Epoch: 37/100, Loss: 0.328083\n",
            "Epoch: 38/100, Loss: 0.322676\n",
            "Epoch: 39/100, Loss: 0.317341\n",
            "Epoch: 40/100, Loss: 0.312078\n",
            "Epoch: 41/100, Loss: 0.306883\n",
            "Epoch: 42/100, Loss: 0.301756\n",
            "Epoch: 43/100, Loss: 0.296697\n",
            "Epoch: 44/100, Loss: 0.291702\n",
            "Epoch: 45/100, Loss: 0.286771\n",
            "Epoch: 46/100, Loss: 0.281904\n",
            "Epoch: 47/100, Loss: 0.277096\n",
            "Epoch: 48/100, Loss: 0.272354\n",
            "Epoch: 49/100, Loss: 0.267670\n",
            "Epoch: 50/100, Loss: 0.263045\n",
            "Epoch: 51/100, Loss: 0.258473\n",
            "Epoch: 52/100, Loss: 0.253958\n",
            "Epoch: 53/100, Loss: 0.249503\n",
            "Epoch: 54/100, Loss: 0.245101\n",
            "Epoch: 55/100, Loss: 0.240756\n",
            "Epoch: 56/100, Loss: 0.236465\n",
            "Epoch: 57/100, Loss: 0.232226\n",
            "Epoch: 58/100, Loss: 0.228039\n",
            "Epoch: 59/100, Loss: 0.223907\n",
            "Epoch: 60/100, Loss: 0.219828\n",
            "Epoch: 61/100, Loss: 0.215800\n",
            "Epoch: 62/100, Loss: 0.211826\n",
            "Epoch: 63/100, Loss: 0.207901\n",
            "Epoch: 64/100, Loss: 0.204025\n",
            "Epoch: 65/100, Loss: 0.200197\n",
            "Epoch: 66/100, Loss: 0.196417\n",
            "Epoch: 67/100, Loss: 0.192688\n",
            "Epoch: 68/100, Loss: 0.189008\n",
            "Epoch: 69/100, Loss: 0.185373\n",
            "Epoch: 70/100, Loss: 0.181788\n",
            "Epoch: 71/100, Loss: 0.178252\n",
            "Epoch: 72/100, Loss: 0.174760\n",
            "Epoch: 73/100, Loss: 0.171318\n",
            "Epoch: 74/100, Loss: 0.167929\n",
            "Epoch: 75/100, Loss: 0.164586\n",
            "Epoch: 76/100, Loss: 0.161291\n",
            "Epoch: 77/100, Loss: 0.158043\n",
            "Epoch: 78/100, Loss: 0.154848\n",
            "Epoch: 79/100, Loss: 0.151704\n",
            "Epoch: 80/100, Loss: 0.148617\n",
            "Epoch: 81/100, Loss: 0.145589\n",
            "Epoch: 82/100, Loss: 0.142611\n",
            "Epoch: 83/100, Loss: 0.139654\n",
            "Epoch: 84/100, Loss: 0.136713\n",
            "Epoch: 85/100, Loss: 0.133796\n",
            "Epoch: 86/100, Loss: 0.130912\n",
            "Epoch: 87/100, Loss: 0.128080\n",
            "Epoch: 88/100, Loss: 0.125308\n",
            "Epoch: 89/100, Loss: 0.122596\n",
            "Epoch: 90/100, Loss: 0.119936\n",
            "Epoch: 91/100, Loss: 0.117315\n",
            "Epoch: 92/100, Loss: 0.114744\n",
            "Epoch: 93/100, Loss: 0.112230\n",
            "Epoch: 94/100, Loss: 0.109769\n",
            "Epoch: 95/100, Loss: 0.107361\n",
            "Epoch: 96/100, Loss: 0.104960\n",
            "Epoch: 97/100, Loss: 0.102577\n",
            "Epoch: 98/100, Loss: 0.100234\n",
            "Epoch: 99/100, Loss: 0.097940\n",
            "Epoch: 100/100, Loss: 0.095674\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6594\n",
            "Normalised mutual info score on k-means on latent space: 0.600749501701675\n",
            "ARI score on k-means on latent space: 0.49557071624221094\n",
            "K-means cluster error on latent space: 44362.67578125\n",
            "K-means silhouette score on latent space: 0.20147756 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7076\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6482978595852162 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5499273252185927 \n",
            "\n",
            "ROUND NUMBER  5 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.942407\n",
            "Epoch: 2/100, Loss: 0.733217\n",
            "Epoch: 3/100, Loss: 0.651549\n",
            "Epoch: 4/100, Loss: 0.612382\n",
            "Epoch: 5/100, Loss: 0.587682\n",
            "Epoch: 6/100, Loss: 0.569373\n",
            "Epoch: 7/100, Loss: 0.554387\n",
            "Epoch: 8/100, Loss: 0.541371\n",
            "Epoch: 9/100, Loss: 0.529651\n",
            "Epoch: 10/100, Loss: 0.518824\n",
            "Epoch: 11/100, Loss: 0.508682\n",
            "Epoch: 12/100, Loss: 0.499078\n",
            "Epoch: 13/100, Loss: 0.489906\n",
            "Epoch: 14/100, Loss: 0.481085\n",
            "Epoch: 15/100, Loss: 0.472581\n",
            "Epoch: 16/100, Loss: 0.464351\n",
            "Epoch: 17/100, Loss: 0.456367\n",
            "Epoch: 18/100, Loss: 0.448598\n",
            "Epoch: 19/100, Loss: 0.441031\n",
            "Epoch: 20/100, Loss: 0.433648\n",
            "Epoch: 21/100, Loss: 0.426434\n",
            "Epoch: 22/100, Loss: 0.419380\n",
            "Epoch: 23/100, Loss: 0.412470\n",
            "Epoch: 24/100, Loss: 0.405698\n",
            "Epoch: 25/100, Loss: 0.399053\n",
            "Epoch: 26/100, Loss: 0.392529\n",
            "Epoch: 27/100, Loss: 0.386123\n",
            "Epoch: 28/100, Loss: 0.379828\n",
            "Epoch: 29/100, Loss: 0.373642\n",
            "Epoch: 30/100, Loss: 0.367553\n",
            "Epoch: 31/100, Loss: 0.361562\n",
            "Epoch: 32/100, Loss: 0.355665\n",
            "Epoch: 33/100, Loss: 0.349857\n",
            "Epoch: 34/100, Loss: 0.344140\n",
            "Epoch: 35/100, Loss: 0.338506\n",
            "Epoch: 36/100, Loss: 0.332947\n",
            "Epoch: 37/100, Loss: 0.327467\n",
            "Epoch: 38/100, Loss: 0.322062\n",
            "Epoch: 39/100, Loss: 0.316736\n",
            "Epoch: 40/100, Loss: 0.311476\n",
            "Epoch: 41/100, Loss: 0.306288\n",
            "Epoch: 42/100, Loss: 0.301167\n",
            "Epoch: 43/100, Loss: 0.296114\n",
            "Epoch: 44/100, Loss: 0.291124\n",
            "Epoch: 45/100, Loss: 0.286197\n",
            "Epoch: 46/100, Loss: 0.281331\n",
            "Epoch: 47/100, Loss: 0.276529\n",
            "Epoch: 48/100, Loss: 0.271786\n",
            "Epoch: 49/100, Loss: 0.267101\n",
            "Epoch: 50/100, Loss: 0.262474\n",
            "Epoch: 51/100, Loss: 0.257906\n",
            "Epoch: 52/100, Loss: 0.253395\n",
            "Epoch: 53/100, Loss: 0.248941\n",
            "Epoch: 54/100, Loss: 0.244542\n",
            "Epoch: 55/100, Loss: 0.240198\n",
            "Epoch: 56/100, Loss: 0.235908\n",
            "Epoch: 57/100, Loss: 0.231672\n",
            "Epoch: 58/100, Loss: 0.227490\n",
            "Epoch: 59/100, Loss: 0.223358\n",
            "Epoch: 60/100, Loss: 0.219278\n",
            "Epoch: 61/100, Loss: 0.215250\n",
            "Epoch: 62/100, Loss: 0.211275\n",
            "Epoch: 63/100, Loss: 0.207345\n",
            "Epoch: 64/100, Loss: 0.203463\n",
            "Epoch: 65/100, Loss: 0.199631\n",
            "Epoch: 66/100, Loss: 0.195849\n",
            "Epoch: 67/100, Loss: 0.192119\n",
            "Epoch: 68/100, Loss: 0.188433\n",
            "Epoch: 69/100, Loss: 0.184797\n",
            "Epoch: 70/100, Loss: 0.181210\n",
            "Epoch: 71/100, Loss: 0.177676\n",
            "Epoch: 72/100, Loss: 0.174191\n",
            "Epoch: 73/100, Loss: 0.170756\n",
            "Epoch: 74/100, Loss: 0.167363\n",
            "Epoch: 75/100, Loss: 0.164019\n",
            "Epoch: 76/100, Loss: 0.160726\n",
            "Epoch: 77/100, Loss: 0.157482\n",
            "Epoch: 78/100, Loss: 0.154290\n",
            "Epoch: 79/100, Loss: 0.151146\n",
            "Epoch: 80/100, Loss: 0.148065\n",
            "Epoch: 81/100, Loss: 0.145049\n",
            "Epoch: 82/100, Loss: 0.142099\n",
            "Epoch: 83/100, Loss: 0.139229\n",
            "Epoch: 84/100, Loss: 0.136336\n",
            "Epoch: 85/100, Loss: 0.133375\n",
            "Epoch: 86/100, Loss: 0.130408\n",
            "Epoch: 87/100, Loss: 0.127493\n",
            "Epoch: 88/100, Loss: 0.124650\n",
            "Epoch: 89/100, Loss: 0.121891\n",
            "Epoch: 90/100, Loss: 0.119196\n",
            "Epoch: 91/100, Loss: 0.116561\n",
            "Epoch: 92/100, Loss: 0.113971\n",
            "Epoch: 93/100, Loss: 0.111425\n",
            "Epoch: 94/100, Loss: 0.108933\n",
            "Epoch: 95/100, Loss: 0.106481\n",
            "Epoch: 96/100, Loss: 0.104078\n",
            "Epoch: 97/100, Loss: 0.101725\n",
            "Epoch: 98/100, Loss: 0.099407\n",
            "Epoch: 99/100, Loss: 0.097114\n",
            "Epoch: 100/100, Loss: 0.094844\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6813\n",
            "Normalised mutual info score on k-means on latent space: 0.6257918770399229\n",
            "ARI score on k-means on latent space: 0.5276842213053536\n",
            "K-means cluster error on latent space: 43574.91796875\n",
            "K-means silhouette score on latent space: 0.17886822 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7127\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6863071792713789 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5796941572445339 \n",
            "\n",
            "ROUND NUMBER  6 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.944794\n",
            "Epoch: 2/100, Loss: 0.756634\n",
            "Epoch: 3/100, Loss: 0.674963\n",
            "Epoch: 4/100, Loss: 0.627739\n",
            "Epoch: 5/100, Loss: 0.598847\n",
            "Epoch: 6/100, Loss: 0.578487\n",
            "Epoch: 7/100, Loss: 0.562152\n",
            "Epoch: 8/100, Loss: 0.548293\n",
            "Epoch: 9/100, Loss: 0.536003\n",
            "Epoch: 10/100, Loss: 0.524722\n",
            "Epoch: 11/100, Loss: 0.514139\n",
            "Epoch: 12/100, Loss: 0.504088\n",
            "Epoch: 13/100, Loss: 0.494490\n",
            "Epoch: 14/100, Loss: 0.485270\n",
            "Epoch: 15/100, Loss: 0.476391\n",
            "Epoch: 16/100, Loss: 0.467840\n",
            "Epoch: 17/100, Loss: 0.459565\n",
            "Epoch: 18/100, Loss: 0.451554\n",
            "Epoch: 19/100, Loss: 0.443774\n",
            "Epoch: 20/100, Loss: 0.436208\n",
            "Epoch: 21/100, Loss: 0.428833\n",
            "Epoch: 22/100, Loss: 0.421632\n",
            "Epoch: 23/100, Loss: 0.414591\n",
            "Epoch: 24/100, Loss: 0.407699\n",
            "Epoch: 25/100, Loss: 0.400959\n",
            "Epoch: 26/100, Loss: 0.394348\n",
            "Epoch: 27/100, Loss: 0.387869\n",
            "Epoch: 28/100, Loss: 0.381513\n",
            "Epoch: 29/100, Loss: 0.375267\n",
            "Epoch: 30/100, Loss: 0.369126\n",
            "Epoch: 31/100, Loss: 0.363089\n",
            "Epoch: 32/100, Loss: 0.357150\n",
            "Epoch: 33/100, Loss: 0.351301\n",
            "Epoch: 34/100, Loss: 0.345546\n",
            "Epoch: 35/100, Loss: 0.339874\n",
            "Epoch: 36/100, Loss: 0.334288\n",
            "Epoch: 37/100, Loss: 0.328783\n",
            "Epoch: 38/100, Loss: 0.323357\n",
            "Epoch: 39/100, Loss: 0.318012\n",
            "Epoch: 40/100, Loss: 0.312734\n",
            "Epoch: 41/100, Loss: 0.307527\n",
            "Epoch: 42/100, Loss: 0.302389\n",
            "Epoch: 43/100, Loss: 0.297320\n",
            "Epoch: 44/100, Loss: 0.292318\n",
            "Epoch: 45/100, Loss: 0.287377\n",
            "Epoch: 46/100, Loss: 0.282503\n",
            "Epoch: 47/100, Loss: 0.277690\n",
            "Epoch: 48/100, Loss: 0.272937\n",
            "Epoch: 49/100, Loss: 0.268242\n",
            "Epoch: 50/100, Loss: 0.263607\n",
            "Epoch: 51/100, Loss: 0.259029\n",
            "Epoch: 52/100, Loss: 0.254506\n",
            "Epoch: 53/100, Loss: 0.250042\n",
            "Epoch: 54/100, Loss: 0.245636\n",
            "Epoch: 55/100, Loss: 0.241284\n",
            "Epoch: 56/100, Loss: 0.236989\n",
            "Epoch: 57/100, Loss: 0.232743\n",
            "Epoch: 58/100, Loss: 0.228551\n",
            "Epoch: 59/100, Loss: 0.224410\n",
            "Epoch: 60/100, Loss: 0.220322\n",
            "Epoch: 61/100, Loss: 0.216283\n",
            "Epoch: 62/100, Loss: 0.212296\n",
            "Epoch: 63/100, Loss: 0.208360\n",
            "Epoch: 64/100, Loss: 0.204476\n",
            "Epoch: 65/100, Loss: 0.200640\n",
            "Epoch: 66/100, Loss: 0.196853\n",
            "Epoch: 67/100, Loss: 0.193118\n",
            "Epoch: 68/100, Loss: 0.189429\n",
            "Epoch: 69/100, Loss: 0.185791\n",
            "Epoch: 70/100, Loss: 0.182197\n",
            "Epoch: 71/100, Loss: 0.178653\n",
            "Epoch: 72/100, Loss: 0.175156\n",
            "Epoch: 73/100, Loss: 0.171705\n",
            "Epoch: 74/100, Loss: 0.168300\n",
            "Epoch: 75/100, Loss: 0.164940\n",
            "Epoch: 76/100, Loss: 0.161626\n",
            "Epoch: 77/100, Loss: 0.158358\n",
            "Epoch: 78/100, Loss: 0.155133\n",
            "Epoch: 79/100, Loss: 0.151955\n",
            "Epoch: 80/100, Loss: 0.148822\n",
            "Epoch: 81/100, Loss: 0.145735\n",
            "Epoch: 82/100, Loss: 0.142693\n",
            "Epoch: 83/100, Loss: 0.139693\n",
            "Epoch: 84/100, Loss: 0.136737\n",
            "Epoch: 85/100, Loss: 0.133828\n",
            "Epoch: 86/100, Loss: 0.130963\n",
            "Epoch: 87/100, Loss: 0.128151\n",
            "Epoch: 88/100, Loss: 0.125391\n",
            "Epoch: 89/100, Loss: 0.122686\n",
            "Epoch: 90/100, Loss: 0.120036\n",
            "Epoch: 91/100, Loss: 0.117436\n",
            "Epoch: 92/100, Loss: 0.114880\n",
            "Epoch: 93/100, Loss: 0.112387\n",
            "Epoch: 94/100, Loss: 0.109928\n",
            "Epoch: 95/100, Loss: 0.107474\n",
            "Epoch: 96/100, Loss: 0.105018\n",
            "Epoch: 97/100, Loss: 0.102589\n",
            "Epoch: 98/100, Loss: 0.100193\n",
            "Epoch: 99/100, Loss: 0.097817\n",
            "Epoch: 100/100, Loss: 0.095481\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.7029\n",
            "Normalised mutual info score on k-means on latent space: 0.6336430253742305\n",
            "ARI score on k-means on latent space: 0.5486580158950847\n",
            "K-means cluster error on latent space: 42935.44140625\n",
            "K-means silhouette score on latent space: 0.17817955 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6458\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.624518296432689 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.48070357324469964 \n",
            "\n",
            "ROUND NUMBER  7 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.931487\n",
            "Epoch: 2/100, Loss: 0.743182\n",
            "Epoch: 3/100, Loss: 0.656329\n",
            "Epoch: 4/100, Loss: 0.613757\n",
            "Epoch: 5/100, Loss: 0.589151\n",
            "Epoch: 6/100, Loss: 0.571260\n",
            "Epoch: 7/100, Loss: 0.556445\n",
            "Epoch: 8/100, Loss: 0.543442\n",
            "Epoch: 9/100, Loss: 0.531649\n",
            "Epoch: 10/100, Loss: 0.520748\n",
            "Epoch: 11/100, Loss: 0.510518\n",
            "Epoch: 12/100, Loss: 0.500827\n",
            "Epoch: 13/100, Loss: 0.491577\n",
            "Epoch: 14/100, Loss: 0.482694\n",
            "Epoch: 15/100, Loss: 0.474134\n",
            "Epoch: 16/100, Loss: 0.465847\n",
            "Epoch: 17/100, Loss: 0.457814\n",
            "Epoch: 18/100, Loss: 0.450002\n",
            "Epoch: 19/100, Loss: 0.442389\n",
            "Epoch: 20/100, Loss: 0.434965\n",
            "Epoch: 21/100, Loss: 0.427715\n",
            "Epoch: 22/100, Loss: 0.420623\n",
            "Epoch: 23/100, Loss: 0.413683\n",
            "Epoch: 24/100, Loss: 0.406886\n",
            "Epoch: 25/100, Loss: 0.400217\n",
            "Epoch: 26/100, Loss: 0.393669\n",
            "Epoch: 27/100, Loss: 0.387245\n",
            "Epoch: 28/100, Loss: 0.380937\n",
            "Epoch: 29/100, Loss: 0.374727\n",
            "Epoch: 30/100, Loss: 0.368621\n",
            "Epoch: 31/100, Loss: 0.362613\n",
            "Epoch: 32/100, Loss: 0.356697\n",
            "Epoch: 33/100, Loss: 0.350873\n",
            "Epoch: 34/100, Loss: 0.345137\n",
            "Epoch: 35/100, Loss: 0.339487\n",
            "Epoch: 36/100, Loss: 0.333913\n",
            "Epoch: 37/100, Loss: 0.328414\n",
            "Epoch: 38/100, Loss: 0.322991\n",
            "Epoch: 39/100, Loss: 0.317643\n",
            "Epoch: 40/100, Loss: 0.312365\n",
            "Epoch: 41/100, Loss: 0.307162\n",
            "Epoch: 42/100, Loss: 0.302026\n",
            "Epoch: 43/100, Loss: 0.296954\n",
            "Epoch: 44/100, Loss: 0.291951\n",
            "Epoch: 45/100, Loss: 0.287009\n",
            "Epoch: 46/100, Loss: 0.282130\n",
            "Epoch: 47/100, Loss: 0.277316\n",
            "Epoch: 48/100, Loss: 0.272560\n",
            "Epoch: 49/100, Loss: 0.267863\n",
            "Epoch: 50/100, Loss: 0.263225\n",
            "Epoch: 51/100, Loss: 0.258646\n",
            "Epoch: 52/100, Loss: 0.254123\n",
            "Epoch: 53/100, Loss: 0.249657\n",
            "Epoch: 54/100, Loss: 0.245248\n",
            "Epoch: 55/100, Loss: 0.240891\n",
            "Epoch: 56/100, Loss: 0.236588\n",
            "Epoch: 57/100, Loss: 0.232341\n",
            "Epoch: 58/100, Loss: 0.228148\n",
            "Epoch: 59/100, Loss: 0.224005\n",
            "Epoch: 60/100, Loss: 0.219914\n",
            "Epoch: 61/100, Loss: 0.215877\n",
            "Epoch: 62/100, Loss: 0.211893\n",
            "Epoch: 63/100, Loss: 0.207955\n",
            "Epoch: 64/100, Loss: 0.204071\n",
            "Epoch: 65/100, Loss: 0.200233\n",
            "Epoch: 66/100, Loss: 0.196448\n",
            "Epoch: 67/100, Loss: 0.192710\n",
            "Epoch: 68/100, Loss: 0.189022\n",
            "Epoch: 69/100, Loss: 0.185383\n",
            "Epoch: 70/100, Loss: 0.181793\n",
            "Epoch: 71/100, Loss: 0.178249\n",
            "Epoch: 72/100, Loss: 0.174756\n",
            "Epoch: 73/100, Loss: 0.171310\n",
            "Epoch: 74/100, Loss: 0.167907\n",
            "Epoch: 75/100, Loss: 0.164552\n",
            "Epoch: 76/100, Loss: 0.161242\n",
            "Epoch: 77/100, Loss: 0.157981\n",
            "Epoch: 78/100, Loss: 0.154769\n",
            "Epoch: 79/100, Loss: 0.151602\n",
            "Epoch: 80/100, Loss: 0.148478\n",
            "Epoch: 81/100, Loss: 0.145404\n",
            "Epoch: 82/100, Loss: 0.142378\n",
            "Epoch: 83/100, Loss: 0.139405\n",
            "Epoch: 84/100, Loss: 0.136481\n",
            "Epoch: 85/100, Loss: 0.133601\n",
            "Epoch: 86/100, Loss: 0.130782\n",
            "Epoch: 87/100, Loss: 0.128014\n",
            "Epoch: 88/100, Loss: 0.125311\n",
            "Epoch: 89/100, Loss: 0.122652\n",
            "Epoch: 90/100, Loss: 0.120057\n",
            "Epoch: 91/100, Loss: 0.117519\n",
            "Epoch: 92/100, Loss: 0.114978\n",
            "Epoch: 93/100, Loss: 0.112412\n",
            "Epoch: 94/100, Loss: 0.109859\n",
            "Epoch: 95/100, Loss: 0.107302\n",
            "Epoch: 96/100, Loss: 0.104778\n",
            "Epoch: 97/100, Loss: 0.102314\n",
            "Epoch: 98/100, Loss: 0.099917\n",
            "Epoch: 99/100, Loss: 0.097595\n",
            "Epoch: 100/100, Loss: 0.095327\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6289\n",
            "Normalised mutual info score on k-means on latent space: 0.5659045386810145\n",
            "ARI score on k-means on latent space: 0.457298043941456\n",
            "K-means cluster error on latent space: 43882.515625\n",
            "K-means silhouette score on latent space: 0.18811342 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7242\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6942090605313642 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5773835493159343 \n",
            "\n",
            "ROUND NUMBER  8 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.903570\n",
            "Epoch: 2/100, Loss: 0.708346\n",
            "Epoch: 3/100, Loss: 0.642049\n",
            "Epoch: 4/100, Loss: 0.608860\n",
            "Epoch: 5/100, Loss: 0.586916\n",
            "Epoch: 6/100, Loss: 0.569868\n",
            "Epoch: 7/100, Loss: 0.555453\n",
            "Epoch: 8/100, Loss: 0.542713\n",
            "Epoch: 9/100, Loss: 0.531104\n",
            "Epoch: 10/100, Loss: 0.520327\n",
            "Epoch: 11/100, Loss: 0.510175\n",
            "Epoch: 12/100, Loss: 0.500527\n",
            "Epoch: 13/100, Loss: 0.491304\n",
            "Epoch: 14/100, Loss: 0.482448\n",
            "Epoch: 15/100, Loss: 0.473905\n",
            "Epoch: 16/100, Loss: 0.465641\n",
            "Epoch: 17/100, Loss: 0.457626\n",
            "Epoch: 18/100, Loss: 0.449825\n",
            "Epoch: 19/100, Loss: 0.442229\n",
            "Epoch: 20/100, Loss: 0.434817\n",
            "Epoch: 21/100, Loss: 0.427578\n",
            "Epoch: 22/100, Loss: 0.420494\n",
            "Epoch: 23/100, Loss: 0.413562\n",
            "Epoch: 24/100, Loss: 0.406765\n",
            "Epoch: 25/100, Loss: 0.400099\n",
            "Epoch: 26/100, Loss: 0.393558\n",
            "Epoch: 27/100, Loss: 0.387133\n",
            "Epoch: 28/100, Loss: 0.380819\n",
            "Epoch: 29/100, Loss: 0.374615\n",
            "Epoch: 30/100, Loss: 0.368507\n",
            "Epoch: 31/100, Loss: 0.362500\n",
            "Epoch: 32/100, Loss: 0.356589\n",
            "Epoch: 33/100, Loss: 0.350771\n",
            "Epoch: 34/100, Loss: 0.345039\n",
            "Epoch: 35/100, Loss: 0.339390\n",
            "Epoch: 36/100, Loss: 0.333820\n",
            "Epoch: 37/100, Loss: 0.328329\n",
            "Epoch: 38/100, Loss: 0.322916\n",
            "Epoch: 39/100, Loss: 0.317582\n",
            "Epoch: 40/100, Loss: 0.312319\n",
            "Epoch: 41/100, Loss: 0.307123\n",
            "Epoch: 42/100, Loss: 0.301996\n",
            "Epoch: 43/100, Loss: 0.296938\n",
            "Epoch: 44/100, Loss: 0.291946\n",
            "Epoch: 45/100, Loss: 0.287018\n",
            "Epoch: 46/100, Loss: 0.282154\n",
            "Epoch: 47/100, Loss: 0.277353\n",
            "Epoch: 48/100, Loss: 0.272612\n",
            "Epoch: 49/100, Loss: 0.267928\n",
            "Epoch: 50/100, Loss: 0.263303\n",
            "Epoch: 51/100, Loss: 0.258732\n",
            "Epoch: 52/100, Loss: 0.254222\n",
            "Epoch: 53/100, Loss: 0.249765\n",
            "Epoch: 54/100, Loss: 0.245364\n",
            "Epoch: 55/100, Loss: 0.241018\n",
            "Epoch: 56/100, Loss: 0.236729\n",
            "Epoch: 57/100, Loss: 0.232493\n",
            "Epoch: 58/100, Loss: 0.228308\n",
            "Epoch: 59/100, Loss: 0.224176\n",
            "Epoch: 60/100, Loss: 0.220096\n",
            "Epoch: 61/100, Loss: 0.216065\n",
            "Epoch: 62/100, Loss: 0.212085\n",
            "Epoch: 63/100, Loss: 0.208162\n",
            "Epoch: 64/100, Loss: 0.204283\n",
            "Epoch: 65/100, Loss: 0.200458\n",
            "Epoch: 66/100, Loss: 0.196679\n",
            "Epoch: 67/100, Loss: 0.192949\n",
            "Epoch: 68/100, Loss: 0.189269\n",
            "Epoch: 69/100, Loss: 0.185638\n",
            "Epoch: 70/100, Loss: 0.182053\n",
            "Epoch: 71/100, Loss: 0.178514\n",
            "Epoch: 72/100, Loss: 0.175021\n",
            "Epoch: 73/100, Loss: 0.171576\n",
            "Epoch: 74/100, Loss: 0.168177\n",
            "Epoch: 75/100, Loss: 0.164827\n",
            "Epoch: 76/100, Loss: 0.161522\n",
            "Epoch: 77/100, Loss: 0.158262\n",
            "Epoch: 78/100, Loss: 0.155049\n",
            "Epoch: 79/100, Loss: 0.151884\n",
            "Epoch: 80/100, Loss: 0.148765\n",
            "Epoch: 81/100, Loss: 0.145693\n",
            "Epoch: 82/100, Loss: 0.142665\n",
            "Epoch: 83/100, Loss: 0.139689\n",
            "Epoch: 84/100, Loss: 0.136760\n",
            "Epoch: 85/100, Loss: 0.133883\n",
            "Epoch: 86/100, Loss: 0.131063\n",
            "Epoch: 87/100, Loss: 0.128312\n",
            "Epoch: 88/100, Loss: 0.125626\n",
            "Epoch: 89/100, Loss: 0.122930\n",
            "Epoch: 90/100, Loss: 0.120170\n",
            "Epoch: 91/100, Loss: 0.117443\n",
            "Epoch: 92/100, Loss: 0.114797\n",
            "Epoch: 93/100, Loss: 0.112226\n",
            "Epoch: 94/100, Loss: 0.109718\n",
            "Epoch: 95/100, Loss: 0.107260\n",
            "Epoch: 96/100, Loss: 0.104856\n",
            "Epoch: 97/100, Loss: 0.102485\n",
            "Epoch: 98/100, Loss: 0.100158\n",
            "Epoch: 99/100, Loss: 0.097881\n",
            "Epoch: 100/100, Loss: 0.095640\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6992\n",
            "Normalised mutual info score on k-means on latent space: 0.6149530167277167\n",
            "ARI score on k-means on latent space: 0.5279059551011545\n",
            "K-means cluster error on latent space: 44884.1015625\n",
            "K-means silhouette score on latent space: 0.18419106 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7191\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.7018222712482302 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5946186413677044 \n",
            "\n",
            "ROUND NUMBER  9 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.930247\n",
            "Epoch: 2/100, Loss: 0.732866\n",
            "Epoch: 3/100, Loss: 0.651485\n",
            "Epoch: 4/100, Loss: 0.613244\n",
            "Epoch: 5/100, Loss: 0.589004\n",
            "Epoch: 6/100, Loss: 0.571097\n",
            "Epoch: 7/100, Loss: 0.556258\n",
            "Epoch: 8/100, Loss: 0.543309\n",
            "Epoch: 9/100, Loss: 0.531614\n",
            "Epoch: 10/100, Loss: 0.520783\n",
            "Epoch: 11/100, Loss: 0.510586\n",
            "Epoch: 12/100, Loss: 0.500899\n",
            "Epoch: 13/100, Loss: 0.491633\n",
            "Epoch: 14/100, Loss: 0.482726\n",
            "Epoch: 15/100, Loss: 0.474138\n",
            "Epoch: 16/100, Loss: 0.465826\n",
            "Epoch: 17/100, Loss: 0.457762\n",
            "Epoch: 18/100, Loss: 0.449929\n",
            "Epoch: 19/100, Loss: 0.442307\n",
            "Epoch: 20/100, Loss: 0.434879\n",
            "Epoch: 21/100, Loss: 0.427626\n",
            "Epoch: 22/100, Loss: 0.420534\n",
            "Epoch: 23/100, Loss: 0.413592\n",
            "Epoch: 24/100, Loss: 0.406790\n",
            "Epoch: 25/100, Loss: 0.400121\n",
            "Epoch: 26/100, Loss: 0.393574\n",
            "Epoch: 27/100, Loss: 0.387152\n",
            "Epoch: 28/100, Loss: 0.380839\n",
            "Epoch: 29/100, Loss: 0.374635\n",
            "Epoch: 30/100, Loss: 0.368533\n",
            "Epoch: 31/100, Loss: 0.362525\n",
            "Epoch: 32/100, Loss: 0.356615\n",
            "Epoch: 33/100, Loss: 0.350795\n",
            "Epoch: 34/100, Loss: 0.345060\n",
            "Epoch: 35/100, Loss: 0.339413\n",
            "Epoch: 36/100, Loss: 0.333844\n",
            "Epoch: 37/100, Loss: 0.328356\n",
            "Epoch: 38/100, Loss: 0.322946\n",
            "Epoch: 39/100, Loss: 0.317609\n",
            "Epoch: 40/100, Loss: 0.312347\n",
            "Epoch: 41/100, Loss: 0.307155\n",
            "Epoch: 42/100, Loss: 0.302029\n",
            "Epoch: 43/100, Loss: 0.296967\n",
            "Epoch: 44/100, Loss: 0.291973\n",
            "Epoch: 45/100, Loss: 0.287043\n",
            "Epoch: 46/100, Loss: 0.282175\n",
            "Epoch: 47/100, Loss: 0.277370\n",
            "Epoch: 48/100, Loss: 0.272625\n",
            "Epoch: 49/100, Loss: 0.267938\n",
            "Epoch: 50/100, Loss: 0.263310\n",
            "Epoch: 51/100, Loss: 0.258744\n",
            "Epoch: 52/100, Loss: 0.254231\n",
            "Epoch: 53/100, Loss: 0.249772\n",
            "Epoch: 54/100, Loss: 0.245369\n",
            "Epoch: 55/100, Loss: 0.241022\n",
            "Epoch: 56/100, Loss: 0.236730\n",
            "Epoch: 57/100, Loss: 0.232492\n",
            "Epoch: 58/100, Loss: 0.228307\n",
            "Epoch: 59/100, Loss: 0.224175\n",
            "Epoch: 60/100, Loss: 0.220094\n",
            "Epoch: 61/100, Loss: 0.216064\n",
            "Epoch: 62/100, Loss: 0.212081\n",
            "Epoch: 63/100, Loss: 0.208151\n",
            "Epoch: 64/100, Loss: 0.204270\n",
            "Epoch: 65/100, Loss: 0.200441\n",
            "Epoch: 66/100, Loss: 0.196659\n",
            "Epoch: 67/100, Loss: 0.192927\n",
            "Epoch: 68/100, Loss: 0.189241\n",
            "Epoch: 69/100, Loss: 0.185603\n",
            "Epoch: 70/100, Loss: 0.182014\n",
            "Epoch: 71/100, Loss: 0.178469\n",
            "Epoch: 72/100, Loss: 0.174973\n",
            "Epoch: 73/100, Loss: 0.171524\n",
            "Epoch: 74/100, Loss: 0.168123\n",
            "Epoch: 75/100, Loss: 0.164769\n",
            "Epoch: 76/100, Loss: 0.161458\n",
            "Epoch: 77/100, Loss: 0.158191\n",
            "Epoch: 78/100, Loss: 0.154974\n",
            "Epoch: 79/100, Loss: 0.151798\n",
            "Epoch: 80/100, Loss: 0.148671\n",
            "Epoch: 81/100, Loss: 0.145593\n",
            "Epoch: 82/100, Loss: 0.142558\n",
            "Epoch: 83/100, Loss: 0.139568\n",
            "Epoch: 84/100, Loss: 0.136623\n",
            "Epoch: 85/100, Loss: 0.133720\n",
            "Epoch: 86/100, Loss: 0.130863\n",
            "Epoch: 87/100, Loss: 0.128059\n",
            "Epoch: 88/100, Loss: 0.125319\n",
            "Epoch: 89/100, Loss: 0.122629\n",
            "Epoch: 90/100, Loss: 0.119968\n",
            "Epoch: 91/100, Loss: 0.117361\n",
            "Epoch: 92/100, Loss: 0.114796\n",
            "Epoch: 93/100, Loss: 0.112245\n",
            "Epoch: 94/100, Loss: 0.109704\n",
            "Epoch: 95/100, Loss: 0.107174\n",
            "Epoch: 96/100, Loss: 0.104685\n",
            "Epoch: 97/100, Loss: 0.102244\n",
            "Epoch: 98/100, Loss: 0.099868\n",
            "Epoch: 99/100, Loss: 0.097550\n",
            "Epoch: 100/100, Loss: 0.095289\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6831\n",
            "Normalised mutual info score on k-means on latent space: 0.5922929679814738\n",
            "ARI score on k-means on latent space: 0.5074101725865477\n",
            "K-means cluster error on latent space: 46458.5\n",
            "K-means silhouette score on latent space: 0.18492001 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7005\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6753675810035124 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5629685113379825 \n",
            "\n",
            "ROUND NUMBER  10 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.917726\n",
            "Epoch: 2/100, Loss: 0.709425\n",
            "Epoch: 3/100, Loss: 0.638344\n",
            "Epoch: 4/100, Loss: 0.605672\n",
            "Epoch: 5/100, Loss: 0.583923\n",
            "Epoch: 6/100, Loss: 0.567042\n",
            "Epoch: 7/100, Loss: 0.552832\n",
            "Epoch: 8/100, Loss: 0.540286\n",
            "Epoch: 9/100, Loss: 0.528884\n",
            "Epoch: 10/100, Loss: 0.518292\n",
            "Epoch: 11/100, Loss: 0.508316\n",
            "Epoch: 12/100, Loss: 0.498833\n",
            "Epoch: 13/100, Loss: 0.489758\n",
            "Epoch: 14/100, Loss: 0.481011\n",
            "Epoch: 15/100, Loss: 0.472568\n",
            "Epoch: 16/100, Loss: 0.464384\n",
            "Epoch: 17/100, Loss: 0.456429\n",
            "Epoch: 18/100, Loss: 0.448684\n",
            "Epoch: 19/100, Loss: 0.441128\n",
            "Epoch: 20/100, Loss: 0.433751\n",
            "Epoch: 21/100, Loss: 0.426539\n",
            "Epoch: 22/100, Loss: 0.419479\n",
            "Epoch: 23/100, Loss: 0.412564\n",
            "Epoch: 24/100, Loss: 0.405788\n",
            "Epoch: 25/100, Loss: 0.399140\n",
            "Epoch: 26/100, Loss: 0.392614\n",
            "Epoch: 27/100, Loss: 0.386202\n",
            "Epoch: 28/100, Loss: 0.379903\n",
            "Epoch: 29/100, Loss: 0.373704\n",
            "Epoch: 30/100, Loss: 0.367605\n",
            "Epoch: 31/100, Loss: 0.361605\n",
            "Epoch: 32/100, Loss: 0.355697\n",
            "Epoch: 33/100, Loss: 0.349883\n",
            "Epoch: 34/100, Loss: 0.344155\n",
            "Epoch: 35/100, Loss: 0.338508\n",
            "Epoch: 36/100, Loss: 0.332946\n",
            "Epoch: 37/100, Loss: 0.327462\n",
            "Epoch: 38/100, Loss: 0.322051\n",
            "Epoch: 39/100, Loss: 0.316714\n",
            "Epoch: 40/100, Loss: 0.311448\n",
            "Epoch: 41/100, Loss: 0.306253\n",
            "Epoch: 42/100, Loss: 0.301125\n",
            "Epoch: 43/100, Loss: 0.296068\n",
            "Epoch: 44/100, Loss: 0.291074\n",
            "Epoch: 45/100, Loss: 0.286144\n",
            "Epoch: 46/100, Loss: 0.281272\n",
            "Epoch: 47/100, Loss: 0.276465\n",
            "Epoch: 48/100, Loss: 0.271721\n",
            "Epoch: 49/100, Loss: 0.267034\n",
            "Epoch: 50/100, Loss: 0.262409\n",
            "Epoch: 51/100, Loss: 0.257839\n",
            "Epoch: 52/100, Loss: 0.253325\n",
            "Epoch: 53/100, Loss: 0.248869\n",
            "Epoch: 54/100, Loss: 0.244466\n",
            "Epoch: 55/100, Loss: 0.240125\n",
            "Epoch: 56/100, Loss: 0.235835\n",
            "Epoch: 57/100, Loss: 0.231594\n",
            "Epoch: 58/100, Loss: 0.227407\n",
            "Epoch: 59/100, Loss: 0.223274\n",
            "Epoch: 60/100, Loss: 0.219194\n",
            "Epoch: 61/100, Loss: 0.215165\n",
            "Epoch: 62/100, Loss: 0.211185\n",
            "Epoch: 63/100, Loss: 0.207255\n",
            "Epoch: 64/100, Loss: 0.203376\n",
            "Epoch: 65/100, Loss: 0.199547\n",
            "Epoch: 66/100, Loss: 0.195768\n",
            "Epoch: 67/100, Loss: 0.192039\n",
            "Epoch: 68/100, Loss: 0.188363\n",
            "Epoch: 69/100, Loss: 0.184733\n",
            "Epoch: 70/100, Loss: 0.181149\n",
            "Epoch: 71/100, Loss: 0.177611\n",
            "Epoch: 72/100, Loss: 0.174121\n",
            "Epoch: 73/100, Loss: 0.170681\n",
            "Epoch: 74/100, Loss: 0.167284\n",
            "Epoch: 75/100, Loss: 0.163934\n",
            "Epoch: 76/100, Loss: 0.160628\n",
            "Epoch: 77/100, Loss: 0.157370\n",
            "Epoch: 78/100, Loss: 0.154161\n",
            "Epoch: 79/100, Loss: 0.151000\n",
            "Epoch: 80/100, Loss: 0.147887\n",
            "Epoch: 81/100, Loss: 0.144819\n",
            "Epoch: 82/100, Loss: 0.141799\n",
            "Epoch: 83/100, Loss: 0.138828\n",
            "Epoch: 84/100, Loss: 0.135919\n",
            "Epoch: 85/100, Loss: 0.133068\n",
            "Epoch: 86/100, Loss: 0.130277\n",
            "Epoch: 87/100, Loss: 0.127558\n",
            "Epoch: 88/100, Loss: 0.124902\n",
            "Epoch: 89/100, Loss: 0.122159\n",
            "Epoch: 90/100, Loss: 0.119332\n",
            "Epoch: 91/100, Loss: 0.116596\n",
            "Epoch: 92/100, Loss: 0.113946\n",
            "Epoch: 93/100, Loss: 0.111369\n",
            "Epoch: 94/100, Loss: 0.108849\n",
            "Epoch: 95/100, Loss: 0.106381\n",
            "Epoch: 96/100, Loss: 0.103964\n",
            "Epoch: 97/100, Loss: 0.101589\n",
            "Epoch: 98/100, Loss: 0.099272\n",
            "Epoch: 99/100, Loss: 0.097038\n",
            "Epoch: 100/100, Loss: 0.094856\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6742\n",
            "Normalised mutual info score on k-means on latent space: 0.6045183063259794\n",
            "ARI score on k-means on latent space: 0.506933514324261\n",
            "K-means cluster error on latent space: 45054.51953125\n",
            "K-means silhouette score on latent space: 0.1808282 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6835\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.7087112222878619 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.57897294935301 \n",
            "\n",
            "Average k-means accuracy score at latent space: 0.68181 \n",
            "\n",
            "Average k-means NMI score at latent space: 0.6099299194356549 \n",
            "\n",
            "Average k-means ARI score at latent space: 0.5161516526117795 \n",
            "\n",
            "Average agglomerative clustering accuracy score at latent space: 0.6991799999999999 \n",
            "\n",
            "Average agglomerative clustering NMI score at latent space: 0.6821415070375043 \n",
            "\n",
            "Average agglomerative clustering ARI score at latent space: 0.5647784593993722 \n",
            "\n",
            "Average k-means silhouette score on latent space: 0.18389072567224501 \n",
            "\n",
            "Average k-means cluster error on latent space: 44371.671875 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_for_k_150 = run_experiment(150, dataloader, 784, datapoints = images, datapoints_reshaped = images_reshaped, labels = labels, epochs=100, hl1_neurons=32, hl2_neurons=64, hl3_neurons=128, latent_dimension=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhFaEGm_7yr5",
        "outputId": "80006ddf-ebe5-4e87-ac3d-b9825c277507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment results for k-means with k = 150 clusters:\n",
            "\n",
            "Running k-means algorithm in order to get our pseudolabels: \n",
            "\n",
            "K_means greedy accuracy score (initial space): 0.8738\n",
            "Normalised mutual info score (initial space): 0.7705088909624284\n",
            "ARI (initial space): 0.7506550534402108 \n",
            "\n",
            "Using the autoencoder model on our data: \n",
            "\n",
            "ROUND NUMBER  1 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.924478\n",
            "Epoch: 2/100, Loss: 0.730658\n",
            "Epoch: 3/100, Loss: 0.655697\n",
            "Epoch: 4/100, Loss: 0.618224\n",
            "Epoch: 5/100, Loss: 0.593085\n",
            "Epoch: 6/100, Loss: 0.573992\n",
            "Epoch: 7/100, Loss: 0.558596\n",
            "Epoch: 8/100, Loss: 0.545362\n",
            "Epoch: 9/100, Loss: 0.533477\n",
            "Epoch: 10/100, Loss: 0.522493\n",
            "Epoch: 11/100, Loss: 0.512200\n",
            "Epoch: 12/100, Loss: 0.502451\n",
            "Epoch: 13/100, Loss: 0.493148\n",
            "Epoch: 14/100, Loss: 0.484221\n",
            "Epoch: 15/100, Loss: 0.475622\n",
            "Epoch: 16/100, Loss: 0.467298\n",
            "Epoch: 17/100, Loss: 0.459218\n",
            "Epoch: 18/100, Loss: 0.451361\n",
            "Epoch: 19/100, Loss: 0.443704\n",
            "Epoch: 20/100, Loss: 0.436237\n",
            "Epoch: 21/100, Loss: 0.428936\n",
            "Epoch: 22/100, Loss: 0.421794\n",
            "Epoch: 23/100, Loss: 0.414801\n",
            "Epoch: 24/100, Loss: 0.407958\n",
            "Epoch: 25/100, Loss: 0.401245\n",
            "Epoch: 26/100, Loss: 0.394656\n",
            "Epoch: 27/100, Loss: 0.388190\n",
            "Epoch: 28/100, Loss: 0.381835\n",
            "Epoch: 29/100, Loss: 0.375588\n",
            "Epoch: 30/100, Loss: 0.369445\n",
            "Epoch: 31/100, Loss: 0.363403\n",
            "Epoch: 32/100, Loss: 0.357457\n",
            "Epoch: 33/100, Loss: 0.351602\n",
            "Epoch: 34/100, Loss: 0.345835\n",
            "Epoch: 35/100, Loss: 0.340155\n",
            "Epoch: 36/100, Loss: 0.334559\n",
            "Epoch: 37/100, Loss: 0.329041\n",
            "Epoch: 38/100, Loss: 0.323599\n",
            "Epoch: 39/100, Loss: 0.318235\n",
            "Epoch: 40/100, Loss: 0.312945\n",
            "Epoch: 41/100, Loss: 0.307726\n",
            "Epoch: 42/100, Loss: 0.302575\n",
            "Epoch: 43/100, Loss: 0.297489\n",
            "Epoch: 44/100, Loss: 0.292473\n",
            "Epoch: 45/100, Loss: 0.287519\n",
            "Epoch: 46/100, Loss: 0.282634\n",
            "Epoch: 47/100, Loss: 0.277812\n",
            "Epoch: 48/100, Loss: 0.273050\n",
            "Epoch: 49/100, Loss: 0.268344\n",
            "Epoch: 50/100, Loss: 0.263701\n",
            "Epoch: 51/100, Loss: 0.259114\n",
            "Epoch: 52/100, Loss: 0.254584\n",
            "Epoch: 53/100, Loss: 0.250110\n",
            "Epoch: 54/100, Loss: 0.245692\n",
            "Epoch: 55/100, Loss: 0.241333\n",
            "Epoch: 56/100, Loss: 0.237031\n",
            "Epoch: 57/100, Loss: 0.232781\n",
            "Epoch: 58/100, Loss: 0.228585\n",
            "Epoch: 59/100, Loss: 0.224437\n",
            "Epoch: 60/100, Loss: 0.220342\n",
            "Epoch: 61/100, Loss: 0.216304\n",
            "Epoch: 62/100, Loss: 0.212313\n",
            "Epoch: 63/100, Loss: 0.208373\n",
            "Epoch: 64/100, Loss: 0.204485\n",
            "Epoch: 65/100, Loss: 0.200640\n",
            "Epoch: 66/100, Loss: 0.196849\n",
            "Epoch: 67/100, Loss: 0.193109\n",
            "Epoch: 68/100, Loss: 0.189418\n",
            "Epoch: 69/100, Loss: 0.185780\n",
            "Epoch: 70/100, Loss: 0.182186\n",
            "Epoch: 71/100, Loss: 0.178643\n",
            "Epoch: 72/100, Loss: 0.175147\n",
            "Epoch: 73/100, Loss: 0.171698\n",
            "Epoch: 74/100, Loss: 0.168297\n",
            "Epoch: 75/100, Loss: 0.164943\n",
            "Epoch: 76/100, Loss: 0.161641\n",
            "Epoch: 77/100, Loss: 0.158387\n",
            "Epoch: 78/100, Loss: 0.155189\n",
            "Epoch: 79/100, Loss: 0.152049\n",
            "Epoch: 80/100, Loss: 0.148959\n",
            "Epoch: 81/100, Loss: 0.145918\n",
            "Epoch: 82/100, Loss: 0.142906\n",
            "Epoch: 83/100, Loss: 0.139922\n",
            "Epoch: 84/100, Loss: 0.136966\n",
            "Epoch: 85/100, Loss: 0.134071\n",
            "Epoch: 86/100, Loss: 0.131234\n",
            "Epoch: 87/100, Loss: 0.128440\n",
            "Epoch: 88/100, Loss: 0.125676\n",
            "Epoch: 89/100, Loss: 0.122957\n",
            "Epoch: 90/100, Loss: 0.120305\n",
            "Epoch: 91/100, Loss: 0.117689\n",
            "Epoch: 92/100, Loss: 0.115120\n",
            "Epoch: 93/100, Loss: 0.112589\n",
            "Epoch: 94/100, Loss: 0.110095\n",
            "Epoch: 95/100, Loss: 0.107602\n",
            "Epoch: 96/100, Loss: 0.105118\n",
            "Epoch: 97/100, Loss: 0.102670\n",
            "Epoch: 98/100, Loss: 0.100264\n",
            "Epoch: 99/100, Loss: 0.097902\n",
            "Epoch: 100/100, Loss: 0.095575\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.709\n",
            "Normalised mutual info score on k-means on latent space: 0.6208099604679983\n",
            "ARI score on k-means on latent space: 0.5392144679391733\n",
            "K-means cluster error on latent space: 44309.890625\n",
            "K-means silhouette score on latent space: 0.18447007 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7129\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6858074398785301 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5773288507104261 \n",
            "\n",
            "ROUND NUMBER  2 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.945324\n",
            "Epoch: 2/100, Loss: 0.754278\n",
            "Epoch: 3/100, Loss: 0.664845\n",
            "Epoch: 4/100, Loss: 0.619924\n",
            "Epoch: 5/100, Loss: 0.592602\n",
            "Epoch: 6/100, Loss: 0.573193\n",
            "Epoch: 7/100, Loss: 0.557598\n",
            "Epoch: 8/100, Loss: 0.544171\n",
            "Epoch: 9/100, Loss: 0.532152\n",
            "Epoch: 10/100, Loss: 0.521097\n",
            "Epoch: 11/100, Loss: 0.510768\n",
            "Epoch: 12/100, Loss: 0.501005\n",
            "Epoch: 13/100, Loss: 0.491704\n",
            "Epoch: 14/100, Loss: 0.482786\n",
            "Epoch: 15/100, Loss: 0.474199\n",
            "Epoch: 16/100, Loss: 0.465890\n",
            "Epoch: 17/100, Loss: 0.457835\n",
            "Epoch: 18/100, Loss: 0.450009\n",
            "Epoch: 19/100, Loss: 0.442393\n",
            "Epoch: 20/100, Loss: 0.434967\n",
            "Epoch: 21/100, Loss: 0.427718\n",
            "Epoch: 22/100, Loss: 0.420627\n",
            "Epoch: 23/100, Loss: 0.413682\n",
            "Epoch: 24/100, Loss: 0.406881\n",
            "Epoch: 25/100, Loss: 0.400216\n",
            "Epoch: 26/100, Loss: 0.393668\n",
            "Epoch: 27/100, Loss: 0.387240\n",
            "Epoch: 28/100, Loss: 0.380927\n",
            "Epoch: 29/100, Loss: 0.374721\n",
            "Epoch: 30/100, Loss: 0.368617\n",
            "Epoch: 31/100, Loss: 0.362614\n",
            "Epoch: 32/100, Loss: 0.356704\n",
            "Epoch: 33/100, Loss: 0.350884\n",
            "Epoch: 34/100, Loss: 0.345149\n",
            "Epoch: 35/100, Loss: 0.339495\n",
            "Epoch: 36/100, Loss: 0.333927\n",
            "Epoch: 37/100, Loss: 0.328433\n",
            "Epoch: 38/100, Loss: 0.323017\n",
            "Epoch: 39/100, Loss: 0.317676\n",
            "Epoch: 40/100, Loss: 0.312405\n",
            "Epoch: 41/100, Loss: 0.307205\n",
            "Epoch: 42/100, Loss: 0.302075\n",
            "Epoch: 43/100, Loss: 0.297010\n",
            "Epoch: 44/100, Loss: 0.292013\n",
            "Epoch: 45/100, Loss: 0.287077\n",
            "Epoch: 46/100, Loss: 0.282200\n",
            "Epoch: 47/100, Loss: 0.277386\n",
            "Epoch: 48/100, Loss: 0.272629\n",
            "Epoch: 49/100, Loss: 0.267936\n",
            "Epoch: 50/100, Loss: 0.263304\n",
            "Epoch: 51/100, Loss: 0.258728\n",
            "Epoch: 52/100, Loss: 0.254211\n",
            "Epoch: 53/100, Loss: 0.249747\n",
            "Epoch: 54/100, Loss: 0.245339\n",
            "Epoch: 55/100, Loss: 0.240985\n",
            "Epoch: 56/100, Loss: 0.236687\n",
            "Epoch: 57/100, Loss: 0.232443\n",
            "Epoch: 58/100, Loss: 0.228253\n",
            "Epoch: 59/100, Loss: 0.224115\n",
            "Epoch: 60/100, Loss: 0.220030\n",
            "Epoch: 61/100, Loss: 0.215996\n",
            "Epoch: 62/100, Loss: 0.212008\n",
            "Epoch: 63/100, Loss: 0.208075\n",
            "Epoch: 64/100, Loss: 0.204190\n",
            "Epoch: 65/100, Loss: 0.200354\n",
            "Epoch: 66/100, Loss: 0.196569\n",
            "Epoch: 67/100, Loss: 0.192833\n",
            "Epoch: 68/100, Loss: 0.189146\n",
            "Epoch: 69/100, Loss: 0.185507\n",
            "Epoch: 70/100, Loss: 0.181917\n",
            "Epoch: 71/100, Loss: 0.178374\n",
            "Epoch: 72/100, Loss: 0.174878\n",
            "Epoch: 73/100, Loss: 0.171432\n",
            "Epoch: 74/100, Loss: 0.168031\n",
            "Epoch: 75/100, Loss: 0.164676\n",
            "Epoch: 76/100, Loss: 0.161366\n",
            "Epoch: 77/100, Loss: 0.158104\n",
            "Epoch: 78/100, Loss: 0.154887\n",
            "Epoch: 79/100, Loss: 0.151720\n",
            "Epoch: 80/100, Loss: 0.148602\n",
            "Epoch: 81/100, Loss: 0.145534\n",
            "Epoch: 82/100, Loss: 0.142523\n",
            "Epoch: 83/100, Loss: 0.139568\n",
            "Epoch: 84/100, Loss: 0.136681\n",
            "Epoch: 85/100, Loss: 0.133869\n",
            "Epoch: 86/100, Loss: 0.131117\n",
            "Epoch: 87/100, Loss: 0.128310\n",
            "Epoch: 88/100, Loss: 0.125420\n",
            "Epoch: 89/100, Loss: 0.122596\n",
            "Epoch: 90/100, Loss: 0.119868\n",
            "Epoch: 91/100, Loss: 0.117214\n",
            "Epoch: 92/100, Loss: 0.114616\n",
            "Epoch: 93/100, Loss: 0.112068\n",
            "Epoch: 94/100, Loss: 0.109553\n",
            "Epoch: 95/100, Loss: 0.107077\n",
            "Epoch: 96/100, Loss: 0.104660\n",
            "Epoch: 97/100, Loss: 0.102296\n",
            "Epoch: 98/100, Loss: 0.099977\n",
            "Epoch: 99/100, Loss: 0.097695\n",
            "Epoch: 100/100, Loss: 0.095451\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6688\n",
            "Normalised mutual info score on k-means on latent space: 0.5927181557188638\n",
            "ARI score on k-means on latent space: 0.5005083198444008\n",
            "K-means cluster error on latent space: 44416.68359375\n",
            "K-means silhouette score on latent space: 0.19045684 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7168\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6818466356069642 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5765426068775162 \n",
            "\n",
            "ROUND NUMBER  3 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.932289\n",
            "Epoch: 2/100, Loss: 0.737030\n",
            "Epoch: 3/100, Loss: 0.654312\n",
            "Epoch: 4/100, Loss: 0.613826\n",
            "Epoch: 5/100, Loss: 0.588451\n",
            "Epoch: 6/100, Loss: 0.570189\n",
            "Epoch: 7/100, Loss: 0.555468\n",
            "Epoch: 8/100, Loss: 0.542664\n",
            "Epoch: 9/100, Loss: 0.531079\n",
            "Epoch: 10/100, Loss: 0.520329\n",
            "Epoch: 11/100, Loss: 0.510207\n",
            "Epoch: 12/100, Loss: 0.500572\n",
            "Epoch: 13/100, Loss: 0.491336\n",
            "Epoch: 14/100, Loss: 0.482451\n",
            "Epoch: 15/100, Loss: 0.473862\n",
            "Epoch: 16/100, Loss: 0.465552\n",
            "Epoch: 17/100, Loss: 0.457488\n",
            "Epoch: 18/100, Loss: 0.449655\n",
            "Epoch: 19/100, Loss: 0.442028\n",
            "Epoch: 20/100, Loss: 0.434582\n",
            "Epoch: 21/100, Loss: 0.427310\n",
            "Epoch: 22/100, Loss: 0.420200\n",
            "Epoch: 23/100, Loss: 0.413235\n",
            "Epoch: 24/100, Loss: 0.406413\n",
            "Epoch: 25/100, Loss: 0.399729\n",
            "Epoch: 26/100, Loss: 0.393170\n",
            "Epoch: 27/100, Loss: 0.386733\n",
            "Epoch: 28/100, Loss: 0.380413\n",
            "Epoch: 29/100, Loss: 0.374198\n",
            "Epoch: 30/100, Loss: 0.368086\n",
            "Epoch: 31/100, Loss: 0.362075\n",
            "Epoch: 32/100, Loss: 0.356160\n",
            "Epoch: 33/100, Loss: 0.350337\n",
            "Epoch: 34/100, Loss: 0.344602\n",
            "Epoch: 35/100, Loss: 0.338954\n",
            "Epoch: 36/100, Loss: 0.333384\n",
            "Epoch: 37/100, Loss: 0.327895\n",
            "Epoch: 38/100, Loss: 0.322480\n",
            "Epoch: 39/100, Loss: 0.317144\n",
            "Epoch: 40/100, Loss: 0.311880\n",
            "Epoch: 41/100, Loss: 0.306686\n",
            "Epoch: 42/100, Loss: 0.301563\n",
            "Epoch: 43/100, Loss: 0.296508\n",
            "Epoch: 44/100, Loss: 0.291515\n",
            "Epoch: 45/100, Loss: 0.286589\n",
            "Epoch: 46/100, Loss: 0.281724\n",
            "Epoch: 47/100, Loss: 0.276922\n",
            "Epoch: 48/100, Loss: 0.272179\n",
            "Epoch: 49/100, Loss: 0.267496\n",
            "Epoch: 50/100, Loss: 0.262874\n",
            "Epoch: 51/100, Loss: 0.258311\n",
            "Epoch: 52/100, Loss: 0.253803\n",
            "Epoch: 53/100, Loss: 0.249349\n",
            "Epoch: 54/100, Loss: 0.244949\n",
            "Epoch: 55/100, Loss: 0.240604\n",
            "Epoch: 56/100, Loss: 0.236315\n",
            "Epoch: 57/100, Loss: 0.232078\n",
            "Epoch: 58/100, Loss: 0.227891\n",
            "Epoch: 59/100, Loss: 0.223760\n",
            "Epoch: 60/100, Loss: 0.219678\n",
            "Epoch: 61/100, Loss: 0.215650\n",
            "Epoch: 62/100, Loss: 0.211673\n",
            "Epoch: 63/100, Loss: 0.207750\n",
            "Epoch: 64/100, Loss: 0.203873\n",
            "Epoch: 65/100, Loss: 0.200045\n",
            "Epoch: 66/100, Loss: 0.196269\n",
            "Epoch: 67/100, Loss: 0.192540\n",
            "Epoch: 68/100, Loss: 0.188859\n",
            "Epoch: 69/100, Loss: 0.185224\n",
            "Epoch: 70/100, Loss: 0.181639\n",
            "Epoch: 71/100, Loss: 0.178104\n",
            "Epoch: 72/100, Loss: 0.174620\n",
            "Epoch: 73/100, Loss: 0.171179\n",
            "Epoch: 74/100, Loss: 0.167783\n",
            "Epoch: 75/100, Loss: 0.164435\n",
            "Epoch: 76/100, Loss: 0.161136\n",
            "Epoch: 77/100, Loss: 0.157879\n",
            "Epoch: 78/100, Loss: 0.154669\n",
            "Epoch: 79/100, Loss: 0.151498\n",
            "Epoch: 80/100, Loss: 0.148373\n",
            "Epoch: 81/100, Loss: 0.145297\n",
            "Epoch: 82/100, Loss: 0.142268\n",
            "Epoch: 83/100, Loss: 0.139291\n",
            "Epoch: 84/100, Loss: 0.136369\n",
            "Epoch: 85/100, Loss: 0.133503\n",
            "Epoch: 86/100, Loss: 0.130689\n",
            "Epoch: 87/100, Loss: 0.127913\n",
            "Epoch: 88/100, Loss: 0.125153\n",
            "Epoch: 89/100, Loss: 0.122400\n",
            "Epoch: 90/100, Loss: 0.119679\n",
            "Epoch: 91/100, Loss: 0.117013\n",
            "Epoch: 92/100, Loss: 0.114412\n",
            "Epoch: 93/100, Loss: 0.111869\n",
            "Epoch: 94/100, Loss: 0.109368\n",
            "Epoch: 95/100, Loss: 0.106911\n",
            "Epoch: 96/100, Loss: 0.104500\n",
            "Epoch: 97/100, Loss: 0.102135\n",
            "Epoch: 98/100, Loss: 0.099805\n",
            "Epoch: 99/100, Loss: 0.097520\n",
            "Epoch: 100/100, Loss: 0.095274\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6386\n",
            "Normalised mutual info score on k-means on latent space: 0.5948016984539929\n",
            "ARI score on k-means on latent space: 0.48563668375158914\n",
            "K-means cluster error on latent space: 44888.38671875\n",
            "K-means silhouette score on latent space: 0.19127029 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7051\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.7053762268689677 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5757322445922224 \n",
            "\n",
            "ROUND NUMBER  4 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.933759\n",
            "Epoch: 2/100, Loss: 0.746361\n",
            "Epoch: 3/100, Loss: 0.665939\n",
            "Epoch: 4/100, Loss: 0.622163\n",
            "Epoch: 5/100, Loss: 0.593714\n",
            "Epoch: 6/100, Loss: 0.572748\n",
            "Epoch: 7/100, Loss: 0.556559\n",
            "Epoch: 8/100, Loss: 0.543096\n",
            "Epoch: 9/100, Loss: 0.531183\n",
            "Epoch: 10/100, Loss: 0.520304\n",
            "Epoch: 11/100, Loss: 0.510157\n",
            "Epoch: 12/100, Loss: 0.500564\n",
            "Epoch: 13/100, Loss: 0.491417\n",
            "Epoch: 14/100, Loss: 0.482645\n",
            "Epoch: 15/100, Loss: 0.474184\n",
            "Epoch: 16/100, Loss: 0.465995\n",
            "Epoch: 17/100, Loss: 0.458043\n",
            "Epoch: 18/100, Loss: 0.450304\n",
            "Epoch: 19/100, Loss: 0.442760\n",
            "Epoch: 20/100, Loss: 0.435396\n",
            "Epoch: 21/100, Loss: 0.428189\n",
            "Epoch: 22/100, Loss: 0.421136\n",
            "Epoch: 23/100, Loss: 0.414219\n",
            "Epoch: 24/100, Loss: 0.407444\n",
            "Epoch: 25/100, Loss: 0.400792\n",
            "Epoch: 26/100, Loss: 0.394258\n",
            "Epoch: 27/100, Loss: 0.387845\n",
            "Epoch: 28/100, Loss: 0.381536\n",
            "Epoch: 29/100, Loss: 0.375332\n",
            "Epoch: 30/100, Loss: 0.369230\n",
            "Epoch: 31/100, Loss: 0.363224\n",
            "Epoch: 32/100, Loss: 0.357309\n",
            "Epoch: 33/100, Loss: 0.351479\n",
            "Epoch: 34/100, Loss: 0.345735\n",
            "Epoch: 35/100, Loss: 0.340075\n",
            "Epoch: 36/100, Loss: 0.334495\n",
            "Epoch: 37/100, Loss: 0.328997\n",
            "Epoch: 38/100, Loss: 0.323573\n",
            "Epoch: 39/100, Loss: 0.318223\n",
            "Epoch: 40/100, Loss: 0.312945\n",
            "Epoch: 41/100, Loss: 0.307740\n",
            "Epoch: 42/100, Loss: 0.302600\n",
            "Epoch: 43/100, Loss: 0.297530\n",
            "Epoch: 44/100, Loss: 0.292521\n",
            "Epoch: 45/100, Loss: 0.287574\n",
            "Epoch: 46/100, Loss: 0.282690\n",
            "Epoch: 47/100, Loss: 0.277871\n",
            "Epoch: 48/100, Loss: 0.273113\n",
            "Epoch: 49/100, Loss: 0.268410\n",
            "Epoch: 50/100, Loss: 0.263765\n",
            "Epoch: 51/100, Loss: 0.259184\n",
            "Epoch: 52/100, Loss: 0.254656\n",
            "Epoch: 53/100, Loss: 0.250189\n",
            "Epoch: 54/100, Loss: 0.245778\n",
            "Epoch: 55/100, Loss: 0.241419\n",
            "Epoch: 56/100, Loss: 0.237116\n",
            "Epoch: 57/100, Loss: 0.232867\n",
            "Epoch: 58/100, Loss: 0.228670\n",
            "Epoch: 59/100, Loss: 0.224527\n",
            "Epoch: 60/100, Loss: 0.220433\n",
            "Epoch: 61/100, Loss: 0.216394\n",
            "Epoch: 62/100, Loss: 0.212402\n",
            "Epoch: 63/100, Loss: 0.208465\n",
            "Epoch: 64/100, Loss: 0.204578\n",
            "Epoch: 65/100, Loss: 0.200738\n",
            "Epoch: 66/100, Loss: 0.196947\n",
            "Epoch: 67/100, Loss: 0.193206\n",
            "Epoch: 68/100, Loss: 0.189515\n",
            "Epoch: 69/100, Loss: 0.185872\n",
            "Epoch: 70/100, Loss: 0.182275\n",
            "Epoch: 71/100, Loss: 0.178728\n",
            "Epoch: 72/100, Loss: 0.175226\n",
            "Epoch: 73/100, Loss: 0.171771\n",
            "Epoch: 74/100, Loss: 0.168366\n",
            "Epoch: 75/100, Loss: 0.165006\n",
            "Epoch: 76/100, Loss: 0.161694\n",
            "Epoch: 77/100, Loss: 0.158431\n",
            "Epoch: 78/100, Loss: 0.155209\n",
            "Epoch: 79/100, Loss: 0.152034\n",
            "Epoch: 80/100, Loss: 0.148904\n",
            "Epoch: 81/100, Loss: 0.145821\n",
            "Epoch: 82/100, Loss: 0.142785\n",
            "Epoch: 83/100, Loss: 0.139800\n",
            "Epoch: 84/100, Loss: 0.136870\n",
            "Epoch: 85/100, Loss: 0.133994\n",
            "Epoch: 86/100, Loss: 0.131171\n",
            "Epoch: 87/100, Loss: 0.128380\n",
            "Epoch: 88/100, Loss: 0.125617\n",
            "Epoch: 89/100, Loss: 0.122896\n",
            "Epoch: 90/100, Loss: 0.120222\n",
            "Epoch: 91/100, Loss: 0.117598\n",
            "Epoch: 92/100, Loss: 0.114982\n",
            "Epoch: 93/100, Loss: 0.112392\n",
            "Epoch: 94/100, Loss: 0.109834\n",
            "Epoch: 95/100, Loss: 0.107335\n",
            "Epoch: 96/100, Loss: 0.104898\n",
            "Epoch: 97/100, Loss: 0.102494\n",
            "Epoch: 98/100, Loss: 0.100133\n",
            "Epoch: 99/100, Loss: 0.097813\n",
            "Epoch: 100/100, Loss: 0.095540\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6561\n",
            "Normalised mutual info score on k-means on latent space: 0.597601623666904\n",
            "ARI score on k-means on latent space: 0.49063329984326975\n",
            "K-means cluster error on latent space: 41097.05859375\n",
            "K-means silhouette score on latent space: 0.19508693 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7152\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6961089409274792 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5756857434889532 \n",
            "\n",
            "ROUND NUMBER  5 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.980074\n",
            "Epoch: 2/100, Loss: 0.786234\n",
            "Epoch: 3/100, Loss: 0.687507\n",
            "Epoch: 4/100, Loss: 0.629897\n",
            "Epoch: 5/100, Loss: 0.597888\n",
            "Epoch: 6/100, Loss: 0.576569\n",
            "Epoch: 7/100, Loss: 0.559842\n",
            "Epoch: 8/100, Loss: 0.545748\n",
            "Epoch: 9/100, Loss: 0.533280\n",
            "Epoch: 10/100, Loss: 0.521903\n",
            "Epoch: 11/100, Loss: 0.511353\n",
            "Epoch: 12/100, Loss: 0.501455\n",
            "Epoch: 13/100, Loss: 0.492063\n",
            "Epoch: 14/100, Loss: 0.483095\n",
            "Epoch: 15/100, Loss: 0.474472\n",
            "Epoch: 16/100, Loss: 0.466153\n",
            "Epoch: 17/100, Loss: 0.458098\n",
            "Epoch: 18/100, Loss: 0.450274\n",
            "Epoch: 19/100, Loss: 0.442663\n",
            "Epoch: 20/100, Loss: 0.435237\n",
            "Epoch: 21/100, Loss: 0.427989\n",
            "Epoch: 22/100, Loss: 0.420898\n",
            "Epoch: 23/100, Loss: 0.413959\n",
            "Epoch: 24/100, Loss: 0.407162\n",
            "Epoch: 25/100, Loss: 0.400499\n",
            "Epoch: 26/100, Loss: 0.393957\n",
            "Epoch: 27/100, Loss: 0.387531\n",
            "Epoch: 28/100, Loss: 0.381223\n",
            "Epoch: 29/100, Loss: 0.375020\n",
            "Epoch: 30/100, Loss: 0.368917\n",
            "Epoch: 31/100, Loss: 0.362914\n",
            "Epoch: 32/100, Loss: 0.357003\n",
            "Epoch: 33/100, Loss: 0.351179\n",
            "Epoch: 34/100, Loss: 0.345443\n",
            "Epoch: 35/100, Loss: 0.339792\n",
            "Epoch: 36/100, Loss: 0.334222\n",
            "Epoch: 37/100, Loss: 0.328726\n",
            "Epoch: 38/100, Loss: 0.323310\n",
            "Epoch: 39/100, Loss: 0.317971\n",
            "Epoch: 40/100, Loss: 0.312697\n",
            "Epoch: 41/100, Loss: 0.307495\n",
            "Epoch: 42/100, Loss: 0.302362\n",
            "Epoch: 43/100, Loss: 0.297299\n",
            "Epoch: 44/100, Loss: 0.292300\n",
            "Epoch: 45/100, Loss: 0.287363\n",
            "Epoch: 46/100, Loss: 0.282488\n",
            "Epoch: 47/100, Loss: 0.277675\n",
            "Epoch: 48/100, Loss: 0.272923\n",
            "Epoch: 49/100, Loss: 0.268233\n",
            "Epoch: 50/100, Loss: 0.263601\n",
            "Epoch: 51/100, Loss: 0.259020\n",
            "Epoch: 52/100, Loss: 0.254499\n",
            "Epoch: 53/100, Loss: 0.250034\n",
            "Epoch: 54/100, Loss: 0.245624\n",
            "Epoch: 55/100, Loss: 0.241270\n",
            "Epoch: 56/100, Loss: 0.236968\n",
            "Epoch: 57/100, Loss: 0.232723\n",
            "Epoch: 58/100, Loss: 0.228526\n",
            "Epoch: 59/100, Loss: 0.224384\n",
            "Epoch: 60/100, Loss: 0.220294\n",
            "Epoch: 61/100, Loss: 0.216254\n",
            "Epoch: 62/100, Loss: 0.212265\n",
            "Epoch: 63/100, Loss: 0.208329\n",
            "Epoch: 64/100, Loss: 0.204439\n",
            "Epoch: 65/100, Loss: 0.200600\n",
            "Epoch: 66/100, Loss: 0.196809\n",
            "Epoch: 67/100, Loss: 0.193067\n",
            "Epoch: 68/100, Loss: 0.189374\n",
            "Epoch: 69/100, Loss: 0.185732\n",
            "Epoch: 70/100, Loss: 0.182136\n",
            "Epoch: 71/100, Loss: 0.178587\n",
            "Epoch: 72/100, Loss: 0.175085\n",
            "Epoch: 73/100, Loss: 0.171631\n",
            "Epoch: 74/100, Loss: 0.168221\n",
            "Epoch: 75/100, Loss: 0.164859\n",
            "Epoch: 76/100, Loss: 0.161548\n",
            "Epoch: 77/100, Loss: 0.158282\n",
            "Epoch: 78/100, Loss: 0.155059\n",
            "Epoch: 79/100, Loss: 0.151884\n",
            "Epoch: 80/100, Loss: 0.148755\n",
            "Epoch: 81/100, Loss: 0.145673\n",
            "Epoch: 82/100, Loss: 0.142643\n",
            "Epoch: 83/100, Loss: 0.139657\n",
            "Epoch: 84/100, Loss: 0.136725\n",
            "Epoch: 85/100, Loss: 0.133850\n",
            "Epoch: 86/100, Loss: 0.131032\n",
            "Epoch: 87/100, Loss: 0.128253\n",
            "Epoch: 88/100, Loss: 0.125502\n",
            "Epoch: 89/100, Loss: 0.122779\n",
            "Epoch: 90/100, Loss: 0.120072\n",
            "Epoch: 91/100, Loss: 0.117414\n",
            "Epoch: 92/100, Loss: 0.114784\n",
            "Epoch: 93/100, Loss: 0.112202\n",
            "Epoch: 94/100, Loss: 0.109642\n",
            "Epoch: 95/100, Loss: 0.107125\n",
            "Epoch: 96/100, Loss: 0.104660\n",
            "Epoch: 97/100, Loss: 0.102252\n",
            "Epoch: 98/100, Loss: 0.099898\n",
            "Epoch: 99/100, Loss: 0.097590\n",
            "Epoch: 100/100, Loss: 0.095327\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.7136\n",
            "Normalised mutual info score on k-means on latent space: 0.6331671440901108\n",
            "ARI score on k-means on latent space: 0.5525850295741114\n",
            "K-means cluster error on latent space: 41910.20703125\n",
            "K-means silhouette score on latent space: 0.1954515 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6769\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.668503756599903 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5327907481079761 \n",
            "\n",
            "ROUND NUMBER  6 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.940037\n",
            "Epoch: 2/100, Loss: 0.730642\n",
            "Epoch: 3/100, Loss: 0.654897\n",
            "Epoch: 4/100, Loss: 0.617713\n",
            "Epoch: 5/100, Loss: 0.592999\n",
            "Epoch: 6/100, Loss: 0.573880\n",
            "Epoch: 7/100, Loss: 0.558067\n",
            "Epoch: 8/100, Loss: 0.544314\n",
            "Epoch: 9/100, Loss: 0.532025\n",
            "Epoch: 10/100, Loss: 0.520816\n",
            "Epoch: 11/100, Loss: 0.510404\n",
            "Epoch: 12/100, Loss: 0.500594\n",
            "Epoch: 13/100, Loss: 0.491265\n",
            "Epoch: 14/100, Loss: 0.482332\n",
            "Epoch: 15/100, Loss: 0.473737\n",
            "Epoch: 16/100, Loss: 0.465429\n",
            "Epoch: 17/100, Loss: 0.457369\n",
            "Epoch: 18/100, Loss: 0.449542\n",
            "Epoch: 19/100, Loss: 0.441926\n",
            "Epoch: 20/100, Loss: 0.434502\n",
            "Epoch: 21/100, Loss: 0.427254\n",
            "Epoch: 22/100, Loss: 0.420168\n",
            "Epoch: 23/100, Loss: 0.413234\n",
            "Epoch: 24/100, Loss: 0.406447\n",
            "Epoch: 25/100, Loss: 0.399785\n",
            "Epoch: 26/100, Loss: 0.393247\n",
            "Epoch: 27/100, Loss: 0.386832\n",
            "Epoch: 28/100, Loss: 0.380524\n",
            "Epoch: 29/100, Loss: 0.374315\n",
            "Epoch: 30/100, Loss: 0.368216\n",
            "Epoch: 31/100, Loss: 0.362212\n",
            "Epoch: 32/100, Loss: 0.356302\n",
            "Epoch: 33/100, Loss: 0.350479\n",
            "Epoch: 34/100, Loss: 0.344744\n",
            "Epoch: 35/100, Loss: 0.339092\n",
            "Epoch: 36/100, Loss: 0.333523\n",
            "Epoch: 37/100, Loss: 0.328035\n",
            "Epoch: 38/100, Loss: 0.322623\n",
            "Epoch: 39/100, Loss: 0.317284\n",
            "Epoch: 40/100, Loss: 0.312018\n",
            "Epoch: 41/100, Loss: 0.306819\n",
            "Epoch: 42/100, Loss: 0.301688\n",
            "Epoch: 43/100, Loss: 0.296621\n",
            "Epoch: 44/100, Loss: 0.291621\n",
            "Epoch: 45/100, Loss: 0.286687\n",
            "Epoch: 46/100, Loss: 0.281812\n",
            "Epoch: 47/100, Loss: 0.277004\n",
            "Epoch: 48/100, Loss: 0.272254\n",
            "Epoch: 49/100, Loss: 0.267565\n",
            "Epoch: 50/100, Loss: 0.262935\n",
            "Epoch: 51/100, Loss: 0.258362\n",
            "Epoch: 52/100, Loss: 0.253849\n",
            "Epoch: 53/100, Loss: 0.249390\n",
            "Epoch: 54/100, Loss: 0.244988\n",
            "Epoch: 55/100, Loss: 0.240639\n",
            "Epoch: 56/100, Loss: 0.236343\n",
            "Epoch: 57/100, Loss: 0.232106\n",
            "Epoch: 58/100, Loss: 0.227921\n",
            "Epoch: 59/100, Loss: 0.223787\n",
            "Epoch: 60/100, Loss: 0.219703\n",
            "Epoch: 61/100, Loss: 0.215674\n",
            "Epoch: 62/100, Loss: 0.211692\n",
            "Epoch: 63/100, Loss: 0.207759\n",
            "Epoch: 64/100, Loss: 0.203873\n",
            "Epoch: 65/100, Loss: 0.200040\n",
            "Epoch: 66/100, Loss: 0.196256\n",
            "Epoch: 67/100, Loss: 0.192523\n",
            "Epoch: 68/100, Loss: 0.188839\n",
            "Epoch: 69/100, Loss: 0.185200\n",
            "Epoch: 70/100, Loss: 0.181609\n",
            "Epoch: 71/100, Loss: 0.178070\n",
            "Epoch: 72/100, Loss: 0.174576\n",
            "Epoch: 73/100, Loss: 0.171128\n",
            "Epoch: 74/100, Loss: 0.167728\n",
            "Epoch: 75/100, Loss: 0.164372\n",
            "Epoch: 76/100, Loss: 0.161062\n",
            "Epoch: 77/100, Loss: 0.157797\n",
            "Epoch: 78/100, Loss: 0.154584\n",
            "Epoch: 79/100, Loss: 0.151413\n",
            "Epoch: 80/100, Loss: 0.148284\n",
            "Epoch: 81/100, Loss: 0.145198\n",
            "Epoch: 82/100, Loss: 0.142163\n",
            "Epoch: 83/100, Loss: 0.139182\n",
            "Epoch: 84/100, Loss: 0.136252\n",
            "Epoch: 85/100, Loss: 0.133369\n",
            "Epoch: 86/100, Loss: 0.130533\n",
            "Epoch: 87/100, Loss: 0.127745\n",
            "Epoch: 88/100, Loss: 0.125010\n",
            "Epoch: 89/100, Loss: 0.122349\n",
            "Epoch: 90/100, Loss: 0.119781\n",
            "Epoch: 91/100, Loss: 0.117203\n",
            "Epoch: 92/100, Loss: 0.114560\n",
            "Epoch: 93/100, Loss: 0.111932\n",
            "Epoch: 94/100, Loss: 0.109388\n",
            "Epoch: 95/100, Loss: 0.106886\n",
            "Epoch: 96/100, Loss: 0.104431\n",
            "Epoch: 97/100, Loss: 0.102028\n",
            "Epoch: 98/100, Loss: 0.099687\n",
            "Epoch: 99/100, Loss: 0.097411\n",
            "Epoch: 100/100, Loss: 0.095203\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6486\n",
            "Normalised mutual info score on k-means on latent space: 0.5838990817375467\n",
            "ARI score on k-means on latent space: 0.4790566740410823\n",
            "K-means cluster error on latent space: 45756.32421875\n",
            "K-means silhouette score on latent space: 0.17379993 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6545\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.643941147599645 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5074462893002678 \n",
            "\n",
            "ROUND NUMBER  7 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.924477\n",
            "Epoch: 2/100, Loss: 0.721664\n",
            "Epoch: 3/100, Loss: 0.646310\n",
            "Epoch: 4/100, Loss: 0.610786\n",
            "Epoch: 5/100, Loss: 0.587828\n",
            "Epoch: 6/100, Loss: 0.570082\n",
            "Epoch: 7/100, Loss: 0.555300\n",
            "Epoch: 8/100, Loss: 0.542375\n",
            "Epoch: 9/100, Loss: 0.530650\n",
            "Epoch: 10/100, Loss: 0.519789\n",
            "Epoch: 11/100, Loss: 0.509583\n",
            "Epoch: 12/100, Loss: 0.499892\n",
            "Epoch: 13/100, Loss: 0.490638\n",
            "Epoch: 14/100, Loss: 0.481754\n",
            "Epoch: 15/100, Loss: 0.473190\n",
            "Epoch: 16/100, Loss: 0.464910\n",
            "Epoch: 17/100, Loss: 0.456882\n",
            "Epoch: 18/100, Loss: 0.449081\n",
            "Epoch: 19/100, Loss: 0.441479\n",
            "Epoch: 20/100, Loss: 0.434063\n",
            "Epoch: 21/100, Loss: 0.426820\n",
            "Epoch: 22/100, Loss: 0.419732\n",
            "Epoch: 23/100, Loss: 0.412790\n",
            "Epoch: 24/100, Loss: 0.405987\n",
            "Epoch: 25/100, Loss: 0.399320\n",
            "Epoch: 26/100, Loss: 0.392777\n",
            "Epoch: 27/100, Loss: 0.386352\n",
            "Epoch: 28/100, Loss: 0.380039\n",
            "Epoch: 29/100, Loss: 0.373832\n",
            "Epoch: 30/100, Loss: 0.367730\n",
            "Epoch: 31/100, Loss: 0.361727\n",
            "Epoch: 32/100, Loss: 0.355819\n",
            "Epoch: 33/100, Loss: 0.349999\n",
            "Epoch: 34/100, Loss: 0.344267\n",
            "Epoch: 35/100, Loss: 0.338620\n",
            "Epoch: 36/100, Loss: 0.333056\n",
            "Epoch: 37/100, Loss: 0.327566\n",
            "Epoch: 38/100, Loss: 0.322155\n",
            "Epoch: 39/100, Loss: 0.316819\n",
            "Epoch: 40/100, Loss: 0.311555\n",
            "Epoch: 41/100, Loss: 0.306360\n",
            "Epoch: 42/100, Loss: 0.301237\n",
            "Epoch: 43/100, Loss: 0.296180\n",
            "Epoch: 44/100, Loss: 0.291186\n",
            "Epoch: 45/100, Loss: 0.286258\n",
            "Epoch: 46/100, Loss: 0.281393\n",
            "Epoch: 47/100, Loss: 0.276587\n",
            "Epoch: 48/100, Loss: 0.271845\n",
            "Epoch: 49/100, Loss: 0.267159\n",
            "Epoch: 50/100, Loss: 0.262533\n",
            "Epoch: 51/100, Loss: 0.257968\n",
            "Epoch: 52/100, Loss: 0.253459\n",
            "Epoch: 53/100, Loss: 0.249006\n",
            "Epoch: 54/100, Loss: 0.244608\n",
            "Epoch: 55/100, Loss: 0.240267\n",
            "Epoch: 56/100, Loss: 0.235979\n",
            "Epoch: 57/100, Loss: 0.231745\n",
            "Epoch: 58/100, Loss: 0.227562\n",
            "Epoch: 59/100, Loss: 0.223433\n",
            "Epoch: 60/100, Loss: 0.219356\n",
            "Epoch: 61/100, Loss: 0.215330\n",
            "Epoch: 62/100, Loss: 0.211354\n",
            "Epoch: 63/100, Loss: 0.207426\n",
            "Epoch: 64/100, Loss: 0.203553\n",
            "Epoch: 65/100, Loss: 0.199726\n",
            "Epoch: 66/100, Loss: 0.195951\n",
            "Epoch: 67/100, Loss: 0.192224\n",
            "Epoch: 68/100, Loss: 0.188549\n",
            "Epoch: 69/100, Loss: 0.184921\n",
            "Epoch: 70/100, Loss: 0.181341\n",
            "Epoch: 71/100, Loss: 0.177809\n",
            "Epoch: 72/100, Loss: 0.174325\n",
            "Epoch: 73/100, Loss: 0.170887\n",
            "Epoch: 74/100, Loss: 0.167493\n",
            "Epoch: 75/100, Loss: 0.164147\n",
            "Epoch: 76/100, Loss: 0.160848\n",
            "Epoch: 77/100, Loss: 0.157598\n",
            "Epoch: 78/100, Loss: 0.154398\n",
            "Epoch: 79/100, Loss: 0.151244\n",
            "Epoch: 80/100, Loss: 0.148137\n",
            "Epoch: 81/100, Loss: 0.145080\n",
            "Epoch: 82/100, Loss: 0.142074\n",
            "Epoch: 83/100, Loss: 0.139122\n",
            "Epoch: 84/100, Loss: 0.136227\n",
            "Epoch: 85/100, Loss: 0.133386\n",
            "Epoch: 86/100, Loss: 0.130593\n",
            "Epoch: 87/100, Loss: 0.127828\n",
            "Epoch: 88/100, Loss: 0.125094\n",
            "Epoch: 89/100, Loss: 0.122404\n",
            "Epoch: 90/100, Loss: 0.119738\n",
            "Epoch: 91/100, Loss: 0.117085\n",
            "Epoch: 92/100, Loss: 0.114472\n",
            "Epoch: 93/100, Loss: 0.111905\n",
            "Epoch: 94/100, Loss: 0.109393\n",
            "Epoch: 95/100, Loss: 0.106918\n",
            "Epoch: 96/100, Loss: 0.104485\n",
            "Epoch: 97/100, Loss: 0.102109\n",
            "Epoch: 98/100, Loss: 0.099787\n",
            "Epoch: 99/100, Loss: 0.097490\n",
            "Epoch: 100/100, Loss: 0.095198\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.61\n",
            "Normalised mutual info score on k-means on latent space: 0.5587698164814172\n",
            "ARI score on k-means on latent space: 0.44249205974045236\n",
            "K-means cluster error on latent space: 45553.17578125\n",
            "K-means silhouette score on latent space: 0.17677273 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6579\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.652515659536636 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5195002426654228 \n",
            "\n",
            "ROUND NUMBER  8 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.924210\n",
            "Epoch: 2/100, Loss: 0.741956\n",
            "Epoch: 3/100, Loss: 0.661769\n",
            "Epoch: 4/100, Loss: 0.617350\n",
            "Epoch: 5/100, Loss: 0.590171\n",
            "Epoch: 6/100, Loss: 0.571290\n",
            "Epoch: 7/100, Loss: 0.555759\n",
            "Epoch: 8/100, Loss: 0.542400\n",
            "Epoch: 9/100, Loss: 0.530478\n",
            "Epoch: 10/100, Loss: 0.519606\n",
            "Epoch: 11/100, Loss: 0.509468\n",
            "Epoch: 12/100, Loss: 0.499874\n",
            "Epoch: 13/100, Loss: 0.490715\n",
            "Epoch: 14/100, Loss: 0.481922\n",
            "Epoch: 15/100, Loss: 0.473435\n",
            "Epoch: 16/100, Loss: 0.465213\n",
            "Epoch: 17/100, Loss: 0.457228\n",
            "Epoch: 18/100, Loss: 0.449467\n",
            "Epoch: 19/100, Loss: 0.441902\n",
            "Epoch: 20/100, Loss: 0.434522\n",
            "Epoch: 21/100, Loss: 0.427306\n",
            "Epoch: 22/100, Loss: 0.420253\n",
            "Epoch: 23/100, Loss: 0.413342\n",
            "Epoch: 24/100, Loss: 0.406568\n",
            "Epoch: 25/100, Loss: 0.399923\n",
            "Epoch: 26/100, Loss: 0.393399\n",
            "Epoch: 27/100, Loss: 0.386992\n",
            "Epoch: 28/100, Loss: 0.380694\n",
            "Epoch: 29/100, Loss: 0.374502\n",
            "Epoch: 30/100, Loss: 0.368416\n",
            "Epoch: 31/100, Loss: 0.362420\n",
            "Epoch: 32/100, Loss: 0.356518\n",
            "Epoch: 33/100, Loss: 0.350710\n",
            "Epoch: 34/100, Loss: 0.344985\n",
            "Epoch: 35/100, Loss: 0.339344\n",
            "Epoch: 36/100, Loss: 0.333781\n",
            "Epoch: 37/100, Loss: 0.328297\n",
            "Epoch: 38/100, Loss: 0.322890\n",
            "Epoch: 39/100, Loss: 0.317556\n",
            "Epoch: 40/100, Loss: 0.312297\n",
            "Epoch: 41/100, Loss: 0.307107\n",
            "Epoch: 42/100, Loss: 0.301984\n",
            "Epoch: 43/100, Loss: 0.296927\n",
            "Epoch: 44/100, Loss: 0.291934\n",
            "Epoch: 45/100, Loss: 0.287005\n",
            "Epoch: 46/100, Loss: 0.282137\n",
            "Epoch: 47/100, Loss: 0.277331\n",
            "Epoch: 48/100, Loss: 0.272586\n",
            "Epoch: 49/100, Loss: 0.267900\n",
            "Epoch: 50/100, Loss: 0.263271\n",
            "Epoch: 51/100, Loss: 0.258699\n",
            "Epoch: 52/100, Loss: 0.254184\n",
            "Epoch: 53/100, Loss: 0.249728\n",
            "Epoch: 54/100, Loss: 0.245323\n",
            "Epoch: 55/100, Loss: 0.240978\n",
            "Epoch: 56/100, Loss: 0.236685\n",
            "Epoch: 57/100, Loss: 0.232446\n",
            "Epoch: 58/100, Loss: 0.228258\n",
            "Epoch: 59/100, Loss: 0.224124\n",
            "Epoch: 60/100, Loss: 0.220041\n",
            "Epoch: 61/100, Loss: 0.216012\n",
            "Epoch: 62/100, Loss: 0.212028\n",
            "Epoch: 63/100, Loss: 0.208098\n",
            "Epoch: 64/100, Loss: 0.204218\n",
            "Epoch: 65/100, Loss: 0.200391\n",
            "Epoch: 66/100, Loss: 0.196611\n",
            "Epoch: 67/100, Loss: 0.192884\n",
            "Epoch: 68/100, Loss: 0.189201\n",
            "Epoch: 69/100, Loss: 0.185566\n",
            "Epoch: 70/100, Loss: 0.181979\n",
            "Epoch: 71/100, Loss: 0.178439\n",
            "Epoch: 72/100, Loss: 0.174946\n",
            "Epoch: 73/100, Loss: 0.171501\n",
            "Epoch: 74/100, Loss: 0.168101\n",
            "Epoch: 75/100, Loss: 0.164748\n",
            "Epoch: 76/100, Loss: 0.161442\n",
            "Epoch: 77/100, Loss: 0.158176\n",
            "Epoch: 78/100, Loss: 0.154959\n",
            "Epoch: 79/100, Loss: 0.151783\n",
            "Epoch: 80/100, Loss: 0.148655\n",
            "Epoch: 81/100, Loss: 0.145574\n",
            "Epoch: 82/100, Loss: 0.142537\n",
            "Epoch: 83/100, Loss: 0.139548\n",
            "Epoch: 84/100, Loss: 0.136604\n",
            "Epoch: 85/100, Loss: 0.133701\n",
            "Epoch: 86/100, Loss: 0.130843\n",
            "Epoch: 87/100, Loss: 0.128032\n",
            "Epoch: 88/100, Loss: 0.125265\n",
            "Epoch: 89/100, Loss: 0.122548\n",
            "Epoch: 90/100, Loss: 0.119877\n",
            "Epoch: 91/100, Loss: 0.117242\n",
            "Epoch: 92/100, Loss: 0.114646\n",
            "Epoch: 93/100, Loss: 0.112088\n",
            "Epoch: 94/100, Loss: 0.109565\n",
            "Epoch: 95/100, Loss: 0.107093\n",
            "Epoch: 96/100, Loss: 0.104663\n",
            "Epoch: 97/100, Loss: 0.102288\n",
            "Epoch: 98/100, Loss: 0.099956\n",
            "Epoch: 99/100, Loss: 0.097647\n",
            "Epoch: 100/100, Loss: 0.095388\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6413\n",
            "Normalised mutual info score on k-means on latent space: 0.5827412236556392\n",
            "ARI score on k-means on latent space: 0.474211545835533\n",
            "K-means cluster error on latent space: 41283.85546875\n",
            "K-means silhouette score on latent space: 0.19796266 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6785\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6468329343163605 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5308223632103649 \n",
            "\n",
            "ROUND NUMBER  9 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.935103\n",
            "Epoch: 2/100, Loss: 0.736495\n",
            "Epoch: 3/100, Loss: 0.655536\n",
            "Epoch: 4/100, Loss: 0.614681\n",
            "Epoch: 5/100, Loss: 0.588936\n",
            "Epoch: 6/100, Loss: 0.570446\n",
            "Epoch: 7/100, Loss: 0.555546\n",
            "Epoch: 8/100, Loss: 0.542651\n",
            "Epoch: 9/100, Loss: 0.531035\n",
            "Epoch: 10/100, Loss: 0.520288\n",
            "Epoch: 11/100, Loss: 0.510190\n",
            "Epoch: 12/100, Loss: 0.500598\n",
            "Epoch: 13/100, Loss: 0.491424\n",
            "Epoch: 14/100, Loss: 0.482608\n",
            "Epoch: 15/100, Loss: 0.474102\n",
            "Epoch: 16/100, Loss: 0.465862\n",
            "Epoch: 17/100, Loss: 0.457861\n",
            "Epoch: 18/100, Loss: 0.450080\n",
            "Epoch: 19/100, Loss: 0.442495\n",
            "Epoch: 20/100, Loss: 0.435090\n",
            "Epoch: 21/100, Loss: 0.427856\n",
            "Epoch: 22/100, Loss: 0.420777\n",
            "Epoch: 23/100, Loss: 0.413847\n",
            "Epoch: 24/100, Loss: 0.407051\n",
            "Epoch: 25/100, Loss: 0.400387\n",
            "Epoch: 26/100, Loss: 0.393844\n",
            "Epoch: 27/100, Loss: 0.387423\n",
            "Epoch: 28/100, Loss: 0.381109\n",
            "Epoch: 29/100, Loss: 0.374906\n",
            "Epoch: 30/100, Loss: 0.368804\n",
            "Epoch: 31/100, Loss: 0.362799\n",
            "Epoch: 32/100, Loss: 0.356885\n",
            "Epoch: 33/100, Loss: 0.351065\n",
            "Epoch: 34/100, Loss: 0.345333\n",
            "Epoch: 35/100, Loss: 0.339685\n",
            "Epoch: 36/100, Loss: 0.334114\n",
            "Epoch: 37/100, Loss: 0.328626\n",
            "Epoch: 38/100, Loss: 0.323211\n",
            "Epoch: 39/100, Loss: 0.317874\n",
            "Epoch: 40/100, Loss: 0.312602\n",
            "Epoch: 41/100, Loss: 0.307402\n",
            "Epoch: 42/100, Loss: 0.302277\n",
            "Epoch: 43/100, Loss: 0.297214\n",
            "Epoch: 44/100, Loss: 0.292217\n",
            "Epoch: 45/100, Loss: 0.287285\n",
            "Epoch: 46/100, Loss: 0.282415\n",
            "Epoch: 47/100, Loss: 0.277605\n",
            "Epoch: 48/100, Loss: 0.272857\n",
            "Epoch: 49/100, Loss: 0.268167\n",
            "Epoch: 50/100, Loss: 0.263536\n",
            "Epoch: 51/100, Loss: 0.258964\n",
            "Epoch: 52/100, Loss: 0.254444\n",
            "Epoch: 53/100, Loss: 0.249980\n",
            "Epoch: 54/100, Loss: 0.245571\n",
            "Epoch: 55/100, Loss: 0.241214\n",
            "Epoch: 56/100, Loss: 0.236913\n",
            "Epoch: 57/100, Loss: 0.232665\n",
            "Epoch: 58/100, Loss: 0.228470\n",
            "Epoch: 59/100, Loss: 0.224328\n",
            "Epoch: 60/100, Loss: 0.220236\n",
            "Epoch: 61/100, Loss: 0.216197\n",
            "Epoch: 62/100, Loss: 0.212209\n",
            "Epoch: 63/100, Loss: 0.208272\n",
            "Epoch: 64/100, Loss: 0.204384\n",
            "Epoch: 65/100, Loss: 0.200547\n",
            "Epoch: 66/100, Loss: 0.196761\n",
            "Epoch: 67/100, Loss: 0.193022\n",
            "Epoch: 68/100, Loss: 0.189335\n",
            "Epoch: 69/100, Loss: 0.185694\n",
            "Epoch: 70/100, Loss: 0.182102\n",
            "Epoch: 71/100, Loss: 0.178560\n",
            "Epoch: 72/100, Loss: 0.175065\n",
            "Epoch: 73/100, Loss: 0.171616\n",
            "Epoch: 74/100, Loss: 0.168215\n",
            "Epoch: 75/100, Loss: 0.164862\n",
            "Epoch: 76/100, Loss: 0.161558\n",
            "Epoch: 77/100, Loss: 0.158306\n",
            "Epoch: 78/100, Loss: 0.155106\n",
            "Epoch: 79/100, Loss: 0.151968\n",
            "Epoch: 80/100, Loss: 0.148901\n",
            "Epoch: 81/100, Loss: 0.145898\n",
            "Epoch: 82/100, Loss: 0.142908\n",
            "Epoch: 83/100, Loss: 0.139885\n",
            "Epoch: 84/100, Loss: 0.136911\n",
            "Epoch: 85/100, Loss: 0.133971\n",
            "Epoch: 86/100, Loss: 0.131068\n",
            "Epoch: 87/100, Loss: 0.128237\n",
            "Epoch: 88/100, Loss: 0.125456\n",
            "Epoch: 89/100, Loss: 0.122716\n",
            "Epoch: 90/100, Loss: 0.120033\n",
            "Epoch: 91/100, Loss: 0.117423\n",
            "Epoch: 92/100, Loss: 0.114874\n",
            "Epoch: 93/100, Loss: 0.112358\n",
            "Epoch: 94/100, Loss: 0.109863\n",
            "Epoch: 95/100, Loss: 0.107394\n",
            "Epoch: 96/100, Loss: 0.104963\n",
            "Epoch: 97/100, Loss: 0.102587\n",
            "Epoch: 98/100, Loss: 0.100281\n",
            "Epoch: 99/100, Loss: 0.097999\n",
            "Epoch: 100/100, Loss: 0.095728\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6968\n",
            "Normalised mutual info score on k-means on latent space: 0.6291968317138936\n",
            "ARI score on k-means on latent space: 0.5381525866019633\n",
            "K-means cluster error on latent space: 43695.36328125\n",
            "K-means silhouette score on latent space: 0.18713689 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7534\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.7054528641304306 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5994841939321084 \n",
            "\n",
            "ROUND NUMBER  10 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.929782\n",
            "Epoch: 2/100, Loss: 0.729334\n",
            "Epoch: 3/100, Loss: 0.651895\n",
            "Epoch: 4/100, Loss: 0.612789\n",
            "Epoch: 5/100, Loss: 0.587767\n",
            "Epoch: 6/100, Loss: 0.569146\n",
            "Epoch: 7/100, Loss: 0.553915\n",
            "Epoch: 8/100, Loss: 0.540731\n",
            "Epoch: 9/100, Loss: 0.528903\n",
            "Epoch: 10/100, Loss: 0.518034\n",
            "Epoch: 11/100, Loss: 0.507888\n",
            "Epoch: 12/100, Loss: 0.498311\n",
            "Epoch: 13/100, Loss: 0.489181\n",
            "Epoch: 14/100, Loss: 0.480415\n",
            "Epoch: 15/100, Loss: 0.471958\n",
            "Epoch: 16/100, Loss: 0.463773\n",
            "Epoch: 17/100, Loss: 0.455832\n",
            "Epoch: 18/100, Loss: 0.448105\n",
            "Epoch: 19/100, Loss: 0.440575\n",
            "Epoch: 20/100, Loss: 0.433219\n",
            "Epoch: 21/100, Loss: 0.426030\n",
            "Epoch: 22/100, Loss: 0.418993\n",
            "Epoch: 23/100, Loss: 0.412103\n",
            "Epoch: 24/100, Loss: 0.405350\n",
            "Epoch: 25/100, Loss: 0.398726\n",
            "Epoch: 26/100, Loss: 0.392222\n",
            "Epoch: 27/100, Loss: 0.385832\n",
            "Epoch: 28/100, Loss: 0.379552\n",
            "Epoch: 29/100, Loss: 0.373374\n",
            "Epoch: 30/100, Loss: 0.367296\n",
            "Epoch: 31/100, Loss: 0.361312\n",
            "Epoch: 32/100, Loss: 0.355421\n",
            "Epoch: 33/100, Loss: 0.349615\n",
            "Epoch: 34/100, Loss: 0.343897\n",
            "Epoch: 35/100, Loss: 0.338262\n",
            "Epoch: 36/100, Loss: 0.332704\n",
            "Epoch: 37/100, Loss: 0.327229\n",
            "Epoch: 38/100, Loss: 0.321828\n",
            "Epoch: 39/100, Loss: 0.316498\n",
            "Epoch: 40/100, Loss: 0.311239\n",
            "Epoch: 41/100, Loss: 0.306047\n",
            "Epoch: 42/100, Loss: 0.300924\n",
            "Epoch: 43/100, Loss: 0.295869\n",
            "Epoch: 44/100, Loss: 0.290879\n",
            "Epoch: 45/100, Loss: 0.285953\n",
            "Epoch: 46/100, Loss: 0.281086\n",
            "Epoch: 47/100, Loss: 0.276281\n",
            "Epoch: 48/100, Loss: 0.271538\n",
            "Epoch: 49/100, Loss: 0.266854\n",
            "Epoch: 50/100, Loss: 0.262229\n",
            "Epoch: 51/100, Loss: 0.257661\n",
            "Epoch: 52/100, Loss: 0.253149\n",
            "Epoch: 53/100, Loss: 0.248692\n",
            "Epoch: 54/100, Loss: 0.244292\n",
            "Epoch: 55/100, Loss: 0.239944\n",
            "Epoch: 56/100, Loss: 0.235653\n",
            "Epoch: 57/100, Loss: 0.231418\n",
            "Epoch: 58/100, Loss: 0.227236\n",
            "Epoch: 59/100, Loss: 0.223102\n",
            "Epoch: 60/100, Loss: 0.219022\n",
            "Epoch: 61/100, Loss: 0.214991\n",
            "Epoch: 62/100, Loss: 0.211011\n",
            "Epoch: 63/100, Loss: 0.207083\n",
            "Epoch: 64/100, Loss: 0.203202\n",
            "Epoch: 65/100, Loss: 0.199374\n",
            "Epoch: 66/100, Loss: 0.195594\n",
            "Epoch: 67/100, Loss: 0.191865\n",
            "Epoch: 68/100, Loss: 0.188185\n",
            "Epoch: 69/100, Loss: 0.184551\n",
            "Epoch: 70/100, Loss: 0.180968\n",
            "Epoch: 71/100, Loss: 0.177430\n",
            "Epoch: 72/100, Loss: 0.173940\n",
            "Epoch: 73/100, Loss: 0.170498\n",
            "Epoch: 74/100, Loss: 0.167106\n",
            "Epoch: 75/100, Loss: 0.163754\n",
            "Epoch: 76/100, Loss: 0.160454\n",
            "Epoch: 77/100, Loss: 0.157209\n",
            "Epoch: 78/100, Loss: 0.154016\n",
            "Epoch: 79/100, Loss: 0.150867\n",
            "Epoch: 80/100, Loss: 0.147763\n",
            "Epoch: 81/100, Loss: 0.144701\n",
            "Epoch: 82/100, Loss: 0.141694\n",
            "Epoch: 83/100, Loss: 0.138726\n",
            "Epoch: 84/100, Loss: 0.135796\n",
            "Epoch: 85/100, Loss: 0.132916\n",
            "Epoch: 86/100, Loss: 0.130072\n",
            "Epoch: 87/100, Loss: 0.127265\n",
            "Epoch: 88/100, Loss: 0.124501\n",
            "Epoch: 89/100, Loss: 0.121795\n",
            "Epoch: 90/100, Loss: 0.119146\n",
            "Epoch: 91/100, Loss: 0.116537\n",
            "Epoch: 92/100, Loss: 0.113985\n",
            "Epoch: 93/100, Loss: 0.111449\n",
            "Epoch: 94/100, Loss: 0.108934\n",
            "Epoch: 95/100, Loss: 0.106436\n",
            "Epoch: 96/100, Loss: 0.103950\n",
            "Epoch: 97/100, Loss: 0.101511\n",
            "Epoch: 98/100, Loss: 0.099115\n",
            "Epoch: 99/100, Loss: 0.096777\n",
            "Epoch: 100/100, Loss: 0.094492\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6628\n",
            "Normalised mutual info score on k-means on latent space: 0.6024423575927474\n",
            "ARI score on k-means on latent space: 0.5011803611898231\n",
            "K-means cluster error on latent space: 46280.984375\n",
            "K-means silhouette score on latent space: 0.16553897 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6779\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6480451882040611 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5360495244956728 \n",
            "\n",
            "Average k-means accuracy score at latent space: 0.66456 \n",
            "\n",
            "Average k-means NMI score at latent space: 0.5996147893579113 \n",
            "\n",
            "Average k-means ARI score at latent space: 0.5003671028361399 \n",
            "\n",
            "Average agglomerative clustering accuracy score at latent space: 0.6949099999999999 \n",
            "\n",
            "Average agglomerative clustering NMI score at latent space: 0.6734430793668977 \n",
            "\n",
            "Average agglomerative clustering ARI score at latent space: 0.553138280738093 \n",
            "\n",
            "Average k-means silhouette score on latent space: 0.18579467982053757 \n",
            "\n",
            "Average k-means cluster error on latent space: 43919.19296875 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_for_k_200 = run_experiment(200, dataloader, 784, datapoints = images, datapoints_reshaped = images_reshaped, labels = labels, epochs=100, hl1_neurons=32, hl2_neurons=64, hl3_neurons=128, latent_dimension=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5hJWy_07yxe",
        "outputId": "39c58286-0f1e-4d12-d271-5c3759df1ca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment results for k-means with k = 200 clusters:\n",
            "\n",
            "Running k-means algorithm in order to get our pseudolabels: \n",
            "\n",
            "K_means greedy accuracy score (initial space): 0.8925\n",
            "Normalised mutual info score (initial space): 0.7909753888653926\n",
            "ARI (initial space): 0.7817729314941033 \n",
            "\n",
            "Using the autoencoder model on our data: \n",
            "\n",
            "ROUND NUMBER  1 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.900119\n",
            "Epoch: 2/100, Loss: 0.712290\n",
            "Epoch: 3/100, Loss: 0.642178\n",
            "Epoch: 4/100, Loss: 0.607354\n",
            "Epoch: 5/100, Loss: 0.584438\n",
            "Epoch: 6/100, Loss: 0.566980\n",
            "Epoch: 7/100, Loss: 0.552507\n",
            "Epoch: 8/100, Loss: 0.539860\n",
            "Epoch: 9/100, Loss: 0.528406\n",
            "Epoch: 10/100, Loss: 0.517789\n",
            "Epoch: 11/100, Loss: 0.507798\n",
            "Epoch: 12/100, Loss: 0.498305\n",
            "Epoch: 13/100, Loss: 0.489212\n",
            "Epoch: 14/100, Loss: 0.480461\n",
            "Epoch: 15/100, Loss: 0.472014\n",
            "Epoch: 16/100, Loss: 0.463830\n",
            "Epoch: 17/100, Loss: 0.455879\n",
            "Epoch: 18/100, Loss: 0.448141\n",
            "Epoch: 19/100, Loss: 0.440594\n",
            "Epoch: 20/100, Loss: 0.433232\n",
            "Epoch: 21/100, Loss: 0.426033\n",
            "Epoch: 22/100, Loss: 0.418991\n",
            "Epoch: 23/100, Loss: 0.412092\n",
            "Epoch: 24/100, Loss: 0.405330\n",
            "Epoch: 25/100, Loss: 0.398696\n",
            "Epoch: 26/100, Loss: 0.392189\n",
            "Epoch: 27/100, Loss: 0.385793\n",
            "Epoch: 28/100, Loss: 0.379508\n",
            "Epoch: 29/100, Loss: 0.373331\n",
            "Epoch: 30/100, Loss: 0.367250\n",
            "Epoch: 31/100, Loss: 0.361271\n",
            "Epoch: 32/100, Loss: 0.355387\n",
            "Epoch: 33/100, Loss: 0.349591\n",
            "Epoch: 34/100, Loss: 0.343881\n",
            "Epoch: 35/100, Loss: 0.338251\n",
            "Epoch: 36/100, Loss: 0.332700\n",
            "Epoch: 37/100, Loss: 0.327230\n",
            "Epoch: 38/100, Loss: 0.321840\n",
            "Epoch: 39/100, Loss: 0.316521\n",
            "Epoch: 40/100, Loss: 0.311275\n",
            "Epoch: 41/100, Loss: 0.306096\n",
            "Epoch: 42/100, Loss: 0.300990\n",
            "Epoch: 43/100, Loss: 0.295948\n",
            "Epoch: 44/100, Loss: 0.290967\n",
            "Epoch: 45/100, Loss: 0.286056\n",
            "Epoch: 46/100, Loss: 0.281203\n",
            "Epoch: 47/100, Loss: 0.276413\n",
            "Epoch: 48/100, Loss: 0.271686\n",
            "Epoch: 49/100, Loss: 0.267020\n",
            "Epoch: 50/100, Loss: 0.262410\n",
            "Epoch: 51/100, Loss: 0.257850\n",
            "Epoch: 52/100, Loss: 0.253352\n",
            "Epoch: 53/100, Loss: 0.248910\n",
            "Epoch: 54/100, Loss: 0.244522\n",
            "Epoch: 55/100, Loss: 0.240193\n",
            "Epoch: 56/100, Loss: 0.235912\n",
            "Epoch: 57/100, Loss: 0.231689\n",
            "Epoch: 58/100, Loss: 0.227517\n",
            "Epoch: 59/100, Loss: 0.223398\n",
            "Epoch: 60/100, Loss: 0.219331\n",
            "Epoch: 61/100, Loss: 0.215315\n",
            "Epoch: 62/100, Loss: 0.211350\n",
            "Epoch: 63/100, Loss: 0.207434\n",
            "Epoch: 64/100, Loss: 0.203567\n",
            "Epoch: 65/100, Loss: 0.199752\n",
            "Epoch: 66/100, Loss: 0.195982\n",
            "Epoch: 67/100, Loss: 0.192263\n",
            "Epoch: 68/100, Loss: 0.188592\n",
            "Epoch: 69/100, Loss: 0.184972\n",
            "Epoch: 70/100, Loss: 0.181397\n",
            "Epoch: 71/100, Loss: 0.177868\n",
            "Epoch: 72/100, Loss: 0.174382\n",
            "Epoch: 73/100, Loss: 0.170948\n",
            "Epoch: 74/100, Loss: 0.167558\n",
            "Epoch: 75/100, Loss: 0.164218\n",
            "Epoch: 76/100, Loss: 0.160920\n",
            "Epoch: 77/100, Loss: 0.157674\n",
            "Epoch: 78/100, Loss: 0.154469\n",
            "Epoch: 79/100, Loss: 0.151310\n",
            "Epoch: 80/100, Loss: 0.148204\n",
            "Epoch: 81/100, Loss: 0.145138\n",
            "Epoch: 82/100, Loss: 0.142118\n",
            "Epoch: 83/100, Loss: 0.139144\n",
            "Epoch: 84/100, Loss: 0.136221\n",
            "Epoch: 85/100, Loss: 0.133356\n",
            "Epoch: 86/100, Loss: 0.130556\n",
            "Epoch: 87/100, Loss: 0.127865\n",
            "Epoch: 88/100, Loss: 0.125267\n",
            "Epoch: 89/100, Loss: 0.122637\n",
            "Epoch: 90/100, Loss: 0.119849\n",
            "Epoch: 91/100, Loss: 0.117036\n",
            "Epoch: 92/100, Loss: 0.114302\n",
            "Epoch: 93/100, Loss: 0.111669\n",
            "Epoch: 94/100, Loss: 0.109117\n",
            "Epoch: 95/100, Loss: 0.106621\n",
            "Epoch: 96/100, Loss: 0.104175\n",
            "Epoch: 97/100, Loss: 0.101776\n",
            "Epoch: 98/100, Loss: 0.099423\n",
            "Epoch: 99/100, Loss: 0.097124\n",
            "Epoch: 100/100, Loss: 0.094875\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6522\n",
            "Normalised mutual info score on k-means on latent space: 0.5879782692309701\n",
            "ARI score on k-means on latent space: 0.49099620307801156\n",
            "K-means cluster error on latent space: 43704.75390625\n",
            "K-means silhouette score on latent space: 0.18935594 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6778\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6801438693828332 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5462106793837834 \n",
            "\n",
            "ROUND NUMBER  2 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.912009\n",
            "Epoch: 2/100, Loss: 0.712366\n",
            "Epoch: 3/100, Loss: 0.643190\n",
            "Epoch: 4/100, Loss: 0.609508\n",
            "Epoch: 5/100, Loss: 0.587039\n",
            "Epoch: 6/100, Loss: 0.569845\n",
            "Epoch: 7/100, Loss: 0.555446\n",
            "Epoch: 8/100, Loss: 0.542736\n",
            "Epoch: 9/100, Loss: 0.531170\n",
            "Epoch: 10/100, Loss: 0.520420\n",
            "Epoch: 11/100, Loss: 0.510310\n",
            "Epoch: 12/100, Loss: 0.500703\n",
            "Epoch: 13/100, Loss: 0.491532\n",
            "Epoch: 14/100, Loss: 0.482718\n",
            "Epoch: 15/100, Loss: 0.474206\n",
            "Epoch: 16/100, Loss: 0.465965\n",
            "Epoch: 17/100, Loss: 0.457962\n",
            "Epoch: 18/100, Loss: 0.450177\n",
            "Epoch: 19/100, Loss: 0.442590\n",
            "Epoch: 20/100, Loss: 0.435180\n",
            "Epoch: 21/100, Loss: 0.427930\n",
            "Epoch: 22/100, Loss: 0.420838\n",
            "Epoch: 23/100, Loss: 0.413895\n",
            "Epoch: 24/100, Loss: 0.407089\n",
            "Epoch: 25/100, Loss: 0.400416\n",
            "Epoch: 26/100, Loss: 0.393863\n",
            "Epoch: 27/100, Loss: 0.387429\n",
            "Epoch: 28/100, Loss: 0.381106\n",
            "Epoch: 29/100, Loss: 0.374895\n",
            "Epoch: 30/100, Loss: 0.368789\n",
            "Epoch: 31/100, Loss: 0.362780\n",
            "Epoch: 32/100, Loss: 0.356864\n",
            "Epoch: 33/100, Loss: 0.351038\n",
            "Epoch: 34/100, Loss: 0.345300\n",
            "Epoch: 35/100, Loss: 0.339647\n",
            "Epoch: 36/100, Loss: 0.334074\n",
            "Epoch: 37/100, Loss: 0.328581\n",
            "Epoch: 38/100, Loss: 0.323160\n",
            "Epoch: 39/100, Loss: 0.317818\n",
            "Epoch: 40/100, Loss: 0.312546\n",
            "Epoch: 41/100, Loss: 0.307347\n",
            "Epoch: 42/100, Loss: 0.302214\n",
            "Epoch: 43/100, Loss: 0.297148\n",
            "Epoch: 44/100, Loss: 0.292146\n",
            "Epoch: 45/100, Loss: 0.287207\n",
            "Epoch: 46/100, Loss: 0.282334\n",
            "Epoch: 47/100, Loss: 0.277523\n",
            "Epoch: 48/100, Loss: 0.272771\n",
            "Epoch: 49/100, Loss: 0.268075\n",
            "Epoch: 50/100, Loss: 0.263442\n",
            "Epoch: 51/100, Loss: 0.258868\n",
            "Epoch: 52/100, Loss: 0.254351\n",
            "Epoch: 53/100, Loss: 0.249889\n",
            "Epoch: 54/100, Loss: 0.245484\n",
            "Epoch: 55/100, Loss: 0.241134\n",
            "Epoch: 56/100, Loss: 0.236835\n",
            "Epoch: 57/100, Loss: 0.232590\n",
            "Epoch: 58/100, Loss: 0.228399\n",
            "Epoch: 59/100, Loss: 0.224262\n",
            "Epoch: 60/100, Loss: 0.220178\n",
            "Epoch: 61/100, Loss: 0.216144\n",
            "Epoch: 62/100, Loss: 0.212162\n",
            "Epoch: 63/100, Loss: 0.208232\n",
            "Epoch: 64/100, Loss: 0.204351\n",
            "Epoch: 65/100, Loss: 0.200516\n",
            "Epoch: 66/100, Loss: 0.196734\n",
            "Epoch: 67/100, Loss: 0.193003\n",
            "Epoch: 68/100, Loss: 0.189319\n",
            "Epoch: 69/100, Loss: 0.185685\n",
            "Epoch: 70/100, Loss: 0.182098\n",
            "Epoch: 71/100, Loss: 0.178560\n",
            "Epoch: 72/100, Loss: 0.175070\n",
            "Epoch: 73/100, Loss: 0.171629\n",
            "Epoch: 74/100, Loss: 0.168236\n",
            "Epoch: 75/100, Loss: 0.164887\n",
            "Epoch: 76/100, Loss: 0.161590\n",
            "Epoch: 77/100, Loss: 0.158345\n",
            "Epoch: 78/100, Loss: 0.155150\n",
            "Epoch: 79/100, Loss: 0.152011\n",
            "Epoch: 80/100, Loss: 0.148926\n",
            "Epoch: 81/100, Loss: 0.145889\n",
            "Epoch: 82/100, Loss: 0.142888\n",
            "Epoch: 83/100, Loss: 0.139946\n",
            "Epoch: 84/100, Loss: 0.137030\n",
            "Epoch: 85/100, Loss: 0.134080\n",
            "Epoch: 86/100, Loss: 0.131174\n",
            "Epoch: 87/100, Loss: 0.128324\n",
            "Epoch: 88/100, Loss: 0.125548\n",
            "Epoch: 89/100, Loss: 0.122834\n",
            "Epoch: 90/100, Loss: 0.120191\n",
            "Epoch: 91/100, Loss: 0.117598\n",
            "Epoch: 92/100, Loss: 0.115041\n",
            "Epoch: 93/100, Loss: 0.112550\n",
            "Epoch: 94/100, Loss: 0.110112\n",
            "Epoch: 95/100, Loss: 0.107732\n",
            "Epoch: 96/100, Loss: 0.105340\n",
            "Epoch: 97/100, Loss: 0.102917\n",
            "Epoch: 98/100, Loss: 0.100483\n",
            "Epoch: 99/100, Loss: 0.098078\n",
            "Epoch: 100/100, Loss: 0.095698\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6556\n",
            "Normalised mutual info score on k-means on latent space: 0.6038372655522792\n",
            "ARI score on k-means on latent space: 0.5028404631552648\n",
            "K-means cluster error on latent space: 43882.62890625\n",
            "K-means silhouette score on latent space: 0.17992917 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7135\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6827459912296743 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5768460029383841 \n",
            "\n",
            "ROUND NUMBER  3 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.957408\n",
            "Epoch: 2/100, Loss: 0.765826\n",
            "Epoch: 3/100, Loss: 0.670266\n",
            "Epoch: 4/100, Loss: 0.621792\n",
            "Epoch: 5/100, Loss: 0.593374\n",
            "Epoch: 6/100, Loss: 0.573725\n",
            "Epoch: 7/100, Loss: 0.558181\n",
            "Epoch: 8/100, Loss: 0.544805\n",
            "Epoch: 9/100, Loss: 0.532798\n",
            "Epoch: 10/100, Loss: 0.521738\n",
            "Epoch: 11/100, Loss: 0.511409\n",
            "Epoch: 12/100, Loss: 0.501645\n",
            "Epoch: 13/100, Loss: 0.492346\n",
            "Epoch: 14/100, Loss: 0.483424\n",
            "Epoch: 15/100, Loss: 0.474840\n",
            "Epoch: 16/100, Loss: 0.466536\n",
            "Epoch: 17/100, Loss: 0.458484\n",
            "Epoch: 18/100, Loss: 0.450667\n",
            "Epoch: 19/100, Loss: 0.443047\n",
            "Epoch: 20/100, Loss: 0.435615\n",
            "Epoch: 21/100, Loss: 0.428358\n",
            "Epoch: 22/100, Loss: 0.421268\n",
            "Epoch: 23/100, Loss: 0.414322\n",
            "Epoch: 24/100, Loss: 0.407522\n",
            "Epoch: 25/100, Loss: 0.400855\n",
            "Epoch: 26/100, Loss: 0.394306\n",
            "Epoch: 27/100, Loss: 0.387875\n",
            "Epoch: 28/100, Loss: 0.381555\n",
            "Epoch: 29/100, Loss: 0.375342\n",
            "Epoch: 30/100, Loss: 0.369230\n",
            "Epoch: 31/100, Loss: 0.363219\n",
            "Epoch: 32/100, Loss: 0.357303\n",
            "Epoch: 33/100, Loss: 0.351476\n",
            "Epoch: 34/100, Loss: 0.345740\n",
            "Epoch: 35/100, Loss: 0.340089\n",
            "Epoch: 36/100, Loss: 0.334518\n",
            "Epoch: 37/100, Loss: 0.329025\n",
            "Epoch: 38/100, Loss: 0.323605\n",
            "Epoch: 39/100, Loss: 0.318261\n",
            "Epoch: 40/100, Loss: 0.312990\n",
            "Epoch: 41/100, Loss: 0.307787\n",
            "Epoch: 42/100, Loss: 0.302658\n",
            "Epoch: 43/100, Loss: 0.297591\n",
            "Epoch: 44/100, Loss: 0.292589\n",
            "Epoch: 45/100, Loss: 0.287656\n",
            "Epoch: 46/100, Loss: 0.282782\n",
            "Epoch: 47/100, Loss: 0.277970\n",
            "Epoch: 48/100, Loss: 0.273222\n",
            "Epoch: 49/100, Loss: 0.268532\n",
            "Epoch: 50/100, Loss: 0.263903\n",
            "Epoch: 51/100, Loss: 0.259328\n",
            "Epoch: 52/100, Loss: 0.254812\n",
            "Epoch: 53/100, Loss: 0.250350\n",
            "Epoch: 54/100, Loss: 0.245939\n",
            "Epoch: 55/100, Loss: 0.241586\n",
            "Epoch: 56/100, Loss: 0.237287\n",
            "Epoch: 57/100, Loss: 0.233045\n",
            "Epoch: 58/100, Loss: 0.228851\n",
            "Epoch: 59/100, Loss: 0.224711\n",
            "Epoch: 60/100, Loss: 0.220624\n",
            "Epoch: 61/100, Loss: 0.216585\n",
            "Epoch: 62/100, Loss: 0.212594\n",
            "Epoch: 63/100, Loss: 0.208655\n",
            "Epoch: 64/100, Loss: 0.204768\n",
            "Epoch: 65/100, Loss: 0.200932\n",
            "Epoch: 66/100, Loss: 0.197143\n",
            "Epoch: 67/100, Loss: 0.193403\n",
            "Epoch: 68/100, Loss: 0.189715\n",
            "Epoch: 69/100, Loss: 0.186075\n",
            "Epoch: 70/100, Loss: 0.182481\n",
            "Epoch: 71/100, Loss: 0.178937\n",
            "Epoch: 72/100, Loss: 0.175440\n",
            "Epoch: 73/100, Loss: 0.171990\n",
            "Epoch: 74/100, Loss: 0.168586\n",
            "Epoch: 75/100, Loss: 0.165228\n",
            "Epoch: 76/100, Loss: 0.161915\n",
            "Epoch: 77/100, Loss: 0.158648\n",
            "Epoch: 78/100, Loss: 0.155426\n",
            "Epoch: 79/100, Loss: 0.152251\n",
            "Epoch: 80/100, Loss: 0.149123\n",
            "Epoch: 81/100, Loss: 0.146044\n",
            "Epoch: 82/100, Loss: 0.143015\n",
            "Epoch: 83/100, Loss: 0.140033\n",
            "Epoch: 84/100, Loss: 0.137115\n",
            "Epoch: 85/100, Loss: 0.134250\n",
            "Epoch: 86/100, Loss: 0.131447\n",
            "Epoch: 87/100, Loss: 0.128676\n",
            "Epoch: 88/100, Loss: 0.125917\n",
            "Epoch: 89/100, Loss: 0.123175\n",
            "Epoch: 90/100, Loss: 0.120475\n",
            "Epoch: 91/100, Loss: 0.117825\n",
            "Epoch: 92/100, Loss: 0.115184\n",
            "Epoch: 93/100, Loss: 0.112559\n",
            "Epoch: 94/100, Loss: 0.109980\n",
            "Epoch: 95/100, Loss: 0.107463\n",
            "Epoch: 96/100, Loss: 0.104994\n",
            "Epoch: 97/100, Loss: 0.102579\n",
            "Epoch: 98/100, Loss: 0.100210\n",
            "Epoch: 99/100, Loss: 0.097891\n",
            "Epoch: 100/100, Loss: 0.095616\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6934\n",
            "Normalised mutual info score on k-means on latent space: 0.6143605628727813\n",
            "ARI score on k-means on latent space: 0.5304634653366807\n",
            "K-means cluster error on latent space: 44302.35546875\n",
            "K-means silhouette score on latent space: 0.17900248 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6582\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6537638185515631 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5295417033232966 \n",
            "\n",
            "ROUND NUMBER  4 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.910144\n",
            "Epoch: 2/100, Loss: 0.720261\n",
            "Epoch: 3/100, Loss: 0.648339\n",
            "Epoch: 4/100, Loss: 0.611562\n",
            "Epoch: 5/100, Loss: 0.587690\n",
            "Epoch: 6/100, Loss: 0.569941\n",
            "Epoch: 7/100, Loss: 0.555404\n",
            "Epoch: 8/100, Loss: 0.542765\n",
            "Epoch: 9/100, Loss: 0.531334\n",
            "Epoch: 10/100, Loss: 0.520749\n",
            "Epoch: 11/100, Loss: 0.510779\n",
            "Epoch: 12/100, Loss: 0.501282\n",
            "Epoch: 13/100, Loss: 0.492180\n",
            "Epoch: 14/100, Loss: 0.483415\n",
            "Epoch: 15/100, Loss: 0.474934\n",
            "Epoch: 16/100, Loss: 0.466707\n",
            "Epoch: 17/100, Loss: 0.458707\n",
            "Epoch: 18/100, Loss: 0.450917\n",
            "Epoch: 19/100, Loss: 0.443318\n",
            "Epoch: 20/100, Loss: 0.435892\n",
            "Epoch: 21/100, Loss: 0.428624\n",
            "Epoch: 22/100, Loss: 0.421516\n",
            "Epoch: 23/100, Loss: 0.414554\n",
            "Epoch: 24/100, Loss: 0.407733\n",
            "Epoch: 25/100, Loss: 0.401043\n",
            "Epoch: 26/100, Loss: 0.394477\n",
            "Epoch: 27/100, Loss: 0.388034\n",
            "Epoch: 28/100, Loss: 0.381699\n",
            "Epoch: 29/100, Loss: 0.375473\n",
            "Epoch: 30/100, Loss: 0.369354\n",
            "Epoch: 31/100, Loss: 0.363330\n",
            "Epoch: 32/100, Loss: 0.357404\n",
            "Epoch: 33/100, Loss: 0.351570\n",
            "Epoch: 34/100, Loss: 0.345826\n",
            "Epoch: 35/100, Loss: 0.340166\n",
            "Epoch: 36/100, Loss: 0.334594\n",
            "Epoch: 37/100, Loss: 0.329096\n",
            "Epoch: 38/100, Loss: 0.323680\n",
            "Epoch: 39/100, Loss: 0.318338\n",
            "Epoch: 40/100, Loss: 0.313066\n",
            "Epoch: 41/100, Loss: 0.307865\n",
            "Epoch: 42/100, Loss: 0.302737\n",
            "Epoch: 43/100, Loss: 0.297675\n",
            "Epoch: 44/100, Loss: 0.292678\n",
            "Epoch: 45/100, Loss: 0.287749\n",
            "Epoch: 46/100, Loss: 0.282881\n",
            "Epoch: 47/100, Loss: 0.278078\n",
            "Epoch: 48/100, Loss: 0.273336\n",
            "Epoch: 49/100, Loss: 0.268650\n",
            "Epoch: 50/100, Loss: 0.264022\n",
            "Epoch: 51/100, Loss: 0.259451\n",
            "Epoch: 52/100, Loss: 0.254939\n",
            "Epoch: 53/100, Loss: 0.250480\n",
            "Epoch: 54/100, Loss: 0.246076\n",
            "Epoch: 55/100, Loss: 0.241730\n",
            "Epoch: 56/100, Loss: 0.237436\n",
            "Epoch: 57/100, Loss: 0.233191\n",
            "Epoch: 58/100, Loss: 0.229003\n",
            "Epoch: 59/100, Loss: 0.224866\n",
            "Epoch: 60/100, Loss: 0.220779\n",
            "Epoch: 61/100, Loss: 0.216744\n",
            "Epoch: 62/100, Loss: 0.212760\n",
            "Epoch: 63/100, Loss: 0.208827\n",
            "Epoch: 64/100, Loss: 0.204944\n",
            "Epoch: 65/100, Loss: 0.201110\n",
            "Epoch: 66/100, Loss: 0.197330\n",
            "Epoch: 67/100, Loss: 0.193593\n",
            "Epoch: 68/100, Loss: 0.189905\n",
            "Epoch: 69/100, Loss: 0.186267\n",
            "Epoch: 70/100, Loss: 0.182676\n",
            "Epoch: 71/100, Loss: 0.179132\n",
            "Epoch: 72/100, Loss: 0.175639\n",
            "Epoch: 73/100, Loss: 0.172191\n",
            "Epoch: 74/100, Loss: 0.168793\n",
            "Epoch: 75/100, Loss: 0.165443\n",
            "Epoch: 76/100, Loss: 0.162142\n",
            "Epoch: 77/100, Loss: 0.158885\n",
            "Epoch: 78/100, Loss: 0.155678\n",
            "Epoch: 79/100, Loss: 0.152519\n",
            "Epoch: 80/100, Loss: 0.149407\n",
            "Epoch: 81/100, Loss: 0.146340\n",
            "Epoch: 82/100, Loss: 0.143320\n",
            "Epoch: 83/100, Loss: 0.140348\n",
            "Epoch: 84/100, Loss: 0.137403\n",
            "Epoch: 85/100, Loss: 0.134502\n",
            "Epoch: 86/100, Loss: 0.131643\n",
            "Epoch: 87/100, Loss: 0.128821\n",
            "Epoch: 88/100, Loss: 0.126037\n",
            "Epoch: 89/100, Loss: 0.123294\n",
            "Epoch: 90/100, Loss: 0.120603\n",
            "Epoch: 91/100, Loss: 0.117975\n",
            "Epoch: 92/100, Loss: 0.115398\n",
            "Epoch: 93/100, Loss: 0.112870\n",
            "Epoch: 94/100, Loss: 0.110381\n",
            "Epoch: 95/100, Loss: 0.107937\n",
            "Epoch: 96/100, Loss: 0.105506\n",
            "Epoch: 97/100, Loss: 0.103118\n",
            "Epoch: 98/100, Loss: 0.100767\n",
            "Epoch: 99/100, Loss: 0.098464\n",
            "Epoch: 100/100, Loss: 0.096197\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6915\n",
            "Normalised mutual info score on k-means on latent space: 0.6059979374441491\n",
            "ARI score on k-means on latent space: 0.5175641795910498\n",
            "K-means cluster error on latent space: 42992.77734375\n",
            "K-means silhouette score on latent space: 0.19662815 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7108\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6870825277059307 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5875665542584609 \n",
            "\n",
            "ROUND NUMBER  5 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.942250\n",
            "Epoch: 2/100, Loss: 0.731748\n",
            "Epoch: 3/100, Loss: 0.651506\n",
            "Epoch: 4/100, Loss: 0.613949\n",
            "Epoch: 5/100, Loss: 0.590075\n",
            "Epoch: 6/100, Loss: 0.572052\n",
            "Epoch: 7/100, Loss: 0.557222\n",
            "Epoch: 8/100, Loss: 0.544237\n",
            "Epoch: 9/100, Loss: 0.532506\n",
            "Epoch: 10/100, Loss: 0.521662\n",
            "Epoch: 11/100, Loss: 0.511483\n",
            "Epoch: 12/100, Loss: 0.501819\n",
            "Epoch: 13/100, Loss: 0.492576\n",
            "Epoch: 14/100, Loss: 0.483694\n",
            "Epoch: 15/100, Loss: 0.475118\n",
            "Epoch: 16/100, Loss: 0.466811\n",
            "Epoch: 17/100, Loss: 0.458755\n",
            "Epoch: 18/100, Loss: 0.450905\n",
            "Epoch: 19/100, Loss: 0.443256\n",
            "Epoch: 20/100, Loss: 0.435791\n",
            "Epoch: 21/100, Loss: 0.428499\n",
            "Epoch: 22/100, Loss: 0.421364\n",
            "Epoch: 23/100, Loss: 0.414378\n",
            "Epoch: 24/100, Loss: 0.407530\n",
            "Epoch: 25/100, Loss: 0.400816\n",
            "Epoch: 26/100, Loss: 0.394223\n",
            "Epoch: 27/100, Loss: 0.387756\n",
            "Epoch: 28/100, Loss: 0.381397\n",
            "Epoch: 29/100, Loss: 0.375149\n",
            "Epoch: 30/100, Loss: 0.369008\n",
            "Epoch: 31/100, Loss: 0.362971\n",
            "Epoch: 32/100, Loss: 0.357024\n",
            "Epoch: 33/100, Loss: 0.351175\n",
            "Epoch: 34/100, Loss: 0.345407\n",
            "Epoch: 35/100, Loss: 0.339727\n",
            "Epoch: 36/100, Loss: 0.334130\n",
            "Epoch: 37/100, Loss: 0.328613\n",
            "Epoch: 38/100, Loss: 0.323172\n",
            "Epoch: 39/100, Loss: 0.317807\n",
            "Epoch: 40/100, Loss: 0.312517\n",
            "Epoch: 41/100, Loss: 0.307297\n",
            "Epoch: 42/100, Loss: 0.302148\n",
            "Epoch: 43/100, Loss: 0.297066\n",
            "Epoch: 44/100, Loss: 0.292050\n",
            "Epoch: 45/100, Loss: 0.287099\n",
            "Epoch: 46/100, Loss: 0.282212\n",
            "Epoch: 47/100, Loss: 0.277385\n",
            "Epoch: 48/100, Loss: 0.272624\n",
            "Epoch: 49/100, Loss: 0.267919\n",
            "Epoch: 50/100, Loss: 0.263274\n",
            "Epoch: 51/100, Loss: 0.258690\n",
            "Epoch: 52/100, Loss: 0.254160\n",
            "Epoch: 53/100, Loss: 0.249690\n",
            "Epoch: 54/100, Loss: 0.245276\n",
            "Epoch: 55/100, Loss: 0.240912\n",
            "Epoch: 56/100, Loss: 0.236604\n",
            "Epoch: 57/100, Loss: 0.232350\n",
            "Epoch: 58/100, Loss: 0.228151\n",
            "Epoch: 59/100, Loss: 0.224004\n",
            "Epoch: 60/100, Loss: 0.219906\n",
            "Epoch: 61/100, Loss: 0.215861\n",
            "Epoch: 62/100, Loss: 0.211867\n",
            "Epoch: 63/100, Loss: 0.207920\n",
            "Epoch: 64/100, Loss: 0.204029\n",
            "Epoch: 65/100, Loss: 0.200189\n",
            "Epoch: 66/100, Loss: 0.196396\n",
            "Epoch: 67/100, Loss: 0.192653\n",
            "Epoch: 68/100, Loss: 0.188958\n",
            "Epoch: 69/100, Loss: 0.185317\n",
            "Epoch: 70/100, Loss: 0.181723\n",
            "Epoch: 71/100, Loss: 0.178178\n",
            "Epoch: 72/100, Loss: 0.174678\n",
            "Epoch: 73/100, Loss: 0.171226\n",
            "Epoch: 74/100, Loss: 0.167825\n",
            "Epoch: 75/100, Loss: 0.164466\n",
            "Epoch: 76/100, Loss: 0.161159\n",
            "Epoch: 77/100, Loss: 0.157892\n",
            "Epoch: 78/100, Loss: 0.154677\n",
            "Epoch: 79/100, Loss: 0.151510\n",
            "Epoch: 80/100, Loss: 0.148398\n",
            "Epoch: 81/100, Loss: 0.145340\n",
            "Epoch: 82/100, Loss: 0.142342\n",
            "Epoch: 83/100, Loss: 0.139413\n",
            "Epoch: 84/100, Loss: 0.136536\n",
            "Epoch: 85/100, Loss: 0.133709\n",
            "Epoch: 86/100, Loss: 0.130896\n",
            "Epoch: 87/100, Loss: 0.128017\n",
            "Epoch: 88/100, Loss: 0.125146\n",
            "Epoch: 89/100, Loss: 0.122367\n",
            "Epoch: 90/100, Loss: 0.119654\n",
            "Epoch: 91/100, Loss: 0.117000\n",
            "Epoch: 92/100, Loss: 0.114400\n",
            "Epoch: 93/100, Loss: 0.111844\n",
            "Epoch: 94/100, Loss: 0.109328\n",
            "Epoch: 95/100, Loss: 0.106868\n",
            "Epoch: 96/100, Loss: 0.104460\n",
            "Epoch: 97/100, Loss: 0.102114\n",
            "Epoch: 98/100, Loss: 0.099797\n",
            "Epoch: 99/100, Loss: 0.097516\n",
            "Epoch: 100/100, Loss: 0.095243\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6544\n",
            "Normalised mutual info score on k-means on latent space: 0.6016910990431432\n",
            "ARI score on k-means on latent space: 0.4917875737275288\n",
            "K-means cluster error on latent space: 42319.88671875\n",
            "K-means silhouette score on latent space: 0.18804416 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7124\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6989211396468035 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.57660670379583 \n",
            "\n",
            "ROUND NUMBER  6 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.917670\n",
            "Epoch: 2/100, Loss: 0.714441\n",
            "Epoch: 3/100, Loss: 0.644524\n",
            "Epoch: 4/100, Loss: 0.610693\n",
            "Epoch: 5/100, Loss: 0.588301\n",
            "Epoch: 6/100, Loss: 0.570796\n",
            "Epoch: 7/100, Loss: 0.555967\n",
            "Epoch: 8/100, Loss: 0.542938\n",
            "Epoch: 9/100, Loss: 0.531151\n",
            "Epoch: 10/100, Loss: 0.520253\n",
            "Epoch: 11/100, Loss: 0.510035\n",
            "Epoch: 12/100, Loss: 0.500359\n",
            "Epoch: 13/100, Loss: 0.491132\n",
            "Epoch: 14/100, Loss: 0.482272\n",
            "Epoch: 15/100, Loss: 0.473725\n",
            "Epoch: 16/100, Loss: 0.465456\n",
            "Epoch: 17/100, Loss: 0.457434\n",
            "Epoch: 18/100, Loss: 0.449628\n",
            "Epoch: 19/100, Loss: 0.442027\n",
            "Epoch: 20/100, Loss: 0.434607\n",
            "Epoch: 21/100, Loss: 0.427365\n",
            "Epoch: 22/100, Loss: 0.420280\n",
            "Epoch: 23/100, Loss: 0.413347\n",
            "Epoch: 24/100, Loss: 0.406556\n",
            "Epoch: 25/100, Loss: 0.399898\n",
            "Epoch: 26/100, Loss: 0.393361\n",
            "Epoch: 27/100, Loss: 0.386944\n",
            "Epoch: 28/100, Loss: 0.380632\n",
            "Epoch: 29/100, Loss: 0.374427\n",
            "Epoch: 30/100, Loss: 0.368327\n",
            "Epoch: 31/100, Loss: 0.362319\n",
            "Epoch: 32/100, Loss: 0.356410\n",
            "Epoch: 33/100, Loss: 0.350588\n",
            "Epoch: 34/100, Loss: 0.344853\n",
            "Epoch: 35/100, Loss: 0.339200\n",
            "Epoch: 36/100, Loss: 0.333628\n",
            "Epoch: 37/100, Loss: 0.328136\n",
            "Epoch: 38/100, Loss: 0.322719\n",
            "Epoch: 39/100, Loss: 0.317379\n",
            "Epoch: 40/100, Loss: 0.312109\n",
            "Epoch: 41/100, Loss: 0.306905\n",
            "Epoch: 42/100, Loss: 0.301773\n",
            "Epoch: 43/100, Loss: 0.296705\n",
            "Epoch: 44/100, Loss: 0.291699\n",
            "Epoch: 45/100, Loss: 0.286756\n",
            "Epoch: 46/100, Loss: 0.281878\n",
            "Epoch: 47/100, Loss: 0.277063\n",
            "Epoch: 48/100, Loss: 0.272309\n",
            "Epoch: 49/100, Loss: 0.267617\n",
            "Epoch: 50/100, Loss: 0.262981\n",
            "Epoch: 51/100, Loss: 0.258406\n",
            "Epoch: 52/100, Loss: 0.253885\n",
            "Epoch: 53/100, Loss: 0.249420\n",
            "Epoch: 54/100, Loss: 0.245015\n",
            "Epoch: 55/100, Loss: 0.240663\n",
            "Epoch: 56/100, Loss: 0.236366\n",
            "Epoch: 57/100, Loss: 0.232124\n",
            "Epoch: 58/100, Loss: 0.227932\n",
            "Epoch: 59/100, Loss: 0.223798\n",
            "Epoch: 60/100, Loss: 0.219714\n",
            "Epoch: 61/100, Loss: 0.215678\n",
            "Epoch: 62/100, Loss: 0.211693\n",
            "Epoch: 63/100, Loss: 0.207760\n",
            "Epoch: 64/100, Loss: 0.203881\n",
            "Epoch: 65/100, Loss: 0.200050\n",
            "Epoch: 66/100, Loss: 0.196268\n",
            "Epoch: 67/100, Loss: 0.192536\n",
            "Epoch: 68/100, Loss: 0.188849\n",
            "Epoch: 69/100, Loss: 0.185217\n",
            "Epoch: 70/100, Loss: 0.181631\n",
            "Epoch: 71/100, Loss: 0.178093\n",
            "Epoch: 72/100, Loss: 0.174602\n",
            "Epoch: 73/100, Loss: 0.171159\n",
            "Epoch: 74/100, Loss: 0.167764\n",
            "Epoch: 75/100, Loss: 0.164414\n",
            "Epoch: 76/100, Loss: 0.161111\n",
            "Epoch: 77/100, Loss: 0.157860\n",
            "Epoch: 78/100, Loss: 0.154654\n",
            "Epoch: 79/100, Loss: 0.151492\n",
            "Epoch: 80/100, Loss: 0.148376\n",
            "Epoch: 81/100, Loss: 0.145308\n",
            "Epoch: 82/100, Loss: 0.142294\n",
            "Epoch: 83/100, Loss: 0.139346\n",
            "Epoch: 84/100, Loss: 0.136463\n",
            "Epoch: 85/100, Loss: 0.133668\n",
            "Epoch: 86/100, Loss: 0.130930\n",
            "Epoch: 87/100, Loss: 0.128202\n",
            "Epoch: 88/100, Loss: 0.125412\n",
            "Epoch: 89/100, Loss: 0.122573\n",
            "Epoch: 90/100, Loss: 0.119816\n",
            "Epoch: 91/100, Loss: 0.117117\n",
            "Epoch: 92/100, Loss: 0.114481\n",
            "Epoch: 93/100, Loss: 0.111908\n",
            "Epoch: 94/100, Loss: 0.109386\n",
            "Epoch: 95/100, Loss: 0.106919\n",
            "Epoch: 96/100, Loss: 0.104502\n",
            "Epoch: 97/100, Loss: 0.102125\n",
            "Epoch: 98/100, Loss: 0.099791\n",
            "Epoch: 99/100, Loss: 0.097492\n",
            "Epoch: 100/100, Loss: 0.095226\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6428\n",
            "Normalised mutual info score on k-means on latent space: 0.5780577780042793\n",
            "ARI score on k-means on latent space: 0.4704396723130676\n",
            "K-means cluster error on latent space: 42467.31640625\n",
            "K-means silhouette score on latent space: 0.20057105 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6759\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6789515139877697 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5280044378920226 \n",
            "\n",
            "ROUND NUMBER  7 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.958177\n",
            "Epoch: 2/100, Loss: 0.756959\n",
            "Epoch: 3/100, Loss: 0.660834\n",
            "Epoch: 4/100, Loss: 0.616592\n",
            "Epoch: 5/100, Loss: 0.591174\n",
            "Epoch: 6/100, Loss: 0.572791\n",
            "Epoch: 7/100, Loss: 0.557715\n",
            "Epoch: 8/100, Loss: 0.544617\n",
            "Epoch: 9/100, Loss: 0.532809\n",
            "Epoch: 10/100, Loss: 0.521903\n",
            "Epoch: 11/100, Loss: 0.511665\n",
            "Epoch: 12/100, Loss: 0.501973\n",
            "Epoch: 13/100, Loss: 0.492719\n",
            "Epoch: 14/100, Loss: 0.483822\n",
            "Epoch: 15/100, Loss: 0.475253\n",
            "Epoch: 16/100, Loss: 0.466954\n",
            "Epoch: 17/100, Loss: 0.458899\n",
            "Epoch: 18/100, Loss: 0.451068\n",
            "Epoch: 19/100, Loss: 0.443436\n",
            "Epoch: 20/100, Loss: 0.435985\n",
            "Epoch: 21/100, Loss: 0.428711\n",
            "Epoch: 22/100, Loss: 0.421596\n",
            "Epoch: 23/100, Loss: 0.414631\n",
            "Epoch: 24/100, Loss: 0.407804\n",
            "Epoch: 25/100, Loss: 0.401108\n",
            "Epoch: 26/100, Loss: 0.394536\n",
            "Epoch: 27/100, Loss: 0.388086\n",
            "Epoch: 28/100, Loss: 0.381748\n",
            "Epoch: 29/100, Loss: 0.375518\n",
            "Epoch: 30/100, Loss: 0.369390\n",
            "Epoch: 31/100, Loss: 0.363364\n",
            "Epoch: 32/100, Loss: 0.357430\n",
            "Epoch: 33/100, Loss: 0.351585\n",
            "Epoch: 34/100, Loss: 0.345827\n",
            "Epoch: 35/100, Loss: 0.340154\n",
            "Epoch: 36/100, Loss: 0.334563\n",
            "Epoch: 37/100, Loss: 0.329055\n",
            "Epoch: 38/100, Loss: 0.323619\n",
            "Epoch: 39/100, Loss: 0.318259\n",
            "Epoch: 40/100, Loss: 0.312972\n",
            "Epoch: 41/100, Loss: 0.307754\n",
            "Epoch: 42/100, Loss: 0.302600\n",
            "Epoch: 43/100, Loss: 0.297517\n",
            "Epoch: 44/100, Loss: 0.292500\n",
            "Epoch: 45/100, Loss: 0.287547\n",
            "Epoch: 46/100, Loss: 0.282659\n",
            "Epoch: 47/100, Loss: 0.277831\n",
            "Epoch: 48/100, Loss: 0.273065\n",
            "Epoch: 49/100, Loss: 0.268358\n",
            "Epoch: 50/100, Loss: 0.263708\n",
            "Epoch: 51/100, Loss: 0.259115\n",
            "Epoch: 52/100, Loss: 0.254580\n",
            "Epoch: 53/100, Loss: 0.250101\n",
            "Epoch: 54/100, Loss: 0.245678\n",
            "Epoch: 55/100, Loss: 0.241309\n",
            "Epoch: 56/100, Loss: 0.236997\n",
            "Epoch: 57/100, Loss: 0.232736\n",
            "Epoch: 58/100, Loss: 0.228531\n",
            "Epoch: 59/100, Loss: 0.224378\n",
            "Epoch: 60/100, Loss: 0.220276\n",
            "Epoch: 61/100, Loss: 0.216224\n",
            "Epoch: 62/100, Loss: 0.212226\n",
            "Epoch: 63/100, Loss: 0.208277\n",
            "Epoch: 64/100, Loss: 0.204380\n",
            "Epoch: 65/100, Loss: 0.200532\n",
            "Epoch: 66/100, Loss: 0.196735\n",
            "Epoch: 67/100, Loss: 0.192982\n",
            "Epoch: 68/100, Loss: 0.189282\n",
            "Epoch: 69/100, Loss: 0.185630\n",
            "Epoch: 70/100, Loss: 0.182024\n",
            "Epoch: 71/100, Loss: 0.178466\n",
            "Epoch: 72/100, Loss: 0.174955\n",
            "Epoch: 73/100, Loss: 0.171494\n",
            "Epoch: 74/100, Loss: 0.168083\n",
            "Epoch: 75/100, Loss: 0.164722\n",
            "Epoch: 76/100, Loss: 0.161417\n",
            "Epoch: 77/100, Loss: 0.158155\n",
            "Epoch: 78/100, Loss: 0.154946\n",
            "Epoch: 79/100, Loss: 0.151791\n",
            "Epoch: 80/100, Loss: 0.148691\n",
            "Epoch: 81/100, Loss: 0.145658\n",
            "Epoch: 82/100, Loss: 0.142641\n",
            "Epoch: 83/100, Loss: 0.139617\n",
            "Epoch: 84/100, Loss: 0.136601\n",
            "Epoch: 85/100, Loss: 0.133659\n",
            "Epoch: 86/100, Loss: 0.130772\n",
            "Epoch: 87/100, Loss: 0.127939\n",
            "Epoch: 88/100, Loss: 0.125140\n",
            "Epoch: 89/100, Loss: 0.122385\n",
            "Epoch: 90/100, Loss: 0.119687\n",
            "Epoch: 91/100, Loss: 0.117033\n",
            "Epoch: 92/100, Loss: 0.114419\n",
            "Epoch: 93/100, Loss: 0.111847\n",
            "Epoch: 94/100, Loss: 0.109322\n",
            "Epoch: 95/100, Loss: 0.106833\n",
            "Epoch: 96/100, Loss: 0.104397\n",
            "Epoch: 97/100, Loss: 0.102013\n",
            "Epoch: 98/100, Loss: 0.099655\n",
            "Epoch: 99/100, Loss: 0.097364\n",
            "Epoch: 100/100, Loss: 0.095150\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6867\n",
            "Normalised mutual info score on k-means on latent space: 0.6049966734298695\n",
            "ARI score on k-means on latent space: 0.5196148939127299\n",
            "K-means cluster error on latent space: 43627.9140625\n",
            "K-means silhouette score on latent space: 0.19183606 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.6859\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6569676947965791 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5236749062472995 \n",
            "\n",
            "ROUND NUMBER  8 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.948045\n",
            "Epoch: 2/100, Loss: 0.745539\n",
            "Epoch: 3/100, Loss: 0.658350\n",
            "Epoch: 4/100, Loss: 0.616518\n",
            "Epoch: 5/100, Loss: 0.591287\n",
            "Epoch: 6/100, Loss: 0.572888\n",
            "Epoch: 7/100, Loss: 0.557679\n",
            "Epoch: 8/100, Loss: 0.544396\n",
            "Epoch: 9/100, Loss: 0.532409\n",
            "Epoch: 10/100, Loss: 0.521351\n",
            "Epoch: 11/100, Loss: 0.511004\n",
            "Epoch: 12/100, Loss: 0.501230\n",
            "Epoch: 13/100, Loss: 0.491917\n",
            "Epoch: 14/100, Loss: 0.482985\n",
            "Epoch: 15/100, Loss: 0.474378\n",
            "Epoch: 16/100, Loss: 0.466050\n",
            "Epoch: 17/100, Loss: 0.457983\n",
            "Epoch: 18/100, Loss: 0.450138\n",
            "Epoch: 19/100, Loss: 0.442498\n",
            "Epoch: 20/100, Loss: 0.435048\n",
            "Epoch: 21/100, Loss: 0.427767\n",
            "Epoch: 22/100, Loss: 0.420655\n",
            "Epoch: 23/100, Loss: 0.413691\n",
            "Epoch: 24/100, Loss: 0.406861\n",
            "Epoch: 25/100, Loss: 0.400167\n",
            "Epoch: 26/100, Loss: 0.393597\n",
            "Epoch: 27/100, Loss: 0.387148\n",
            "Epoch: 28/100, Loss: 0.380811\n",
            "Epoch: 29/100, Loss: 0.374581\n",
            "Epoch: 30/100, Loss: 0.368452\n",
            "Epoch: 31/100, Loss: 0.362421\n",
            "Epoch: 32/100, Loss: 0.356489\n",
            "Epoch: 33/100, Loss: 0.350645\n",
            "Epoch: 34/100, Loss: 0.344889\n",
            "Epoch: 35/100, Loss: 0.339219\n",
            "Epoch: 36/100, Loss: 0.333636\n",
            "Epoch: 37/100, Loss: 0.328133\n",
            "Epoch: 38/100, Loss: 0.322704\n",
            "Epoch: 39/100, Loss: 0.317352\n",
            "Epoch: 40/100, Loss: 0.312073\n",
            "Epoch: 41/100, Loss: 0.306867\n",
            "Epoch: 42/100, Loss: 0.301729\n",
            "Epoch: 43/100, Loss: 0.296654\n",
            "Epoch: 44/100, Loss: 0.291649\n",
            "Epoch: 45/100, Loss: 0.286705\n",
            "Epoch: 46/100, Loss: 0.281826\n",
            "Epoch: 47/100, Loss: 0.277009\n",
            "Epoch: 48/100, Loss: 0.272252\n",
            "Epoch: 49/100, Loss: 0.267554\n",
            "Epoch: 50/100, Loss: 0.262917\n",
            "Epoch: 51/100, Loss: 0.258339\n",
            "Epoch: 52/100, Loss: 0.253817\n",
            "Epoch: 53/100, Loss: 0.249354\n",
            "Epoch: 54/100, Loss: 0.244946\n",
            "Epoch: 55/100, Loss: 0.240593\n",
            "Epoch: 56/100, Loss: 0.236295\n",
            "Epoch: 57/100, Loss: 0.232052\n",
            "Epoch: 58/100, Loss: 0.227860\n",
            "Epoch: 59/100, Loss: 0.223722\n",
            "Epoch: 60/100, Loss: 0.219639\n",
            "Epoch: 61/100, Loss: 0.215602\n",
            "Epoch: 62/100, Loss: 0.211616\n",
            "Epoch: 63/100, Loss: 0.207682\n",
            "Epoch: 64/100, Loss: 0.203797\n",
            "Epoch: 65/100, Loss: 0.199963\n",
            "Epoch: 66/100, Loss: 0.196178\n",
            "Epoch: 67/100, Loss: 0.192445\n",
            "Epoch: 68/100, Loss: 0.188763\n",
            "Epoch: 69/100, Loss: 0.185126\n",
            "Epoch: 70/100, Loss: 0.181535\n",
            "Epoch: 71/100, Loss: 0.177991\n",
            "Epoch: 72/100, Loss: 0.174494\n",
            "Epoch: 73/100, Loss: 0.171045\n",
            "Epoch: 74/100, Loss: 0.167643\n",
            "Epoch: 75/100, Loss: 0.164289\n",
            "Epoch: 76/100, Loss: 0.160984\n",
            "Epoch: 77/100, Loss: 0.157722\n",
            "Epoch: 78/100, Loss: 0.154509\n",
            "Epoch: 79/100, Loss: 0.151348\n",
            "Epoch: 80/100, Loss: 0.148235\n",
            "Epoch: 81/100, Loss: 0.145173\n",
            "Epoch: 82/100, Loss: 0.142179\n",
            "Epoch: 83/100, Loss: 0.139251\n",
            "Epoch: 84/100, Loss: 0.136401\n",
            "Epoch: 85/100, Loss: 0.133583\n",
            "Epoch: 86/100, Loss: 0.130686\n",
            "Epoch: 87/100, Loss: 0.127761\n",
            "Epoch: 88/100, Loss: 0.124905\n",
            "Epoch: 89/100, Loss: 0.122106\n",
            "Epoch: 90/100, Loss: 0.119376\n",
            "Epoch: 91/100, Loss: 0.116713\n",
            "Epoch: 92/100, Loss: 0.114110\n",
            "Epoch: 93/100, Loss: 0.111549\n",
            "Epoch: 94/100, Loss: 0.109036\n",
            "Epoch: 95/100, Loss: 0.106569\n",
            "Epoch: 96/100, Loss: 0.104161\n",
            "Epoch: 97/100, Loss: 0.101795\n",
            "Epoch: 98/100, Loss: 0.099459\n",
            "Epoch: 99/100, Loss: 0.097167\n",
            "Epoch: 100/100, Loss: 0.094930\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.7049\n",
            "Normalised mutual info score on k-means on latent space: 0.6225483749788248\n",
            "ARI score on k-means on latent space: 0.541090772463066\n",
            "K-means cluster error on latent space: 45567.29296875\n",
            "K-means silhouette score on latent space: 0.17938133 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7205\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.6857953420428506 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5777007786095117 \n",
            "\n",
            "ROUND NUMBER  9 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.942021\n",
            "Epoch: 2/100, Loss: 0.742356\n",
            "Epoch: 3/100, Loss: 0.660983\n",
            "Epoch: 4/100, Loss: 0.619575\n",
            "Epoch: 5/100, Loss: 0.592797\n",
            "Epoch: 6/100, Loss: 0.573384\n",
            "Epoch: 7/100, Loss: 0.557710\n",
            "Epoch: 8/100, Loss: 0.544186\n",
            "Epoch: 9/100, Loss: 0.532089\n",
            "Epoch: 10/100, Loss: 0.520996\n",
            "Epoch: 11/100, Loss: 0.510643\n",
            "Epoch: 12/100, Loss: 0.500873\n",
            "Epoch: 13/100, Loss: 0.491575\n",
            "Epoch: 14/100, Loss: 0.482672\n",
            "Epoch: 15/100, Loss: 0.474110\n",
            "Epoch: 16/100, Loss: 0.465841\n",
            "Epoch: 17/100, Loss: 0.457824\n",
            "Epoch: 18/100, Loss: 0.450042\n",
            "Epoch: 19/100, Loss: 0.442458\n",
            "Epoch: 20/100, Loss: 0.435065\n",
            "Epoch: 21/100, Loss: 0.427845\n",
            "Epoch: 22/100, Loss: 0.420781\n",
            "Epoch: 23/100, Loss: 0.413866\n",
            "Epoch: 24/100, Loss: 0.407090\n",
            "Epoch: 25/100, Loss: 0.400440\n",
            "Epoch: 26/100, Loss: 0.393910\n",
            "Epoch: 27/100, Loss: 0.387497\n",
            "Epoch: 28/100, Loss: 0.381191\n",
            "Epoch: 29/100, Loss: 0.374990\n",
            "Epoch: 30/100, Loss: 0.368894\n",
            "Epoch: 31/100, Loss: 0.362889\n",
            "Epoch: 32/100, Loss: 0.356981\n",
            "Epoch: 33/100, Loss: 0.351158\n",
            "Epoch: 34/100, Loss: 0.345420\n",
            "Epoch: 35/100, Loss: 0.339767\n",
            "Epoch: 36/100, Loss: 0.334199\n",
            "Epoch: 37/100, Loss: 0.328706\n",
            "Epoch: 38/100, Loss: 0.323288\n",
            "Epoch: 39/100, Loss: 0.317949\n",
            "Epoch: 40/100, Loss: 0.312677\n",
            "Epoch: 41/100, Loss: 0.307479\n",
            "Epoch: 42/100, Loss: 0.302346\n",
            "Epoch: 43/100, Loss: 0.297282\n",
            "Epoch: 44/100, Loss: 0.292281\n",
            "Epoch: 45/100, Loss: 0.287341\n",
            "Epoch: 46/100, Loss: 0.282465\n",
            "Epoch: 47/100, Loss: 0.277654\n",
            "Epoch: 48/100, Loss: 0.272904\n",
            "Epoch: 49/100, Loss: 0.268212\n",
            "Epoch: 50/100, Loss: 0.263576\n",
            "Epoch: 51/100, Loss: 0.258999\n",
            "Epoch: 52/100, Loss: 0.254483\n",
            "Epoch: 53/100, Loss: 0.250023\n",
            "Epoch: 54/100, Loss: 0.245610\n",
            "Epoch: 55/100, Loss: 0.241253\n",
            "Epoch: 56/100, Loss: 0.236952\n",
            "Epoch: 57/100, Loss: 0.232707\n",
            "Epoch: 58/100, Loss: 0.228513\n",
            "Epoch: 59/100, Loss: 0.224371\n",
            "Epoch: 60/100, Loss: 0.220283\n",
            "Epoch: 61/100, Loss: 0.216247\n",
            "Epoch: 62/100, Loss: 0.212257\n",
            "Epoch: 63/100, Loss: 0.208319\n",
            "Epoch: 64/100, Loss: 0.204433\n",
            "Epoch: 65/100, Loss: 0.200598\n",
            "Epoch: 66/100, Loss: 0.196810\n",
            "Epoch: 67/100, Loss: 0.193070\n",
            "Epoch: 68/100, Loss: 0.189378\n",
            "Epoch: 69/100, Loss: 0.185733\n",
            "Epoch: 70/100, Loss: 0.182139\n",
            "Epoch: 71/100, Loss: 0.178591\n",
            "Epoch: 72/100, Loss: 0.175089\n",
            "Epoch: 73/100, Loss: 0.171636\n",
            "Epoch: 74/100, Loss: 0.168232\n",
            "Epoch: 75/100, Loss: 0.164880\n",
            "Epoch: 76/100, Loss: 0.161571\n",
            "Epoch: 77/100, Loss: 0.158310\n",
            "Epoch: 78/100, Loss: 0.155091\n",
            "Epoch: 79/100, Loss: 0.151920\n",
            "Epoch: 80/100, Loss: 0.148792\n",
            "Epoch: 81/100, Loss: 0.145709\n",
            "Epoch: 82/100, Loss: 0.142680\n",
            "Epoch: 83/100, Loss: 0.139698\n",
            "Epoch: 84/100, Loss: 0.136762\n",
            "Epoch: 85/100, Loss: 0.133871\n",
            "Epoch: 86/100, Loss: 0.131017\n",
            "Epoch: 87/100, Loss: 0.128205\n",
            "Epoch: 88/100, Loss: 0.125456\n",
            "Epoch: 89/100, Loss: 0.122749\n",
            "Epoch: 90/100, Loss: 0.120063\n",
            "Epoch: 91/100, Loss: 0.117387\n",
            "Epoch: 92/100, Loss: 0.114737\n",
            "Epoch: 93/100, Loss: 0.112145\n",
            "Epoch: 94/100, Loss: 0.109617\n",
            "Epoch: 95/100, Loss: 0.107149\n",
            "Epoch: 96/100, Loss: 0.104740\n",
            "Epoch: 97/100, Loss: 0.102374\n",
            "Epoch: 98/100, Loss: 0.100042\n",
            "Epoch: 99/100, Loss: 0.097738\n",
            "Epoch: 100/100, Loss: 0.095482\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.7017\n",
            "Normalised mutual info score on k-means on latent space: 0.6197735768329962\n",
            "ARI score on k-means on latent space: 0.5383449406347144\n",
            "K-means cluster error on latent space: 41170.859375\n",
            "K-means silhouette score on latent space: 0.17912446 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7607\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.7011707277981031 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.6080117989163447 \n",
            "\n",
            "ROUND NUMBER  10 :\n",
            "\n",
            "Epoch: 1/100, Loss: 0.922868\n",
            "Epoch: 2/100, Loss: 0.735107\n",
            "Epoch: 3/100, Loss: 0.650614\n",
            "Epoch: 4/100, Loss: 0.611562\n",
            "Epoch: 5/100, Loss: 0.587617\n",
            "Epoch: 6/100, Loss: 0.569778\n",
            "Epoch: 7/100, Loss: 0.555125\n",
            "Epoch: 8/100, Loss: 0.542340\n",
            "Epoch: 9/100, Loss: 0.530758\n",
            "Epoch: 10/100, Loss: 0.520026\n",
            "Epoch: 11/100, Loss: 0.509937\n",
            "Epoch: 12/100, Loss: 0.500344\n",
            "Epoch: 13/100, Loss: 0.491167\n",
            "Epoch: 14/100, Loss: 0.482342\n",
            "Epoch: 15/100, Loss: 0.473828\n",
            "Epoch: 16/100, Loss: 0.465581\n",
            "Epoch: 17/100, Loss: 0.457574\n",
            "Epoch: 18/100, Loss: 0.449779\n",
            "Epoch: 19/100, Loss: 0.442180\n",
            "Epoch: 20/100, Loss: 0.434761\n",
            "Epoch: 21/100, Loss: 0.427516\n",
            "Epoch: 22/100, Loss: 0.420435\n",
            "Epoch: 23/100, Loss: 0.413500\n",
            "Epoch: 24/100, Loss: 0.406701\n",
            "Epoch: 25/100, Loss: 0.400041\n",
            "Epoch: 26/100, Loss: 0.393501\n",
            "Epoch: 27/100, Loss: 0.387083\n",
            "Epoch: 28/100, Loss: 0.380773\n",
            "Epoch: 29/100, Loss: 0.374570\n",
            "Epoch: 30/100, Loss: 0.368473\n",
            "Epoch: 31/100, Loss: 0.362474\n",
            "Epoch: 32/100, Loss: 0.356567\n",
            "Epoch: 33/100, Loss: 0.350753\n",
            "Epoch: 34/100, Loss: 0.345029\n",
            "Epoch: 35/100, Loss: 0.339393\n",
            "Epoch: 36/100, Loss: 0.333838\n",
            "Epoch: 37/100, Loss: 0.328362\n",
            "Epoch: 38/100, Loss: 0.322964\n",
            "Epoch: 39/100, Loss: 0.317643\n",
            "Epoch: 40/100, Loss: 0.312390\n",
            "Epoch: 41/100, Loss: 0.307206\n",
            "Epoch: 42/100, Loss: 0.302092\n",
            "Epoch: 43/100, Loss: 0.297046\n",
            "Epoch: 44/100, Loss: 0.292061\n",
            "Epoch: 45/100, Loss: 0.287140\n",
            "Epoch: 46/100, Loss: 0.282283\n",
            "Epoch: 47/100, Loss: 0.277487\n",
            "Epoch: 48/100, Loss: 0.272751\n",
            "Epoch: 49/100, Loss: 0.268075\n",
            "Epoch: 50/100, Loss: 0.263457\n",
            "Epoch: 51/100, Loss: 0.258895\n",
            "Epoch: 52/100, Loss: 0.254389\n",
            "Epoch: 53/100, Loss: 0.249940\n",
            "Epoch: 54/100, Loss: 0.245544\n",
            "Epoch: 55/100, Loss: 0.241203\n",
            "Epoch: 56/100, Loss: 0.236916\n",
            "Epoch: 57/100, Loss: 0.232682\n",
            "Epoch: 58/100, Loss: 0.228504\n",
            "Epoch: 59/100, Loss: 0.224373\n",
            "Epoch: 60/100, Loss: 0.220294\n",
            "Epoch: 61/100, Loss: 0.216268\n",
            "Epoch: 62/100, Loss: 0.212290\n",
            "Epoch: 63/100, Loss: 0.208364\n",
            "Epoch: 64/100, Loss: 0.204489\n",
            "Epoch: 65/100, Loss: 0.200662\n",
            "Epoch: 66/100, Loss: 0.196880\n",
            "Epoch: 67/100, Loss: 0.193155\n",
            "Epoch: 68/100, Loss: 0.189475\n",
            "Epoch: 69/100, Loss: 0.185843\n",
            "Epoch: 70/100, Loss: 0.182259\n",
            "Epoch: 71/100, Loss: 0.178721\n",
            "Epoch: 72/100, Loss: 0.175229\n",
            "Epoch: 73/100, Loss: 0.171786\n",
            "Epoch: 74/100, Loss: 0.168392\n",
            "Epoch: 75/100, Loss: 0.165044\n",
            "Epoch: 76/100, Loss: 0.161738\n",
            "Epoch: 77/100, Loss: 0.158478\n",
            "Epoch: 78/100, Loss: 0.155265\n",
            "Epoch: 79/100, Loss: 0.152100\n",
            "Epoch: 80/100, Loss: 0.148978\n",
            "Epoch: 81/100, Loss: 0.145908\n",
            "Epoch: 82/100, Loss: 0.142891\n",
            "Epoch: 83/100, Loss: 0.139914\n",
            "Epoch: 84/100, Loss: 0.136985\n",
            "Epoch: 85/100, Loss: 0.134104\n",
            "Epoch: 86/100, Loss: 0.131290\n",
            "Epoch: 87/100, Loss: 0.128555\n",
            "Epoch: 88/100, Loss: 0.125883\n",
            "Epoch: 89/100, Loss: 0.123227\n",
            "Epoch: 90/100, Loss: 0.120524\n",
            "Epoch: 91/100, Loss: 0.117804\n",
            "Epoch: 92/100, Loss: 0.115116\n",
            "Epoch: 93/100, Loss: 0.112494\n",
            "Epoch: 94/100, Loss: 0.109951\n",
            "Epoch: 95/100, Loss: 0.107472\n",
            "Epoch: 96/100, Loss: 0.105030\n",
            "Epoch: 97/100, Loss: 0.102611\n",
            "Epoch: 98/100, Loss: 0.100236\n",
            "Epoch: 99/100, Loss: 0.097919\n",
            "Epoch: 100/100, Loss: 0.095648\n",
            "Creating a k-means model on latent data:\n",
            "\n",
            "K-means with 10 clusters on latent space stats: \n",
            "\n",
            "K-means on latent space greedy accuracy score: 0.6566\n",
            "Normalised mutual info score on k-means on latent space: 0.602014321622476\n",
            "ARI score on k-means on latent space: 0.4887370764993758\n",
            "K-means cluster error on latent space: 44645.58984375\n",
            "K-means silhouette score on latent space: 0.1902564 \n",
            "\n",
            "Doing agglomerative clustering on MLP output vectors:\n",
            "\n",
            "Agglomerative clustering on latent space greedy accuracy score: 0.7272\n",
            "Normalised mutual info score on agglomerative clustering on latent space: 0.7142297028040119 \n",
            "\n",
            "ARI score on agglomerative clustering on latent space: 0.5960946630676562 \n",
            "\n",
            "Average k-means accuracy score at latent space: 0.67398 \n",
            "\n",
            "Average k-means NMI score at latent space: 0.6041255859011769 \n",
            "\n",
            "Average k-means ARI score at latent space: 0.509187924071149 \n",
            "\n",
            "Average agglomerative clustering accuracy score at latent space: 0.7042900000000001 \n",
            "\n",
            "Average agglomerative clustering NMI score at latent space: 0.6839772327946119 \n",
            "\n",
            "Average agglomerative clustering ARI score at latent space: 0.5650258228432591 \n",
            "\n",
            "Average k-means silhouette score on latent space: 0.18741291910409927 \n",
            "\n",
            "Average k-means cluster error on latent space: 43468.1375 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans_acc = np.zeros((10,10))\n",
        "kmeans_NMI = np.zeros((10,10))\n",
        "kmeans_ARI = np.zeros((10,10))\n",
        "agglo_acc = np.zeros((10,10))\n",
        "agglo_NMI = np.zeros((10,10))\n",
        "agglo_ARI = np.zeros((10,10))\n",
        "kmeans_silhouette = np.zeros((10,10))\n",
        "agglo_silhouette = np.zeros((10,10))\n",
        "\n",
        "for i in range(10):\n",
        "  \n",
        "  kmeans_acc[i][0] = results_for_k_20[0][i]\n",
        "  kmeans_acc[i][1] = results_for_k_30[0][i]\n",
        "  kmeans_acc[i][2] = results_for_k_40[0][i]\n",
        "  kmeans_acc[i][3] = results_for_k_50[0][i]\n",
        "  kmeans_acc[i][4] = results_for_k_60[0][i]\n",
        "  kmeans_acc[i][5] = results_for_k_70[0][i]\n",
        "  kmeans_acc[i][6] = results_for_k_80[0][i]\n",
        "  kmeans_acc[i][7] = results_for_k_100[0][i]\n",
        "  kmeans_acc[i][8] = results_for_k_150[0][i]\n",
        "  kmeans_acc[i][9] = results_for_k_200[0][i]\n",
        "\n",
        "  kmeans_NMI[i][0] = results_for_k_20[1][i]\n",
        "  kmeans_NMI[i][1] = results_for_k_30[1][i]\n",
        "  kmeans_NMI[i][2] = results_for_k_40[1][i]\n",
        "  kmeans_NMI[i][3] = results_for_k_50[1][i]\n",
        "  kmeans_NMI[i][4] = results_for_k_60[1][i]\n",
        "  kmeans_NMI[i][5] = results_for_k_70[1][i]\n",
        "  kmeans_NMI[i][6] = results_for_k_80[1][i]\n",
        "  kmeans_NMI[i][7] = results_for_k_100[1][i]\n",
        "  kmeans_NMI[i][8] = results_for_k_150[1][i]\n",
        "  kmeans_NMI[i][9] = results_for_k_200[1][i]\n",
        "\n",
        "  kmeans_ARI[i][0] = results_for_k_20[2][i]\n",
        "  kmeans_ARI[i][1] = results_for_k_30[2][i]\n",
        "  kmeans_ARI[i][2] = results_for_k_40[2][i]\n",
        "  kmeans_ARI[i][3] = results_for_k_50[2][i]\n",
        "  kmeans_ARI[i][4] = results_for_k_60[2][i]\n",
        "  kmeans_ARI[i][5] = results_for_k_70[2][i]\n",
        "  kmeans_ARI[i][6] = results_for_k_80[2][i]\n",
        "  kmeans_ARI[i][7] = results_for_k_100[2][i]\n",
        "  kmeans_ARI[i][8] = results_for_k_150[2][i]\n",
        "  kmeans_ARI[i][9] = results_for_k_200[2][i]\n",
        "\n",
        "  agglo_acc[i][0] = results_for_k_20[3][i]\n",
        "  agglo_acc[i][1] = results_for_k_30[3][i]\n",
        "  agglo_acc[i][2] = results_for_k_40[3][i]\n",
        "  agglo_acc[i][3] = results_for_k_50[3][i]\n",
        "  agglo_acc[i][4] = results_for_k_60[3][i]\n",
        "  agglo_acc[i][5] = results_for_k_70[3][i]\n",
        "  agglo_acc[i][6] = results_for_k_80[3][i]\n",
        "  agglo_acc[i][7] = results_for_k_100[3][i]\n",
        "  agglo_acc[i][8] = results_for_k_150[3][i]\n",
        "  agglo_acc[i][9] = results_for_k_200[3][i]\n",
        "\n",
        "  agglo_NMI[i][0] = results_for_k_20[4][i]\n",
        "  agglo_NMI[i][1] = results_for_k_30[4][i]\n",
        "  agglo_NMI[i][2] = results_for_k_40[4][i]\n",
        "  agglo_NMI[i][3] = results_for_k_50[4][i]\n",
        "  agglo_NMI[i][4] = results_for_k_60[4][i]\n",
        "  agglo_NMI[i][5] = results_for_k_70[4][i]\n",
        "  agglo_NMI[i][6] = results_for_k_80[4][i]\n",
        "  agglo_NMI[i][7] = results_for_k_100[4][i]\n",
        "  agglo_NMI[i][8] = results_for_k_150[4][i]\n",
        "  agglo_NMI[i][9] = results_for_k_200[4][i]\n",
        "\n",
        "  agglo_ARI[i][0] = results_for_k_20[5][i]\n",
        "  agglo_ARI[i][1] = results_for_k_30[5][i]\n",
        "  agglo_ARI[i][2] = results_for_k_40[5][i]\n",
        "  agglo_ARI[i][3] = results_for_k_50[5][i]\n",
        "  agglo_ARI[i][4] = results_for_k_60[5][i]\n",
        "  agglo_ARI[i][5] = results_for_k_70[5][i]\n",
        "  agglo_ARI[i][6] = results_for_k_80[5][i]\n",
        "  agglo_ARI[i][7] = results_for_k_100[5][i]\n",
        "  agglo_ARI[i][8] = results_for_k_150[5][i]\n",
        "  agglo_ARI[i][9] = results_for_k_200[5][i]\n",
        "\n",
        "  kmeans_silhouette[i][0] = results_for_k_20[6][i]\n",
        "  kmeans_silhouette[i][1] = results_for_k_30[6][i]\n",
        "  kmeans_silhouette[i][2] = results_for_k_40[6][i]\n",
        "  kmeans_silhouette[i][3] = results_for_k_50[6][i]\n",
        "  kmeans_silhouette[i][4] = results_for_k_60[6][i]\n",
        "  kmeans_silhouette[i][5] = results_for_k_70[6][i]\n",
        "  kmeans_silhouette[i][6] = results_for_k_80[6][i]\n",
        "  kmeans_silhouette[i][7] = results_for_k_100[6][i]\n",
        "  kmeans_silhouette[i][8] = results_for_k_150[6][i]\n",
        "  kmeans_silhouette[i][9] = results_for_k_200[6][i]\n",
        "\n",
        "  agglo_silhouette[i][0] = results_for_k_20[7][i]\n",
        "  agglo_silhouette[i][1] = results_for_k_30[7][i]\n",
        "  agglo_silhouette[i][2] = results_for_k_40[7][i]\n",
        "  agglo_silhouette[i][3] = results_for_k_50[7][i]\n",
        "  agglo_silhouette[i][4] = results_for_k_60[7][i]\n",
        "  agglo_silhouette[i][5] = results_for_k_70[7][i]\n",
        "  agglo_silhouette[i][6] = results_for_k_80[7][i]\n",
        "  agglo_silhouette[i][7] = results_for_k_100[7][i]\n",
        "  agglo_silhouette[i][8] = results_for_k_150[7][i]\n",
        "  agglo_silhouette[i][9] = results_for_k_200[7][i]\n",
        "\n"
      ],
      "metadata": {
        "id": "9rd88pQAdEkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "id": "06bNou-EjDN2",
        "outputId": "f179b596-71d9-4974-86ab-329e44ce9256"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'K')"
            ]
          },
          "metadata": {},
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1008x792 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "columns = ['20', '30', '40', '50', '60', '70', '80', '100', '150', '200']\n",
        "\n",
        "df = pd.DataFrame(kmeans_acc,\n",
        "                  columns=columns)\n",
        "boxplot = df.boxplot(column=columns, figsize=(14,11))  \n",
        "boxplot.set_ylabel('K-means accuracy on latent space')\n",
        "boxplot.set_xlabel('K')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "id": "WqpclwnajDN2",
        "outputId": "10ef5ac5-2f8a-42cf-fc2a-a84be2f3717a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'K')"
            ]
          },
          "metadata": {},
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1008x792 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "df = pd.DataFrame(kmeans_NMI,\n",
        "                  columns=columns)\n",
        "boxplot = df.boxplot(column=columns, figsize=(14,11))  \n",
        "boxplot.set_ylabel('K-means NMI on latent space')\n",
        "boxplot.set_xlabel('K')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "id": "i0Jov5dHjDN2",
        "outputId": "7290fec5-4cb8-4c65-d130-fc01e2a74ae3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'K')"
            ]
          },
          "metadata": {},
          "execution_count": 43
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1008x792 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0YAAAKDCAYAAAA+dhqrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdf7hld10f+vcnM0ECGeIPMI1JYGIvtUODKKQRa649Qy4IjSRWrGWE1ujY2FYE7MUydO4DAneeJrbSypV6pQwarjpBqa2RCQlc7xw1rWASBZRMkQghhFpBkJCBCEn83D/OHj0M82Nns/Y5+5z1ej3Pfmavtdf+ns/+zN7nnPdZa31XdXcAAADG7LT1LgAAAGC9CUYAAMDoCUYAAMDoCUYAAMDoCUYAAMDoCUYAAMDobV3vAoby6Ec/urdv377eZZzQZz7zmTzykY9c7zI2HH2bjb7NRt9mo2+z0bfZ6Nts9G02+ja7Re7dbbfd9qfd/Zhj12+aYLR9+/bceuut613GCS0vL2dpaWm9y9hw9G02+jYbfZuNvs1G32ajb7PRt9no2+wWuXdV9eHjrXcoHQAAMHqCEQAAMHqCEQAAMHqCEQAAMHqCEQAAMHqCEQAAMHqCEQAAMHqCEQAAMHqCEQAAMHqCEQAAMHqCEQAAMHqCEQAAMHqCEQAAMHqCEQAAMHqCEQAAMHqCEQAAMHpzDUZV9cyqen9V3VFVe47z+JVV9fGqevfk9gOrHntsVb29qg5X1e1VtX2etQIAAOO1dV4DV9WWJK9L8vQkdye5paqu7+7bj9n0zd39guMM8aYk+7r7HVV1ZpK/mFetAADAuM1zj9HFSe7o7g929+eTXJfkimmeWFVPSLK1u9+RJN19pLs/O79SAQCAMZtnMDo3yUdWLd89WXes51TVe6vqLVV1/mTd30jyqar6lar6var6N5M9UAAAAIOr7p7PwFXfleSZ3f0Dk+V/lOSbVh82V1VfleRId3+uqn4wyT/s7qdNnrs/yTcmuSvJm5Pc0N37j/kaVyW5KknOPvvsp1x33XVzeS1DOHLkSM4888z1LmPD0bfZ6Nts9G02+jYbfZuNvs1G32ajb7Nb5N7t3Lnztu6+6Nj1czvHKMlHk5y/avm8ybq/1N2fWLX4hiQ/Prl/d5J3d/cHk6Sq/kuSp2YlLK1+/uuTvD5JLrrool5aWhqw/GEtLy9nketbVPo2G32bjb7NRt9mo2+z0bfZ6Nts9G12G7F38zyU7pYkj6+qC6rqYUmem+T61RtU1TmrFi9PcnjVc7+8qh4zWX5akmMnbQAAABjE3PYYdfcDVfWCJDcl2ZLkjd39vqp6VZJbu/v6JC+sqsuTPJDkk0munDz3wap6SZJfr6pKcluS/zivWgEAgHGb56F06e4bktxwzLqXr7r/siQvO8Fz35Hk6+dZHwAAQDLnYASwCFZ2PA9rXhPXAADrY57nGAEshO6e6va4l7516m0BgM1FMAIAAEZPMAIAAEZPMAIAAEZPMAIAAEZPMAIAAEZPMAIAAEZPMAIAAEZPMAIAAEZPMAIAAEZPMAIAAEZPMAIAAEZPMAIAAEZPMAIAAEZPMAIAAEZPMAIAAEZPMAIAAEZPMAIAAEZv63oXAABQVYOP2d2DjwlsXvYYAQDrrrunuj3upW+deluAh0IwAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wYiEdOHAgF154YS699NJceOGFOXDgwHqXBABsIlV1ytvOnTun2m4e082z9lzHiIVz4MCB7N27N/v378+DDz6YLVu2ZPfu3UmSXbt2rXN1AMBmMM2U7tv3HMydV1+2BtWwCOwxYuHs27cv+/fvz86dO7N169bs3Lkz+/fvz759+9a7NAAANinBiIVz+PDhXHLJJV+w7pJLLsnhw4fXqSIAADY7wYiFs2PHjtx8881fsO7mm2/Ojh071qkiAAA2O8GIhbN3797s3r07hw4dygMPPJBDhw5l9+7d2bt373qXBgDAJmXyBRbO0QkWfviHfziHDx/Ojh07sm/fPhMvAAAwN4IRC2nXrl3ZtWtXlpeXs7S0tN7lAACwyTmUDgAAGD3BCAAAGD3BCAAAGD3BCAAAGD3BCAAAGD3BCAAAGD3BCAAAGD3BCAAAGD0XeAUA2MCqatDxunvQ8WCjsMcIAGAD6+5T3h730rdOtZ1QxJgJRgAAwOg5lA6A4xr68JzEIToALC57jAA4rmkPu3GIDgCbgWAEAACMnmAEAACMnmAEAACMnmAEAACMnmAEAACMnmAEAACMnmAEAACMnmAEAACMnmAEAACM3tb1LgCYXlUNPmZ3Dz4mAMBGY48RbCDdPdXtcS9969TbAgAgGAEAAAhGAAAAghEAADB6ghEAADB6ghEAADB6ghEAADB6ghEAADB6ghEAADB6ghEAADB6ghEAADB6ghEAADB6ghEAADB6ghEAADB6ghEAADB6ghEAADB6ghEAADB6W9e7AAAAYGOoqkHH6+5Bx/tSzHWPUVU9s6reX1V3VNWe4zx+ZVV9vKrePbn9wDGPP6qq7q6qn5pnnQAAwKl191S3x730rVNtt0jmtseoqrYkeV2Spye5O8ktVXV9d99+zKZv7u4XnGCYVyf5zXnVCAAAkMx3j9HFSe7o7g929+eTXJfkimmfXFVPSXJ2krfPqT4AAIAk8w1G5yb5yKrluyfrjvWcqnpvVb2lqs5Pkqo6LclPJHnJHOsDAABIsv6TL/xakgPd/bmq+sEk1yZ5WpJ/nuSG7r77ZCd4VdVVSa5KkrPPPjvLy8vzr3hGR44cWej6FpW+zU7fZqNvs9G3h873t9np22z0bTb6NruN1rt5BqOPJjl/1fJ5k3V/qbs/sWrxDUl+fHL/m5P8r1X1z5OcmeRhVXWku/cc8/zXJ3l9klx00UW9tLQ06AuYxmaemWMRLC8vZz3+Xze8Gw/q2yz0bTb6NhPf32bk/TYbfZuNvs1uA/ZunofS3ZLk8VV1QVU9LMlzk1y/eoOqOmfV4uVJDidJdz+vux/b3duzcjjdm44NRYtiM8/MAQAAYzG3PUbd/UBVvSDJTUm2JHljd7+vql6V5Nbuvj7JC6vq8iQPJPlkkivnVQ8AAMCJzPUco+6+IckNx6x7+ar7L0vyslOM8XNJfm4O5QEAACSZ8wVeAQAANgLBCAAAGD3BCAAAGD3BCAAAGD3BCAAAGL25zkoHAPCkV74999x3/2Djbd9zcLCxzjrj9LznFc8YbDxg4xKMAIC5uue++3Pn1ZcNMtby8nKWlpYGGSsZNmQBG5tD6QAAgNETjAAAgNETjAAAgNETjAAAgNEz+cIJDD2DTjLcCZ5m0AEAgGEJRicw5Aw6ybCz6JhBBwAAhuVQOgAAYPQEIwAAYPQEIwAAYPQEIwAAYPRMvgAAsICGniF3yMmbzJDLZiQYAQAsoCFnyB1ydtzEDLlsTg6lAwAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARm/rehcAAABDedIr35577rt/sPG27zk42FhnnXF63vOKZww2HsMSjABgQFU1+JjdPfiYsFndc9/9ufPqywYZa3l5OUtLS4OMlQwbshieQ+kAYEDdPdXtcS9969TbAjB/ghEAADB6ghEAADB6zjECAICRG3rSimS4c6rWatIKwQgAAEZuyEkrkmEnrlirSSscSgcAAIyeYAQAAIyeYAQAAIyeYAQAAIyeYAQAAIyeYAQAAIye6bqBDcs1FwCAoQhGwIblmgsAwFAcSgcAAIyeYAQAAIyeYAQAAIyec4wAgLnatmNPnnjtnuEGvHa4obbtSJLhzlUENi7BCACYq3sPXz3YRClDTpKSmCgF+CsOpQMAAEZPMAIAAEZPMAIAAEZPMAIAAEZPMAIAAEZPMAIAAEZPMAIAAEZPMAIAAEZPMAIAAEZPMAIAAEZPMAIAAEZPMAIAAEZPMAIAAEZv63oXACRPeuXbc8999w865vY9BwcZ56wzTs97XvGMQcYCAFhUcw1GVfXMJD+ZZEuSN3T31cc8fmWSf5Pko5NVP9Xdb6iqb0jy00keleTBJPu6+83zrBXW0z333Z87r75ssPGWl5eztLQ0yFhDBSwAgEU2t2BUVVuSvC7J05PcneSWqrq+u28/ZtM3d/cLjln32ST/uLs/UFVfk+S2qrqpuz81r3oBAIDxmuceo4uT3NHdH0ySqrouyRVJjg1GX6S7/3DV/f9RVR9L8pgkaxaMtu3Ykydeu2fYQa8dZphtO5JkuL0LAAAwdvMMRucm+ciq5buTfNNxtntOVX1rkj9M8iPdvfo5qaqLkzwsyR/Nq9Djuffw1Q5tAgCAkVjvyRd+LcmB7v5cVf1gVvapPO3og1V1TpL/J8n3dvdfHPvkqroqyVVJcvbZZ2d5eXnQ4oYc78iRI4OON/RrXVRD922Reb/NRt8Ww5he65DG1LehXus8fi4s8v+Dvs1G32Yz+p+p3T2XW5JvTnLTquWXJXnZSbbfkuSeVcuPSvK7Sb5rmq/3lKc8pYf0uJe+ddDxDh06NNhYQ9e2yIbs2yLzfpuNvi2GMb3WIY2pb0O+1qF/Lizy/4O+zUbfZjOmn6lJbu3j5Il5XsfoliSPr6oLquphSZ6b5PrVG0z2CB11eZLDk/UPS/Kfk7ypu98yxxoBAADmdyhddz9QVS9IclNW9ga9sbvfV1WvykpKuz7JC6vq8iQPJPlkkisnT//uJN+a5KsmU3onyZXd/e551QsAAIzXXM8x6u4bktxwzLqXr7r/sqwcYnfs834+yc/PszYAAICj5nkoHQAAwIYgGAEAAKMnGAEAAKO33tcxAgDgOLbt2JMnXrtnuAGvHW6obTuS5LLhBoQFIBgBACygew9fnTuvHiZ8LC8vZ2lpaZCxkmT7noODjQWLwqF0AADA6AlGAADA6AlGAADA6AlGAADA6AlGAADA6AlGAADA6JmuGwCYu0Gnd75xuLHOOuP0wcYCNjbBCACYq6GuxZOsBKwhxwM4yqF0AADA6AlGAADA6AlGAADA6AlGAADA6AlGAADA6AlGAADA6AlGAADA6AlGAADA6AlGAADA6AlGAADA6AlGAADA6AlGAADA6AlGAADA6AlGAADA6AlGAADA6AlGAADA6AlGAADA6AlGAADA6AlGAADA6AlGAADA6E0djKrqEfMsBAAAYL2cMhhV1d+pqtuT/PfJ8pOq6j/MvTIAAIA1Ms0eo3+X5NuSfCJJuvs9Sb51nkUBAACspa3TbNTdH6mq1asenE85AADAWtu2Y0+eeO2eYQe9dphhtu1IksuGGewkpglGH6mqv5Okq+r0JC9Kcni+ZQEAAGvl3sNX586rhwsfy8vLWVpaGmSs7XsODjLOqUwTjP5pkp9Mcm6SjyZ5e5IfmmdRAMzPk1759txz3/2DjjnkD62zzjg973nFMwYbDwCmccpg1N1/muR5a1ALAGvgnvvuX9i/CiZr95dBAFhtmlnprq2qL1+1/BVV9cb5lgUAALB2ppmV7uu7+1NHF7r7z5J84/xKAgAAWFvTBKPTquorji5U1VdmytnsAAAANoJpAs5PJPntqvrlJJXku5Lsm2tVAAAAa2iayRfeVFW3Jdk5WfWd3X37fMsCAABYO9Ne4PV9VfXxJA9Pkqp6bHffNdfKAAAA1sg0s9JdXlUfSPKhJL+R5M4kb5tzXQAAAGtmmskXXp3kqUn+sLsvSHJpknfOtSoAAIA1NE0wur+7P5GV2elO6+5DSS6ac10AAABrZppzjD5VVWcm+a0kv1BVH0vymfmWBQDA9j0HhxvsxuHGOuuM0wcbCxbFNMHoiiR/nuTFSZ6X5Kwkr5pnUQCwiJ70yrfnnvvuH2y8IX/pPeuM0/OeVzxjsPFYf3defdlgY23fc3DQ8WAzmma67s9U1V9LcnGSTya5aXJoHQCMyj333T/YL5fLy8tZWloaZKxk4D0LACM0zax0P5Dkd5J8Z1Yu7vrOqvr+eRcGAACwVqY5lO5Hk3zj0b1EVfVVSf5bkjfOszAAAIC1Ms2sdJ9Icu+q5Xsn6wAAADaFafYY3ZHkXVX1q0k6K5MxvLeq/kWSdPdr5lgfAADA3E0TjP5ocjvqVyf/bhu+HMaiqgYdr7sHHQ8AgHGZZla6Vx69X1WnJTmzuz8916rY9KYNMqYXBQBgLUwzK90vVtWjquqRSf4gye1V9aPzLw0AAGBtTDP5whMme4i+I8nbklyQ5B/NtSoAAIA1NE0wOr2qTs9KMLq+u+/PyiQMAAAAm8I0ky/8TJI7k7wnyW9W1eOSOMcIWHfbduzJE6/dM+yg1w4zzLYdSeL8OADYKKaZfOG1SV57dLmq7kqyc55FAUzj3sNXDzo5x/LycpaWlgYZa/ueg4OMAwCsjWn2GH2BXplO7IE51AIAALAupjnHCAAAYFObZrruL5tmHQAAwEY1zR6j355yHQAAwIZ0wnOMquqvJTk3yRlV9Y1JavLQo5I8Yg1qAwAAWBMnm3zh25JcmeS8JK9Ztf7eJP9qjjUBAACsqRMGo+6+Nsm1VfWc7v5Pa1gTG9iTXvn23HPf/YOOOdS0x2edcXre84pnDDIWAACbyzTTdb+1qr4nyfbV23f3q+ZVFBvXPffd77oyAABsONMEo19Nck+S25J8br7lAAAArL1pgtF53f3MuVcCAACwTqaZrvu/VdUT514JAADAOplmj9ElSa6sqg9l5VC6StLd/fVzrQwAAGCNTLPH6FlJHp/kGUmeneTbJ/+eUlU9s6reX1V3VNWe4zx+ZVV9vKrePbn9wKrHvreqPjC5fe90LwcAAOChO+Ueo+7+cFVdkuTx3f2zVfWYJGee6nlVtSXJ65I8PcndSW6pquu7+/ZjNn1zd7/gmOd+ZZJXJLkoSSe5bfLcP5vqVQEAADwEp9xjVFWvSPLSJC+brDo9yc9PMfbFSe7o7g929+eTXJfkiinr+rYk7+juT07C0DuSmAACAACYi2kOpfv7SS5P8pkk6e7/kWTbFM87N8lHVi3fPVl3rOdU1Xur6i1Vdf5DfC4AAMCXbJrJFz7f3V1VnSRV9cgBv/6vJTnQ3Z+rqh9Mcm2Sp0375Kq6KslVSXL22WdneXl5wNIy6HhHjhwZdLyhX+tQtu3Ykyde+0Wnk31prh1mmG07kuXlId++w/J+m42+zWaR+5aMo3dj69uQxvI6hzamvvmczmaRfzasSd+6+6S3JC9J8jNJPpjknyT57SQvnOJ535zkplXLL0vyspNsvyXJPZP7u5L8zKrHfibJrpN9vac85Sk9pMe99K2Djnfo0KHBxhq6tiHp22z0bTb6NptF7lv3eHo3pr4NaSyvc2hj6pvP6WwW+WfD0LUlubWPkydOeShdd//bJG9J8p+SfF2Sl3f3a6fIXLckeXxVXVBVD0vy3CTXr96gqs5ZtXh5ksOT+zcleUZVfUVVfUVWZsS7aYqvCQAA8JCd8lC6qrqmu1+alQkQjl13Qt39QFW9ICuBZkuSN3b3+6rqVVlJadcneWFVXZ7kgSSfTHLl5LmfrKpXZyVcJcmruvuTD/3lAQAAnNo05xg9PSuz0q32rOOs+yLdfUOSG45Z9/JV91+Wv5rt7tjnvjHJG6eoDza8RT83K7lsmMEAABbUCYNRVf2zJP88yddW1XtXPbQtyX+dd2EwJvcevjp3Xj1c+FheXs7S0tIgY23fc3CQcQAAFtnJ9hj9YpK3JfnXSVb/Kfteh7UBAACbyQmDUXffk+SerMwQl6r66iQPT3JmVZ3Z3XetTYkAAADzdcpZ6arq2VX1gSQfSvIbSe7Myp4kAACATeGUwSjJ/5nkqUn+sLsvSHJpknfOtSoAAIA1NE0wur+7P5HktKo6rbsPJbloznUBAACsmWmm6/5UVZ2Z5DeT/EJVfSzJZ+ZbFgAAwNqZZo/RFUnuS/IjSW5M8kdJnj3PogAAANbSKfcYdffqvUMDXTISAABgcZzsAq/3JunjPZSku/tRc6sKAABgDZ3sOkbb1rIQAACA9TLNOUYAAACbmmAEAACMnmAEAACM3jTXMQIAYEFV1XTbXTPdeN3Hm3tr49i2Y0+eeO2e4QYccE7mbTuS5LLhBmRQpwxGVfWdSa5J8tVZmZHOrHQAAAtimiCzvLycpaWl+RezAO49fHXuvHqY8DF037bvOTjYWAxvmj1GP57k2d19eN7FAAAArIdpzjH6E6EIAADYzKbZY3RrVb05yX9J8rmjK7v7V+ZWFQAAwBqaJhg9Kslnkzxj1bpOIhgBAACbwimDUXd/31oUsogGP0HuxmHGO+uM0wcZBwAAWDHNrHTnJfm/knzLZNVvJXlRd989z8LW21CzmRy1fc/BwccEAACGMc2hdD+b5BeT/IPJ8vMn654+r6IAYBG5PgrA5jVNMHpMd//squWfq6oXz6sgAFhUro8CsHlNM133J6rq+VW1ZXJ7fpJPzLswAACAtTJNMPr+JN+d5H8m+eMk35VktBMyAAAAm880s9J9OMnla1ALAADAuphmjxEAAMCmJhgBAACjJxgBAACjd8JzjKrqX5zsid39muHLAQAAWHsnm3xh25pVAQAAsI5OGIy6+5UneqyqHjmfcgAAgPUw+IWibxxmvLPOOH2QcU7lpNN1V9W5Sc5J8t7u/nxVfXWSFye5MsnXzL88AABg3u68+rJBx9u+5+DgY87byc4xenGSvUnuSPJlVfUfklyT5E1JnrI25QEAY1BV0297zXTbdfeM1QBjdLI9Rlcl+bru/mRVPTbJHyb5lu6+bW1KAwDGYtoQs7y8nKWlpfkWA4zSyYLRn3f3J5Oku++qqvcLRQAb37Yde/LEa/cMO+i1ww21bUeSbKzDLwDY+E4WjM6rqteuWj5n9XJ3v3B+ZQEwL/cevnrQ476H/gv+4Cf/AsAUThaMfvSYZXuLAACATelk03Uf98CIqnp4kmfPrSI2vLFP9QgAwMZz0um6j6qqLUm+LcmuJM9I8ltJfnmOdbFBmeoRAICN6FTXMfq7Sb4nyd9L8jtJviXJBd392TWoDQAAYE2c7DpGdye5K8lPJ3lJd99bVR8SigAAgM3mtJM89pYkX5PkHyZ5dlU9MokrpQEAAJvOCYNRd784yQVJfiLJUpL3J3lMVX13VZ25NuUBAADM38n2GKVXHOruq7ISknYluSLJnWtQGwAAwJqYala6JOnu+5O8Nclbq+qM+ZUEAACwtk66x+hEuvu+oQsBAABYLzMFIwAAgM3kIQWjqjqtqh41r2IAAADWwymDUVX9YlU9ajJd9x8kub2qfnT+pQEAAKyNafYYPaG7P53kO5K8LSuz0/2juVYFAACwhqYJRqdX1elZCUbXT2anc6FXAABg05hmuu6fycp1i96T5Der6nFJPj3PogCmtX3PwWEHvHGY8c464/RBxgEA1sYpg1F3vzbJa1et+nBV7ZxfSQDTufPqywYdb/ueg4OPCQBsDKcMRlX1ZUmek2T7Mdu/ak41AQAArKlpDqX71ST3JLktyefmWw4AAMDamyYYndfdz5x7JQAAAOtkmlnp/ltVPXHulQAAAKyTafYYXZLkyqr6UFYOpask3d1fP9fKAAAA1sg0wehZc68CAABgHU0zXfeHk6SqvjrJw+deEQAAwBo75TlGVXV5VX0gyYeS/EZWLvb6tjnXBQAAsGammXzh1UmemuQPu/uCJJcmeedcqwIAAFhD0wSj+7v7E0lOq6rTuvtQkovmXBcAAMCamWbyhU9V1ZlJfivJL1TVx5J8Zr5lAQAArJ1p9hhdkeSzSV6c5MYkf5Tk2fMsCgAAYC1NMyvdZ6rqcUke393XVtUjkmyZf2kAAABr45TBqKr+SZKrknxlkr+e5Nwk/3dWJmEAYAPavufgsAPeONx4Z51x+mBjAcC0pjnH6IeSXJzkXUnS3R+YXNMIgA3ozqsvG3S87XsODj4mAKy1ac4x+lx3f/7oQlVtTdLzKwkAAGBtTROMfqOq/lWSM6rq6Ul+OcmvzbcsAACAtTNNMNqT5ONJfj/JDya5Icn/Mc+iAAAA1tI0s9L9RZL/OLkBAABsOqfcY1RV315Vv1dVn6yqT1fVvVX16bUoDgAAYC1MMyvdv0/ynUl+v7tNugAAAGw60wSjjyT5g1lCUVU9M8lPZuWCsG/o7qtPsN1zkrwlyd/u7lur6vQkb0jy5EmNb+ruf/1Qvz4AAOMz6LXaXKdtNKYJRv8yyQ1V9RtJPnd0ZXe/5mRPqqotSV6X5OlJ7k5yS1Vd3923H7PdtiQvyuQ6SRP/IMmXdfcTq+oRSW6vqgPdfecU9QIAMFJDXlfNddrGZZpZ6fYl+WyShyfZtup2KhcnuaO7Pzi5DtJ1Sa44znavTnJNkj9fta6TPHJyzaQzknw+ifOaAACAuZhmj9HXdPeFM4x9blYOwzvq7iTftHqDqnpykvO7+2BV/eiqh96SlRD1x0kekeRHuvuTx36BqroqyVVJcvbZZ2d5eXmGMtfOote3qMbStyFf55EjRwYdbyz/B8m4XuuQxtS3oV7r0J/TZBz/D/Po2xjo2+z0bXYbrXfTBKMbquoZ3f32Ib9wVZ2W5DVJrjzOwxcneTDJ1yT5iiS/VVX/b3d/cPVG3f36JK9PkosuuqiXlpaGLHFYNx7MQte3qMbSt4Ff5/Ly8nDjjeX/IBnXax3SmPo24Gsd9HOajOb/YfC+jYS+zWgkn6u52IC9m+ZQun+W5Maqum8yVfe003V/NMn5q5bPm6w7aluSC5MsV9WdSZ6a5PqquijJ9yS5sbvv7+6PJfmvSS6a4msCAAA8ZKcMRt29rbtP6+4zJve3dfejphj7liSPr6oLquphSZ6b5PpV497T3Y/u7u3dvT3JO5Nc3t23JrkrydOSpKoemZXQ9N8f8qsDAACYwjR7jP5SVf3YtNt29wNJXpDkpiSHk/xSd7+vql5VVZef4umvS3JmVb0vKwHrZ7v7vQ+lVgAAgGlNc47Rapcn+bFpN98OiDQAABOWSURBVO7uG5LccMy6l59g26VV949kZcpuAACAuTvhHqOqOv94qyePffvcKgIAAFhjJzuU7h1Vtf2YdU+pqu9P8pNzqwgAAGCNnexQun+R5O1VdVl3f2Cy7l8meV6Svzv3ytjUqmr6ba859Tbd/SVUAwDA2J1wj9Hk/KB/luRtVXVhVf37rJxj9K3dffdaFcjm1N1T3Q4dOjTVdgAA8KU46ax03f3rSb4vyXKSr03ytO7+szWoCwAAYM2c8FC6qro3SWdlwoUvS3Jpko/VyjFQPeW1jAAAABbeCYNRd29by0IAAADWy0O6wCsAAMBmJBgBAACjJxgBAACjd7LrGAEAx9i+5+Bwg9043FhnnXH6YGMBjJFgBABTuvPqywYba/ueg4OOB8CXxqF0AADA6AlGAADA6AlGAADA6AlGAADA6Jl8ARbEoDNdJYPNdmWmKwBgDAQjWABDz0xltisAgIfGoXQAAMDoCUYAAMDoCUYAAMDoCUYAAMDoCUYAAMDoCUYAAMDoCUYAAMDoCUYAAMDoCUYAAMDoCUYAAMDoCUYAAMDoCUYAAMDoCUYAAMDoCUYAAMDoCUYAAMDoCUYAAMDobV3vAja6qpp+22tOvU13fwnVAADA/Gzm333tMfoSdfdUt0OHDk21HQAALKrN/LuvPUbApjf0X7eSxfoLFwDwpbPHCNj0hv7rllAEAJuPYAQAAIyeYAQAAIyeYAQAAIyeYAQAAIyeYAQAAIyeYAQAAIyeYAQAAIyeYAQAAIyeYAQAAIyeYAQAAIyeYAQAAIyeYAQAAIyeYAQAAIze1vUuAIDFVFXTb3vNdNt194zVAMB82WMEwHF191S3Q4cOTb0tACwqwQgAABg9wQgAABg9wQgAABg9wQgAABg9wQgAABg9wQgAABg9wQgAABg9wQgAABg9wQgAABg9wQgAABg9wQgAABg9wQgAABg9wQgAABg9wQgAABg9wQgAABg9wQgAABg9wQgAABg9wQgAABg9wQgAABg9wYiFdODAgVx44YW59NJLc+GFF+bAgQPrXRIAAJvY1vUuAI514MCB7N27N/v378+DDz6YLVu2ZPfu3UmSXbt2rXN1AABsRvYYsXD27duX/fv3Z+fOndm6dWt27tyZ/fv3Z9++fetdGgAAm9Rcg1FVPbOq3l9Vd1TVnpNs95yq6qq6aNW6r6+q366q91XV71fVw+dZK4vj8OHDueSSS75g3SWXXJLDhw+vU0UAAGx2cwtGVbUlyeuSPCvJE5LsqqonHGe7bUlelORdq9ZtTfLzSf5pd/+tJEtJ7p9XrSyWHTt25Oabb/6CdTfffHN27NixThUBALDZzXOP0cVJ7ujuD3b355Ncl+SK42z36iTXJPnzVeuekeS93f2eJOnuT3T3g3OslQWyd+/e7N69O4cOHcoDDzyQQ4cOZffu3dm7d+96lwYAwCY1z8kXzk3ykVXLdyf5ptUbVNWTk5zf3Qer6kdXPfQ3knRV3ZTkMUmu6+4fP/YLVNVVSa5KkrPPPjvLy8vDvoIBHTlyZKHrWyTnnHNOnve85+X7v//7c9ddd+Wxj31snv/85+ecc87Rw4dArx46n9PZ6Nvs9O2h836bjb7NTt9msxHfc+s2K11VnZbkNUmuPM7DW5NckuRvJ/lskl+vqtu6+9dXb9Tdr0/y+iS56KKLemlpaZ4lf0mWl5ezyPUtmqWlpbz61a/Wt1ndeFDfZuD9Nht9m5HP6Uy832ajbzPyOZ3ZRnzPzfNQuo8mOX/V8nmTdUdtS3JhkuWqujPJU5NcP5mA4e4kv9ndf9rdn01yQ5Inz7FWAABgxOYZjG5J8viquqCqHpbkuUmuP/pgd9/T3Y/u7u3dvT3JO5Nc3t23JrkpyROr6hGTiRj+bpLb51grAAAwYnMLRt39QJIXZCXkHE7yS939vqp6VVVdforn/llWDrO7Jcm7k/xudx+cV60AAMC4zfUco+6+ISuHwa1e9/ITbLt0zPLPZ2XKbgAAgLma6wVeAQAANgLBCAAAGD3BCAAAGD3BCABgkzpw4EAuvPDCXHrppbnwwgtz4MCB9S4JFta6XeAVAID5OXDgQPbu3Zv9+/fnwQcfzJYtW7J79+4kya5du9a5Olg89hgBAGxC+/bty/79+7Nz585s3bo1O3fuzP79+7Nv3771Lg0WkmAEALAJHT58OJdccskXrLvkkkty+PDhdaoIFptgBAADqqqpbh++5tun3hZmsWPHjtx8881fsO7mm2/Ojh071qkiWGyCEQAMqLunuh06dGjqbWEWe/fuze7du3Po0KE88MADOXToUHbv3p29e/eud2mwkEy+AACwCR2dYOGHf/iHc/jw4ezYsSP79u0z8QKcgGAEALBJ7dq1K7t27cry8nKWlpbWuxxYaA6lAwAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARk8wAgAARm/rehcAAABrraqm2+6a6cbr7i+hGhaBPUYAAIxOd5/ydujQoam2E4o2B8EIAAAYPcEIAAAYPcEIAAAYPcEIAAAYPcEIAAAYvbkGo6p6ZlW9v6ruqKo9J9nuOVXVVXXRMesfW1VHquol86wTAAAYt7kFo6rakuR1SZ6V5AlJdlXVE46z3bYkL0ryruMM85okb5tXjQAAAMl89xhdnOSO7v5gd38+yXVJrjjOdq9Ock2SP1+9sqq+I8mHkrxvjjUCAADMNRidm+Qjq5bvnqz7S1X15CTnd/fBY9afmeSlSV45x/oAAACSJFvX6wtX1WlZOVTuyuM8/GNJ/l13H6mqk41xVZKrkuTss8/O8vLy4HUO5ciRIwtd36LSt9np20Pn/TYbfZuNvs1G32ajb7PRt9ltxN7NMxh9NMn5q5bPm6w7aluSC5MsT8LPX0tyfVVdnuSbknxXVf14ki9P8hdV9efd/VOrv0B3vz7J65Pkoosu6qWlpTm9lC/d8vJyFrm+RaVvM7rxoL7NwPttNvo2G32bjb7NRt9mo2+z24i9m2cwuiXJ46vqgqwEoucm+Z6jD3b3PUkefXS5qpaTvKS7b03yv65a/2NJjhwbigAAAIYyt3OMuvuBJC9IclOSw0l+qbvfV1WvmuwVAgAAWAhzPceou29IcsMx615+gm2XTrD+xwYvDAAAYJW5XuAVAABgIxCMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0ROMAACA0du63gUA06uq6be9ZrrtunvGagAANg97jGAD6e6pbocOHZp6WwAABCMAAADBCAAAQDACAABGTzACAABGTzACAABGTzACAABGTzACAABGTzACAABGTzACAABGTzACAABGTzACAABGTzACAABGTzACAABGTzACAABGTzACAABGTzACAABGTzACAABGTzACAABGTzACAABGTzACAABGTzACAABGTzACAABGTzACAABGTzACAABGTzACAABGr7p7vWsYRFV9PMmH17uOk3h0kj9d7yI2IH2bjb7NRt9mo2+z0bfZ6Nts9G02+ja7Re7d47r7Mceu3DTBaNFV1a3dfdF617HR6Nts9G02+jYbfZuNvs1G32ajb7PRt9ltxN45lA4AABg9wQgAABg9wWjtvH69C9ig9G02+jYbfZuNvs1G32ajb7PRt9no2+w2XO+cYwQAAIyePUYAAMDoCUYDq6rzq+pQVd1eVe+rqhdN1n9lVb2jqj4w+fcr1rvWRVJVD6+q36mq90z69srJ+guq6l1VdUdVvbmqHrbetS6iqtpSVb9XVW+dLOvbKVTVnVX1+1X17qq6dbLO5/QUqurLq+otVfXfq+pwVX2zvp1aVX3d5L129Pbpqnqx3p1aVf3I5OfCH1TVgcnPC9/jjlFVb6yqj1XVH6xad9z3V6147aR/762qJ69f5evrBH37sar66KrP699b9djLJn17f1V92/pUvf4e6u+7G+U9JxgN74Ek/3t3PyHJU5P8UFU9IcmeJL/e3Y9P8uuTZf7K55I8rbuflOQbkjyzqp6a5Jok/667/5ckf5Zk9zrWuMhelOTwqmV9m87O7v6GVdOJ+pye2k8mubG7/2aSJ2Xlfadvp9Dd75+8174hyVOSfDbJf47enVRVnZvkhUku6u4Lk2xJ8tz4Hnc8P5fkmcesO9H761lJHj+5XZXkp9eoxkX0c/niviUr769vmNxuSJLJ73PPTfK3Js/5D1W1Zc0qXSwP9ffdDfGeE4wG1t1/3N2/O7l/b1Z+aTg3yRVJrp1sdm2S71ifChdTrzgyWTx9cuskT0vylsl6fTuOqjovyWVJ3jBZrujbrHxOT6KqzkryrUn2J0l3f767PxV9e6guTfJH3f3h6N00tiY5o6q2JnlEkj+O73FfpLt/M8knj1l9ovfXFUneNPnZ+84kX15V56xNpYvlBH07kSuSXNfdn+vuDyW5I8nFcytugc3w++6GeM8JRnNUVduTfGOSdyU5u7v/ePLQ/0xy9jqVtbAmh4O9O8nHkrwjyR8l+VR3PzDZ5O6sfOj4Qv8+yb9M8heT5a+Kvk2jk7y9qm6rqqsm63xOT+6CJB9P8rOTQzffUFWPjL49VM9NcmByX+9Oors/muTfJrkrK4HoniS3xfe4aZ3o/XVuko+s2k4Pv9gLJod8vXHVIa76dhxT/r67IXonGM1JVZ2Z5D8leXF3f3r1Y70yFaDpAI/R3Q9ODjM5Lyt/gfmb61zSwquqb0/yse6+bb1r2YAu6e4nZ2X3/g9V1beuftDn9Li2Jnlykp/u7m9M8pkcc+iXvp3c5FyYy5P88rGP6d0Xm/xCekVWQvnXJHlkjn/YE6fg/fWQ/HSSv56VQ/v/OMlPrG85i2uz/b4rGM1BVZ2elTfJL3T3r0xW/8nRXYaTfz+2XvUtusmhOYeSfHNWdrVunTx0XpKPrlthi+lbklxeVXcmuS4rh5f8ZPTtlCZ/iU53fywr53pcHJ/TU7k7yd3d/a7J8luyEpT0bXrPSvK73f0nk2W9O7n/LcmHuvvj3X1/kl/Jyvc93+Omc6L310eTnL9qOz1cpbv/ZPLH2r9I8h/zV4fL6dsqD/H33Q3RO8FoYJPzO/YnOdzdr1n10PVJvndy/3uT/Opa17bIquoxVfXlk/tnJHl6Vo5XPZTkuyab6dsxuvtl3X1ed2/PyuE5/193Py/6dlJV9ciq2nb0fpJnJPmD+JyeVHf/zyQfqaqvm6y6NMnt0beHYlf+6jC6RO9O5a4kT62qR0x+vh59z/keN50Tvb+uT/KPJzOFPTXJPasOfxq9Y859+ftZ+fmQrPTtuVX1ZVV1QVYmEvidta5vEczw++6GeM+5wOvAquqSJL+V5PfzV+d8/KusHHf5S0kem+TDSb67u6c92W/Tq6qvz8pJeluyEth/qbtfVVVfm5U9IV+Z5PeSPL+7P7d+lS6uqlpK8pLu/nZ9O7lJf/7zZHFrkl/s7n1V9VXxOT2pqvqGrEz08bAkH0zyfZl8ZqNvJzUJ4Xcl+druvmeyznvuFGrl8g3/MCuzYP1ekh/IyrkJvsetUlUHkiwleXSSP0nyiiT/Jcd5f01+qf2prByW+Nkk39f/f3t3qGpFGEZheC3wPrwLoxwwKMfuBWiwmvUurOJNWOwKJpNREI5RBItiOeBnOAgWEYsze/7nSTvssNhMmBfmnz3zdovdW/vD73aWq8foJslFkoe/buLbPklyP1fX46OZefnfR+/Av97vnso1J4wAAIDleZQOAABYnjACAACWJ4wAAIDlCSMAAGB5wggAAFieMALgcNp+++3zedv3ba9vuQmAfbv2968AwGlqeyvJ0yS3Z+bj1nsA2C9hBMAhtb2Z5FmS85n5sPUeAPbNH7wCcDhtL5N8TXI2M++23gPA/jljBMARXSZ5k+TB1kMAOA3CCIAj+pHkXpIbbR9vPQaA/XPGCIBDmpnvbe8med3208w833oTAPsljAA4rJn50vZOkldtP8/Mi603AbBPXr4AAAAszxkjAABgecIIAABYnjACAACWJ4wAAIDlCSMAAGB5wggAAFieMAIAAJYnjAAAgOX9BCuwrbxqWo14AAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "df = pd.DataFrame(kmeans_ARI,\n",
        "                  columns=columns)\n",
        "boxplot = df.boxplot(column=columns, figsize=(14,11))  \n",
        "boxplot.set_ylabel('K-means ARI on latent space')\n",
        "boxplot.set_xlabel('K')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "id": "ID5QKqAAjDN3",
        "outputId": "828b75ca-88a3-44ec-b75f-7e4fb6a3d06a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'K')"
            ]
          },
          "metadata": {},
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1008x792 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "df = pd.DataFrame(agglo_acc,\n",
        "                  columns=columns)\n",
        "boxplot = df.boxplot(column=columns, figsize=(14,11))  \n",
        "boxplot.set_ylabel('Agglomerative clustering accuracy on latent space')\n",
        "boxplot.set_xlabel('K')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "id": "r6PLfFLnjDN3",
        "outputId": "ef7e1863-34a3-4f81-d7f7-6793ed7d451d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'K')"
            ]
          },
          "metadata": {},
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1008x792 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "df = pd.DataFrame(agglo_NMI,\n",
        "                  columns=columns)\n",
        "boxplot = df.boxplot(column=columns, figsize=(14,11))  \n",
        "boxplot.set_ylabel('Agglomerative clustering NMI on latent space')\n",
        "boxplot.set_xlabel('K')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "id": "_xSfZpQVjDN4",
        "outputId": "420c9b76-b0f4-4c37-ef0f-fc6d28e8c05f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'K')"
            ]
          },
          "metadata": {},
          "execution_count": 46
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1008x792 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "df = pd.DataFrame(agglo_ARI,\n",
        "                  columns=columns)\n",
        "boxplot = df.boxplot(column=columns, figsize=(14,11))  \n",
        "boxplot.set_ylabel('Agglomerative clustering ARI on latent space')\n",
        "boxplot.set_xlabel('K')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "id": "z-RXXNi2jDN4",
        "outputId": "e26be888-bfb2-468d-dc9b-91277d2a06d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'K')"
            ]
          },
          "metadata": {},
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1008x792 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "df = pd.DataFrame(kmeans_silhouette,\n",
        "                  columns=columns)\n",
        "boxplot = df.boxplot(column=columns, figsize=(14,11))  \n",
        "boxplot.set_ylabel('K-means silhouette score on latent space')\n",
        "boxplot.set_xlabel('K')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(agglo_silhouette,\n",
        "                  columns=columns)\n",
        "boxplot = df.boxplot(column=columns, figsize=(14,11))  \n",
        "boxplot.set_ylabel('Agglomerative clustering silhouette score on latent space')\n",
        "boxplot.set_xlabel('K')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "id": "7j9d3a8ICyWF",
        "outputId": "7a0eb59d-49c7-4cb0-b89c-f99a80d7e9de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'K')"
            ]
          },
          "metadata": {},
          "execution_count": 48
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1008x792 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0px0IAb7zFF"
      },
      "source": [
        "# Silhoutte factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiXA3S51GiGJ"
      },
      "outputs": [],
      "source": [
        "def get_all_representations(dataloader, data_shape, datapoints, datapoints_reshaped, labels, epochs, hl1_neurons, hl2_neurons, hl3_neurons, latent_dimension, list_of_cluster_numbers):\n",
        "\n",
        "  representation_K_tuples = []\n",
        "\n",
        "  for K in list_of_cluster_numbers:\n",
        "    print(\"Creating representation for K =\", K, \":\\n\")\n",
        "    print(\"Running k-means...\\n\")\n",
        "    kmeans = KMeans(n_clusters=K, n_init=1).fit(datapoints_reshaped)\n",
        "    cluster_centers = kmeans.cluster_centers_\n",
        "    cluster_labels = kmeans.labels_\n",
        "\n",
        "    # using the autoencoder model on our data\n",
        "\n",
        "    print(\"Training autoencoder...\\n\")\n",
        "    autoencoder = Autoencoder(input_dimension=data_shape, hl1_neurons=hl1_neurons, hl2_neurons=hl2_neurons, hl3_neurons=hl3_neurons, latent_dimension=latent_dimension).to(device)\n",
        "    autoencoder, loss_list = train_autoencoder(device, dataloader, autoencoder, epochs=100)\n",
        "    latent_data = autoencoder.encoder(datapoints, device).cpu().detach().numpy() \n",
        "\n",
        "    print(\"\\n\")\n",
        "    r = latent_data\n",
        "\n",
        "    representation_K_tuples.append((r,K))\n",
        "\n",
        "  return representation_K_tuples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmBtog19T8W1"
      },
      "outputs": [],
      "source": [
        "def calculate_silhouette_for_each_representation(representation_tuples, list_of_cluster_numbers):\n",
        "  representations = [pair[0] for pair in representation_tuples]\n",
        "  rep_silhouette_list = [[] for i in range(len(representations))]\n",
        "\n",
        "  for index, rep in enumerate(representations):\n",
        "    print(\"Starting process for representation number\", index + 1,\":\\n\")\n",
        "    for K in list_of_cluster_numbers:\n",
        "      print(\"Running kmeans for K =\", K,\"...\")\n",
        "      kmeans = KMeans(n_clusters=K, n_init=10).fit(rep)\n",
        "      k_means_silhouette_score = silhouette_score(rep, kmeans.labels_)\n",
        "      rep_silhouette_list[index].append(k_means_silhouette_score)\n",
        "    print(\"\\n\")\n",
        "\n",
        "  return rep_silhouette_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kk8gc7DsdYkt"
      },
      "outputs": [],
      "source": [
        "def get_max_silhouette_for_each_representation(rep_silhouette_lst):\n",
        "  max_silhouette_tuples = [] # keep tuples in the form of (index, max_silhouette)\n",
        "\n",
        "  for index, silhouette_lst in enumerate(rep_silhouette_lst):\n",
        "    max = silhouette_lst[0]\n",
        "\n",
        "    for i in range(1,len(silhouette_lst)):\n",
        "        if silhouette_lst[i] > max:\n",
        "            max = silhouette_lst[i]\n",
        "\n",
        "    max_silhouette_tuples.append((index, max))\n",
        "\n",
        "  return max_silhouette_tuples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XOqQYpJvZ4F"
      },
      "outputs": [],
      "source": [
        "def calculate_representation_kmeans_acc(representation_tuples, Y):\n",
        "  representations = [pair[0] for pair in representation_tuples]\n",
        "  accuracies = []\n",
        "\n",
        "  for index, rep in enumerate(representations):\n",
        "    kmeans = KMeans(n_clusters=10, n_init=1).fit(rep)\n",
        "    kmeans_clusters = kmeans.labels_\n",
        "    kmeans_greedy_labels = transform_clusters_to_labels(kmeans_clusters, Y)\n",
        "    acc = accuracy_score(Y, kmeans_greedy_labels)\n",
        "    accuracies.append((index,acc))\n",
        "\n",
        "  return accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgmXbbfogBsE"
      },
      "outputs": [],
      "source": [
        "def calculate_normalised_silhouette(max_silhouette_tuples, representation_tuples):\n",
        "  normalised_silhouette_tuples = []\n",
        "  representations = [pair[0] for pair in representation_tuples]\n",
        "\n",
        "  for rep_index, rep in enumerate(representations):\n",
        "    kmeans = KMeans(n_clusters=10, n_init=1).fit(rep)\n",
        "    k_means_silhouette_score = silhouette_score(rep, kmeans.labels_)\n",
        "    normalised_silhouette = (k_means_silhouette_score) / (max_silhouette_tuples[rep_index][1])\n",
        "    normalised_silhouette_tuples.append((rep_index, normalised_silhouette))\n",
        "\n",
        "\n",
        "  return normalised_silhouette_tuples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kV9DUSzVFbyk"
      },
      "outputs": [],
      "source": [
        "def silhouette_factor(dataloader, data_shape, datapoints, datapoints_reshaped, labels, epochs, hl1_neurons, hl2_neurons, hl3_neurons, latent_dimension, list_of_cluster_numbers):\n",
        "  representation_tuples = get_all_representations(dataloader, data_shape, datapoints, datapoints_reshaped, labels, epochs, hl1_neurons, hl2_neurons, hl3_neurons, latent_dimension, list_of_cluster_numbers)\n",
        "  representation_silhouette_list = calculate_silhouette_for_each_representation(representation_tuples, list_of_cluster_numbers)\n",
        "  max_silhouette_list = get_max_silhouette_for_each_representation(representation_silhouette_list)\n",
        "  normalised_silhouette_scores = calculate_normalised_silhouette(max_silhouette_list,representation_tuples)\n",
        "  k_means_accuracies = calculate_representation_kmeans_acc(representation_tuples, labels)\n",
        "  print(normalised_silhouette_scores)\n",
        "  print(k_means_accuracies)\n",
        "\n",
        "  return normalised_silhouette_scores, k_means_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Q470K-oLw8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d27c60fe-1375-4783-97a6-2b9d4cc48707"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating representation for K = 20 :\n",
            "\n",
            "Running k-means...\n",
            "\n",
            "Training autoencoder...\n",
            "\n",
            "Epoch: 1/100, Loss: 0.959420\n",
            "Epoch: 2/100, Loss: 0.751017\n",
            "Epoch: 3/100, Loss: 0.662943\n",
            "Epoch: 4/100, Loss: 0.618362\n",
            "Epoch: 5/100, Loss: 0.591466\n",
            "Epoch: 6/100, Loss: 0.572253\n",
            "Epoch: 7/100, Loss: 0.556912\n",
            "Epoch: 8/100, Loss: 0.543715\n",
            "Epoch: 9/100, Loss: 0.531871\n",
            "Epoch: 10/100, Loss: 0.520964\n",
            "Epoch: 11/100, Loss: 0.510741\n",
            "Epoch: 12/100, Loss: 0.501054\n",
            "Epoch: 13/100, Loss: 0.491816\n",
            "Epoch: 14/100, Loss: 0.482942\n",
            "Epoch: 15/100, Loss: 0.474396\n",
            "Epoch: 16/100, Loss: 0.466131\n",
            "Epoch: 17/100, Loss: 0.458120\n",
            "Epoch: 18/100, Loss: 0.450335\n",
            "Epoch: 19/100, Loss: 0.442753\n",
            "Epoch: 20/100, Loss: 0.435360\n",
            "Epoch: 21/100, Loss: 0.428134\n",
            "Epoch: 22/100, Loss: 0.421070\n",
            "Epoch: 23/100, Loss: 0.414150\n",
            "Epoch: 24/100, Loss: 0.407371\n",
            "Epoch: 25/100, Loss: 0.400721\n",
            "Epoch: 26/100, Loss: 0.394190\n",
            "Epoch: 27/100, Loss: 0.387775\n",
            "Epoch: 28/100, Loss: 0.381471\n",
            "Epoch: 29/100, Loss: 0.375272\n",
            "Epoch: 30/100, Loss: 0.369176\n",
            "Epoch: 31/100, Loss: 0.363176\n",
            "Epoch: 32/100, Loss: 0.357269\n",
            "Epoch: 33/100, Loss: 0.351455\n",
            "Epoch: 34/100, Loss: 0.345728\n",
            "Epoch: 35/100, Loss: 0.340082\n",
            "Epoch: 36/100, Loss: 0.334512\n",
            "Epoch: 37/100, Loss: 0.329025\n",
            "Epoch: 38/100, Loss: 0.323610\n",
            "Epoch: 39/100, Loss: 0.318272\n",
            "Epoch: 40/100, Loss: 0.313005\n",
            "Epoch: 41/100, Loss: 0.307802\n",
            "Epoch: 42/100, Loss: 0.302668\n",
            "Epoch: 43/100, Loss: 0.297599\n",
            "Epoch: 44/100, Loss: 0.292594\n",
            "Epoch: 45/100, Loss: 0.287654\n",
            "Epoch: 46/100, Loss: 0.282778\n",
            "Epoch: 47/100, Loss: 0.277960\n",
            "Epoch: 48/100, Loss: 0.273206\n",
            "Epoch: 49/100, Loss: 0.268508\n",
            "Epoch: 50/100, Loss: 0.263869\n",
            "Epoch: 51/100, Loss: 0.259286\n",
            "Epoch: 52/100, Loss: 0.254760\n",
            "Epoch: 53/100, Loss: 0.250293\n",
            "Epoch: 54/100, Loss: 0.245880\n",
            "Epoch: 55/100, Loss: 0.241520\n",
            "Epoch: 56/100, Loss: 0.237214\n",
            "Epoch: 57/100, Loss: 0.232962\n",
            "Epoch: 58/100, Loss: 0.228762\n",
            "Epoch: 59/100, Loss: 0.224616\n",
            "Epoch: 60/100, Loss: 0.220520\n",
            "Epoch: 61/100, Loss: 0.216475\n",
            "Epoch: 62/100, Loss: 0.212484\n",
            "Epoch: 63/100, Loss: 0.208543\n",
            "Epoch: 64/100, Loss: 0.204655\n",
            "Epoch: 65/100, Loss: 0.200816\n",
            "Epoch: 66/100, Loss: 0.197025\n",
            "Epoch: 67/100, Loss: 0.193285\n",
            "Epoch: 68/100, Loss: 0.189592\n",
            "Epoch: 69/100, Loss: 0.185947\n",
            "Epoch: 70/100, Loss: 0.182353\n",
            "Epoch: 71/100, Loss: 0.178803\n",
            "Epoch: 72/100, Loss: 0.175299\n",
            "Epoch: 73/100, Loss: 0.171842\n",
            "Epoch: 74/100, Loss: 0.168433\n",
            "Epoch: 75/100, Loss: 0.165075\n",
            "Epoch: 76/100, Loss: 0.161763\n",
            "Epoch: 77/100, Loss: 0.158498\n",
            "Epoch: 78/100, Loss: 0.155284\n",
            "Epoch: 79/100, Loss: 0.152113\n",
            "Epoch: 80/100, Loss: 0.148991\n",
            "Epoch: 81/100, Loss: 0.145927\n",
            "Epoch: 82/100, Loss: 0.142913\n",
            "Epoch: 83/100, Loss: 0.139962\n",
            "Epoch: 84/100, Loss: 0.137084\n",
            "Epoch: 85/100, Loss: 0.134261\n",
            "Epoch: 86/100, Loss: 0.131473\n",
            "Epoch: 87/100, Loss: 0.128651\n",
            "Epoch: 88/100, Loss: 0.125831\n",
            "Epoch: 89/100, Loss: 0.123045\n",
            "Epoch: 90/100, Loss: 0.120273\n",
            "Epoch: 91/100, Loss: 0.117544\n",
            "Epoch: 92/100, Loss: 0.114898\n",
            "Epoch: 93/100, Loss: 0.112322\n",
            "Epoch: 94/100, Loss: 0.109799\n",
            "Epoch: 95/100, Loss: 0.107327\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-cb6074aa9513>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnormalised_silhouette_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_means_accuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msilhouette_factor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_reshaped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhl1_neurons\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhl2_neurons\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhl3_neurons\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_of_cluster_numbers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-63272032475a>\u001b[0m in \u001b[0;36msilhouette_factor\u001b[0;34m(dataloader, data_shape, datapoints, datapoints_reshaped, labels, epochs, hl1_neurons, hl2_neurons, hl3_neurons, latent_dimension, list_of_cluster_numbers)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msilhouette_factor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatapoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatapoints_reshaped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhl1_neurons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhl2_neurons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhl3_neurons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_of_cluster_numbers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mrepresentation_tuples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_representations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatapoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatapoints_reshaped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhl1_neurons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhl2_neurons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhl3_neurons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_of_cluster_numbers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mrepresentation_silhouette_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_silhouette_for_each_representation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepresentation_tuples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_of_cluster_numbers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mmax_silhouette_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_max_silhouette_for_each_representation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepresentation_silhouette_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mnormalised_silhouette_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_normalised_silhouette\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_silhouette_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrepresentation_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-7e23d9459e07>\u001b[0m in \u001b[0;36mget_all_representations\u001b[0;34m(dataloader, data_shape, datapoints, datapoints_reshaped, labels, epochs, hl1_neurons, hl2_neurons, hl3_neurons, latent_dimension, list_of_cluster_numbers)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training autoencoder...\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mautoencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhl1_neurons\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhl1_neurons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhl2_neurons\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhl2_neurons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhl3_neurons\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhl3_neurons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlatent_dimension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mautoencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_autoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mlatent_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-d6b441799b9f>\u001b[0m in \u001b[0;36mtrain_autoencoder\u001b[0;34m(device, dataloader, autoencoder, epochs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                         \u001b[0;31m# compute accumulated gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                         \u001b[0;31m# perform parameter update based on current gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "normalised_silhouette_scores, k_means_accuracies = silhouette_factor(dataloader, 784, images, images_reshaped, labels, 100, hl1_neurons=32, hl2_neurons=64, hl3_neurons=128, latent_dimension=10, list_of_cluster_numbers=[20, 30, 40, 50, 60, 70, 80, 100, 150, 200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XpwGFbcSM15"
      },
      "outputs": [],
      "source": [
        "print(k_means_accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnqtR5WVK_br"
      },
      "outputs": [],
      "source": [
        "list_of_cluster_numbers=[20, 30, 40, 50, 60, 70, 80, 100, 150, 200]\n",
        "\n",
        "normalised_silhouette_scores = [t[1] for t in normalised_silhouette_scores]\n",
        "k_means_accuracies = [t[1] for t in k_means_accuracies]\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(list_of_cluster_numbers, normalised_silhouette_scores, c='b', marker='x', label='Silhouette criterion')\n",
        "plt.plot(list_of_cluster_numbers, k_means_accuracies, c='r', marker='s', label='K-means accuracy')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ]
    }
  ]
}